{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run MC with narrowed distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.utils import read_hdf5_array, write_hdf5_array\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "from copy import deepcopy\n",
    "from gsa_framework.utils_setac_lca import get_xgboost_params, plot_base_narrow_Y\n",
    "import pickle\n",
    "from gsa_framework.lca import LCAModelSetac\n",
    "import numpy as np\n",
    "\n",
    "num_params_narrow = 60\n",
    "scaling_factor = 4\n",
    "model_seed = 3333\n",
    "\n",
    "# path_base = Path(\"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\")\n",
    "path_base = Path('/data/user/kim_a/')\n",
    "w = path_base / \"setac_gsa\"\n",
    "path_merlin = path_setac / \"merlin\"\n",
    "\n",
    "# 1. LCA setup\n",
    "# LCA model\n",
    "iterations = 500\n",
    "bw.projects.set_current(\"GSA for setac\")\n",
    "co = bw.Database(\"CH consumption 1.0\")\n",
    "demand_act = [act for act in co if \"Food\" in act[\"name\"]][0]\n",
    "demand = {demand_act: 1}\n",
    "method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "lca = bw.LCA(demand, method)\n",
    "lca.lci()\n",
    "lca.lcia()\n",
    "print(lca.score)\n",
    "\n",
    "num_params = len(lca.tech_params[lca.tech_params[\"uncertainty_type\"] > 1])\n",
    "print(num_params)\n",
    "\n",
    "seed = 923458\n",
    "filename_Y_narrow = \"validation.narrow.Y.{}.{}.{}.{}.hdf5\".format(\n",
    "    iterations, num_params_narrow, scaling_factor, seed\n",
    ")\n",
    "filepath_Y_narrow = path_setac / \"arrays\" / filename_Y_narrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if filepath_Y_narrow.exists():\n",
    "    narrow_Y = read_hdf5_array(filepath_Y_narrow).flatten()\n",
    "else:\n",
    "    tech_params_narrow = deepcopy(lca.tech_params)\n",
    "    # Read params_yes from xgboost model\n",
    "    filepath_params_yes_0 = path_merlin / \"params_yes_0.pickle\"\n",
    "    with open(filepath_params_yes_0, \"rb\") as f:\n",
    "        params_yes_0 = pickle.load(f)\n",
    "    path_model_dir = path_setac / \"regression\" / \"{}_model\".format(model_seed)\n",
    "    model, params_yes_xgboost, _ = get_xgboost_params(path_model_dir, params_yes_0)\n",
    "    params_yes = params_yes_xgboost[:num_params_narrow]\n",
    "    dt = lca.tech_params.dtype\n",
    "    tech_params_narrow = np.array([a for a in lca.tech_params], dtype = dt)\n",
    "    for p in params_yes:\n",
    "        tech_params_narrow[p][\"scale\"] = tech_params_narrow[p][\"scale\"]/scaling_factor\n",
    "\n",
    "    lca_model = LCAModelSetac(demand, method, tech_params_narrow)\n",
    "\n",
    "    filename_X_unitcube = \"validation.base.X.unitcube.{}.{}.{}.hdf5\".format(\n",
    "        iterations, num_params, seed\n",
    "    )\n",
    "    filepath_base_X_unitcube = path_setac / \"arrays\" / filename_X_unitcube\n",
    "    if filepath_base_X_unitcube.exists():\n",
    "        X_unitcube = read_hdf5_array(filepath_base_X_unitcube)\n",
    "    else:\n",
    "        np.random.rand(seed)\n",
    "        X_unitcube = np.random.rand(iterations, num_params)\n",
    "        write_hdf5_array(X_unitcube, filepath_base_X_unitcube)\n",
    "\n",
    "    filename_X_rescaled_narrow = \"validation.narrow.X.rescaled.{}.{}.{}.{}.hdf5\".format(\n",
    "        iterations, num_params_narrow, scaling_factor, seed\n",
    "    )\n",
    "    filepath_X_rescaled_narrow = path_setac / \"arrays\" / filename_X_rescaled_narrow\n",
    "    if filepath_X_rescaled_narrow.exists():\n",
    "        X_rescaled_narrow = read_hdf5_array(filepath_X_rescaled_narrow)\n",
    "    else:\n",
    "        X_rescaled_narrow = lca_model.rescale(X_unitcube, filepath_X_rescaled_narrow)\n",
    "        write_hdf5_array(X_rescaled_narrow, filepath_X_rescaled_narrow)\n",
    "    narrow_Y = lca_model(X_rescaled_narrow)\n",
    "    write_hdf5_array(narrow_Y, filepath_Y_narrow)\n",
    "\n",
    "filename_base_Y = \"validation.base.Y.{}.{}.{}.hdf5\".format(iterations, num_params, seed)\n",
    "filepath_base_Y = path_setac / \"arrays\" / filename_base_Y\n",
    "base_Y = read_hdf5_array(filepath_base_Y).flatten()\n",
    "plot_base_narrow_Y(base_Y, narrow_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run MC narrowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brightway2 as bw\n",
    "from pathlib import Path\n",
    "from gsa_framework.lca import LCAModel, LCAModelSetac\n",
    "from gsa_framework.validation import Validation\n",
    "from gsa_framework.utils_setac_lca import *\n",
    "from gsa_framework.utils import read_hdf5_array, write_hdf5_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 2000\n",
    "\n",
    "seed = 923458\n",
    "model_seed = 3333\n",
    "\n",
    "num_params_narrow_list = [12,36,60,84]\n",
    "scaling_factor_list = [2,4,6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/setac_gsa/')\n",
    "path_merlin = path_base / \"merlin\"\n",
    "filepath_scores_lsa_dict = path_merlin / \"scores_lsa_dict.pickle\"\n",
    "filepath_params_yes_0 = path_merlin / \"params_yes_0.pickle\"\n",
    "filepath_params_yes_6 = path_merlin / \"params_yes_6.pickle\"\n",
    "\n",
    "with open(filepath_scores_lsa_dict, 'rb') as f:\n",
    "    scores_lsa_dict = pickle.load(f)\n",
    "    \n",
    "with open(filepath_params_yes_0, 'rb') as f:\n",
    "    params_yes_0 = pickle.load(f)\n",
    "    \n",
    "with open(filepath_params_yes_6, 'rb') as f:\n",
    "    params_yes_6 = pickle.load(f)\n",
    "    \n",
    "# path_base = Path(\n",
    "#     \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "# )\n",
    "path_base = Path('/data/user/kim_a/')\n",
    "write_dir = path_base / \"setac_gsa\"\n",
    "path_model_dir = write_dir / \"regression\" / \"{}_model\".format(model_seed)\n",
    "\n",
    "# LCA model\n",
    "bw.projects.set_current(\"GSA for setac\")\n",
    "co = bw.Database(\"CH consumption 1.0\")\n",
    "demand_act = [act for act in co if \"Food\" in act[\"name\"]][0]\n",
    "demand = {demand_act: 1}\n",
    "method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "lca = bw.LCA(demand, method)\n",
    "lca.lci()\n",
    "lca.lcia()\n",
    "print(lca.score)\n",
    "\n",
    "num_params = len(lca.tech_params[lca.tech_params[\"uncertainty_type\"] > 1])\n",
    "print(num_params)\n",
    "\n",
    "# Read params_yes from xgboost model\n",
    "model, params_yes_xgboost, _ = get_xgboost_params(path_model_dir, params_yes_0)\n",
    "\n",
    "lca_model = LCAModel(demand, method, write_dir)\n",
    "amounts = lca.tech_params[lca.tech_params[\"uncertainty_type\"]>1][\"amount\"]\n",
    "validation = Validation(\n",
    "    lca_model,\n",
    "    iterations=iterations,\n",
    "    seed=seed,\n",
    "    default_x_rescaled=amounts, #TODO change??\n",
    "    write_dir=write_dir,\n",
    ")\n",
    "\n",
    "for num_params_narrow in num_params_narrow_list:\n",
    "    for scaling_factor in scaling_factor_list:\n",
    "        \n",
    "        print(num_params_narrow, scaling_factor)\n",
    "\n",
    "        filename_Y_narrow = \"validation.narrow.Y.{}.{}.div{}.{}.hdf5\".format(\n",
    "            iterations, num_params_narrow, scaling_factor, seed\n",
    "        )\n",
    "        filepath_Y_narrow = write_dir / \"arrays\" / filename_Y_narrow\n",
    "\n",
    "        filename_X_unitcube = \"validation.base.X.unitcube.{}.{}.{}.hdf5\".format(\n",
    "            iterations, num_params, seed\n",
    "        )\n",
    "        filepath_base_X_unitcube = write_dir / \"arrays\" / filename_X_unitcube\n",
    "\n",
    "        filename_X_rescaled_narrow = \"validation.narrow.X.rescaled.{}.{}.div{}.{}.hdf5\".format(\n",
    "            iterations, num_params_narrow, scaling_factor, seed\n",
    "        )\n",
    "        filepath_X_rescaled_narrow = write_dir / \"arrays\" / filename_X_rescaled_narrow\n",
    "        \n",
    "        params_yes = params_yes_xgboost[:num_params_narrow]\n",
    "        # Construct tech_params\n",
    "        dtype = lca.tech_params.dtype\n",
    "        tech_params_narrow = np.array([a for a in lca.tech_params], dtype = dtype)\n",
    "        for p in params_yes:\n",
    "            tech_params_narrow[p][\"scale\"] = tech_params_narrow[p][\"scale\"] / scaling_factor\n",
    "\n",
    "        lca_model_setac = LCAModelSetac(demand, method, tech_params_narrow)\n",
    "\n",
    "        if filepath_Y_narrow.exists():\n",
    "            narrow_Y = read_hdf5_array(filepath_Y_narrow).flatten()\n",
    "        else:\n",
    "            if filepath_X_rescaled_narrow.exists():\n",
    "                X_rescaled_narrow = read_hdf5_array(filepath_X_rescaled_narrow)\n",
    "            else:\n",
    "                if filepath_base_X_unitcube.exists():\n",
    "                    X_unitcube = read_hdf5_array(filepath_base_X_unitcube)\n",
    "                else:\n",
    "                    np.random.rand(seed)\n",
    "                    X_unitcube = np.random.rand(iterations, num_params)\n",
    "                    write_hdf5_array(X_unitcube, filepath_base_X_unitcube)\n",
    "                X_rescaled_narrow = lca_model_setac.rescale(X_unitcube)\n",
    "                write_hdf5_array(X_rescaled_narrow, filepath_X_rescaled_narrow)\n",
    "\n",
    "            narrow_Y = lca_model_setac(X_rescaled_narrow)\n",
    "            write_hdf5_array(narrow_Y, filepath_Y_narrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.plot_histogram_base_Y_influential_Y(\n",
    "    narrow_Y, tag=num_params_narrow, save_fig=False, bin_min=2300, bin_max=3300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bw]",
   "language": "python",
   "name": "conda-env-bw-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
