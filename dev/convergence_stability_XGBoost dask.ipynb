{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_pc = \"merlin_paper_gsa\"\n",
    "if 'merlin' in which_pc:\n",
    "    path_dask_logs = '/data/user/kim_a/dask_logs'\n",
    "    if not os.path.exists(path_dask_logs):\n",
    "        os.makedirs(path_dask_logs)\n",
    "    cluster = SLURMCluster(cores     = 10,\n",
    "                           processes = 6,\n",
    "                           memory    =\"80GB\", \n",
    "                           walltime  = '12:00:00',\n",
    "                           interface ='ib0',\n",
    "                           local_directory = path_dask_logs,\n",
    "                           log_directory   = path_dask_logs,\n",
    "                           queue=\"daily\",\n",
    "                           ) \n",
    "elif 'local' in which_pc:\n",
    "    cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 60\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://192.168.196.21:35128</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.196.21:8787/status' target='_blank'>http://192.168.196.21:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>60</li>\n",
       "  <li><b>Cores: </b>60</li>\n",
       "  <li><b>Memory: </b>799.80 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://192.168.196.21:35128' processes=60 threads=60, memory=799.80 GB>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.sensitivity_analysis.delta_moment import delta_moment\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.utils import write_pickle, read_hdf5_array, write_hdf5_array, read_pickle\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_worker(iterations_current, seed):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    num_params = 10000\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    stability_dir = write_dir / \"stability_intermediate_deltaGsaNr1\"\n",
    "    filepath_S = stability_dir / \"S.step{}.seed{}.pickle\".format(iterations_current, seed)\n",
    "    if not filepath_S.exists():\n",
    "        filepath_X_rescaled = stability_dir / \"X.rescaled.step{}.seed{}.hdf5\".format(iterations_current, seed)\n",
    "        filepath_Y = stability_dir / \"Y.step{}.seed{}.hdf5\".format(iterations_current, seed)\n",
    "        S_dict = delta_moment(\n",
    "            filepath_Y=filepath_Y,\n",
    "            filepath_X=filepath_X_rescaled,\n",
    "            iterations=iterations_current,\n",
    "            num_params=num_params,\n",
    "            seed=seed,\n",
    "            num_resamples=1,\n",
    "        )\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "        return S_dict\n",
    "    else:\n",
    "        print(\"{} already exists\".format(filepath_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act[\"name\"]][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "\n",
    "    # Define some variables\n",
    "    num_params = 10000\n",
    "    num_influential = num_params // 100\n",
    "    iterations_validation = 500\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    validation_seed = 7043\n",
    "    fig_format = [\"html\", \"pickle\"]\n",
    "\n",
    "    iterations = 2 * num_params\n",
    "    num_resamples = 1\n",
    "    \n",
    "    gsa = DeltaMoment(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        num_resamples=num_resamples,\n",
    "        seed=gsa_seed,\n",
    "    )\n",
    "    conv = Convergence(\n",
    "        gsa.filepath_Y,\n",
    "        gsa.num_params,\n",
    "        gsa.generate_gsa_indices,\n",
    "        gsa.gsa_label,\n",
    "        write_dir,\n",
    "        num_steps=25,\n",
    "    )\n",
    "    num_bootstrap = 10\n",
    "    np.random.seed(gsa_seed)\n",
    "    stability_seeds = np.random.randint(\n",
    "        low=0,\n",
    "        high=2147483647,\n",
    "        size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    "    )\n",
    "    \n",
    "    stability_dir = write_dir / \"stability_intermediate_{}\".format(gsa.gsa_label)\n",
    "    stability_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     X_rescaled = read_hdf5_array(gsa.filepath_X_rescaled)\n",
    "#     Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "    \n",
    "#     for i, iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "#         print(iterations_current)\n",
    "#         for seed in stability_seeds[i,:]:\n",
    "#             np.random.seed(seed)\n",
    "#             r = np.random.choice(np.arange(iterations), size=iterations_current, replace=False)\n",
    "#             r.sort()\n",
    "#             assert len(set(r)) == len(r)\n",
    "#             Xcurrent = X_rescaled[r,:]\n",
    "#             Ycurrent = Y[r]\n",
    "#             fp_X = stability_dir / \"X.rescaled.step{}.seed{}.hdf5\".format(iterations_current, seed)\n",
    "#             fp_Y = stability_dir / \"Y.step{}.seed{}.hdf5\".format(iterations_current, seed)\n",
    "#             write_hdf5_array(Xcurrent, fp_X)\n",
    "#             write_hdf5_array(Ycurrent, fp_Y)\n",
    "    model_evals = []\n",
    "    model_evals_all = []\n",
    "    task_per_worker = dask.delayed(compute_per_worker)\n",
    "    for i, iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "        if i%6==0 and i>0:\n",
    "            print(len(model_evals))\n",
    "            model_evals_all.append(model_evals)\n",
    "            model_evals = []\n",
    "        for seed in stability_seeds[i,:]:\n",
    "            model_eval = task_per_worker(iterations_current, seed)\n",
    "            model_evals.append(model_eval)\n",
    "        \n",
    "    model_evals_all.append(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Test\n",
    "# compute_per_worker(800, 1800109006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for model_evals in model_evals_all:\n",
    "    print(len(model_evals))\n",
    "    dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.utils import write_pickle, read_hdf5_array, write_hdf5_array\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(filepath_Y_worker=None,lca=None):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act[\"name\"]][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "\n",
    "    # Define some variables\n",
    "    num_params = 10000\n",
    "    num_influential = num_params // 100\n",
    "    iterations_validation = 500\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    \n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params,lca=lca)\n",
    "    gsa_seed = 3403\n",
    "    validation_seed = 7043\n",
    "    fig_format = [\"html\", \"pickle\"]\n",
    "\n",
    "    iterations = 2 * num_params\n",
    "    \n",
    "    num_boost_round = 400\n",
    "    tuning_parameters = {\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": 0.1,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"refresh_leaf\": True,\n",
    "        \"subsample\": 0.6,\n",
    "        \"min_child_weight\": 0.5,\n",
    "    }\n",
    "    iterations = 2 * num_params\n",
    "    gsa = GradientBoosting(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        seed=gsa_seed,\n",
    "        tuning_parameters=tuning_parameters,\n",
    "        num_boost_round=num_boost_round,\n",
    "        xgb_model=None,\n",
    "    )\n",
    "    \n",
    "    convergence_dir = write_dir / \"convergence_intermediate_{}\".format(gsa.gsa_label)\n",
    "    convergence_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if filepath_Y_worker is None:\n",
    "        filepath_Y_worker = gsa.filepath_Y\n",
    "    conv = Convergence(\n",
    "        filepath_Y_worker,\n",
    "        gsa.num_params,\n",
    "        gsa.generate_gsa_indices,\n",
    "        gsa.gsa_label,\n",
    "        write_dir,\n",
    "        num_steps=100,\n",
    "    )\n",
    "    return gsa, conv, convergence_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_worker(iterations_current, filepath_Y_worker, lca):\n",
    "    gsa, conv, convergence_dir = setup(filepath_Y_worker, lca)\n",
    "    filepath_convergence_dict = convergence_dir / conv.create_convergence_dict_filepath()\n",
    "    selected_iterations = conv.iterations_order[0:iterations_current]\n",
    "    parameters_convergence_dict = {\n",
    "        \"iterations\": iterations_current,\n",
    "        \"iterations_step\": conv.iterations_step,\n",
    "        \"selected_iterations\": selected_iterations,\n",
    "        \"flag_convergence\": True,\n",
    "    }\n",
    "    gsa_indices_dict = conv.gsa_func(**parameters_convergence_dict)\n",
    "    write_pickle(gsa_indices_dict, filepath_convergence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "#     )\n",
    "    gsa, conv, convergence_dir = setup()\n",
    "    task_per_worker = dask.delayed(compute_per_worker)\n",
    "    Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "    \n",
    "    model_evals = []\n",
    "    \n",
    "    for i, iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "        \n",
    "        filename = \"S.{}.{}.{}Step{}.{}.pickle\".format(\n",
    "            gsa.gsa_label, gsa.sampling_label, iterations_current, 800, 3403\n",
    "        )\n",
    "        filepath = (\n",
    "            convergence_dir / filename\n",
    "        )\n",
    "        if not filepath.exists() and iterations_current!=200 and iterations_current!=400:\n",
    "            filepath_Y_worker = convergence_dir / \"Y.{}.hdf5\".format(iterations_current)\n",
    "            write_hdf5_array(Y,filepath_Y_worker)\n",
    "            model_eval = task_per_worker(\n",
    "                iterations_current,\n",
    "                filepath_Y_worker,\n",
    "                gsa.model.lca,\n",
    "            )\n",
    "            model_evals.append(model_eval)\n",
    "        else:\n",
    "            print(filepath.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Test\n",
    "# gsa, conv, convergence_dir = setup()\n",
    "# filepath_Y_worker = convergence_dir / \"Y.{}.hdf5\".format(200)\n",
    "# compute_per_worker(200, filepath_Y_worker, gsa.model.lca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = dask.compute(model_evals[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "# LCA model\n",
    "bw.projects.set_current(\"GSA for paper\")\n",
    "co = bw.Database(\"CH consumption 1.0\")\n",
    "act = [act for act in co if \"Food\" in act[\"name\"]][0]\n",
    "demand = {act: 1}\n",
    "method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "\n",
    "# Define some variables\n",
    "num_params = 10000\n",
    "num_influential = num_params // 100\n",
    "iterations_validation = 500\n",
    "write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "\n",
    "model = LCAModel(demand, method, write_dir, num_params=num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.static_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.sensitivity_analysis.gradient_boosting import xgboost_scores\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.utils import write_pickle, read_hdf5_array, write_hdf5_array, read_pickle\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_worker(iterations_current, seed):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    num_params = 10000\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    stability_dir = write_dir / \"stability_intermediate_xgboostGsaN400D6E10S60\"\n",
    "    \n",
    "    filepath_S = stability_dir / \"S.step{}.seed{}.pickle\".format(iterations_current, seed)\n",
    "    if not filepath_S.exists():\n",
    "        filepath_X = stability_dir / \"X.step{}.seed{}.hdf5\".format(iterations_current, seed)\n",
    "        filepath_Y = stability_dir / \"Y.step{}.seed{}.hdf5\".format(iterations_current, seed)\n",
    "        num_boost_round = 400\n",
    "        tuning_parameters = {\n",
    "            \"max_depth\": 6,\n",
    "            \"eta\": 0.1,\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"n_jobs\": -1,\n",
    "            \"refresh_leaf\": True,\n",
    "            \"subsample\": 0.6,\n",
    "            \"min_child_weight\": 0.5,\n",
    "        }\n",
    "        S_dict = xgboost_scores(\n",
    "            filepath_Y,\n",
    "            filepath_X,\n",
    "            iterations_current,\n",
    "            tuning_parameters=tuning_parameters,\n",
    "            train_test_ratio=0.8,\n",
    "            num_boost_round=num_boost_round,\n",
    "            xgb_model=None,\n",
    "        )\n",
    "\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "        return S_dict\n",
    "    else:\n",
    "        print(\"{} already exists\".format(filepath_S))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "#     )\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act[\"name\"]][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "\n",
    "    # Define some variables\n",
    "    num_params = 10000\n",
    "    num_influential = num_params // 100\n",
    "    iterations_validation = 500\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    validation_seed = 7043\n",
    "    fig_format = [\"html\", \"pickle\"]\n",
    "\n",
    "    iterations = 2 * num_params\n",
    "    \n",
    "    num_boost_round = 400\n",
    "    tuning_parameters = {\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": 0.1,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"refresh_leaf\": True,\n",
    "        \"subsample\": 0.6,\n",
    "        \"min_child_weight\": 0.5,\n",
    "    }\n",
    "    gsa = GradientBoosting(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        seed=gsa_seed,\n",
    "        tuning_parameters=tuning_parameters,\n",
    "        num_boost_round=num_boost_round,\n",
    "        xgb_model=None,\n",
    "    )\n",
    "    \n",
    "    conv = Convergence(\n",
    "        gsa.filepath_Y,\n",
    "        gsa.num_params,\n",
    "        gsa.generate_gsa_indices,\n",
    "        gsa.gsa_label,\n",
    "        write_dir,\n",
    "        num_steps=25,\n",
    "    )\n",
    "    num_bootstrap = 10\n",
    "    np.random.seed(gsa_seed)\n",
    "    stability_seeds = np.random.randint(\n",
    "        low=0,\n",
    "        high=2147483647,\n",
    "        size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    "    )\n",
    "    \n",
    "    stability_dir = write_dir / \"stability_intermediate_{}\".format(gsa.gsa_label)\n",
    "    stability_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     X_rescaled = read_hdf5_array(gsa.filepath_X_rescaled)\n",
    "#     Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "    \n",
    "#     for i, iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "#         print(iterations_current)\n",
    "#         for seed in stability_seeds[i,:]:\n",
    "#             np.random.seed(seed)\n",
    "#             r = np.random.choice(np.arange(iterations), size=iterations_current, replace=False)\n",
    "#             r.sort()\n",
    "#             assert len(set(r)) == len(r)\n",
    "#             Xcurrent = X_rescaled[r,:]\n",
    "#             Ycurrent = Y[r]\n",
    "#             fp_X = stability_dir / \"X.step{}.seed{}.hdf5\".format(iterations_current, seed)\n",
    "#             fp_Y = stability_dir / \"Y.step{}.seed{}.hdf5\".format(iterations_current, seed)\n",
    "#             write_hdf5_array(Xcurrent, fp_X)\n",
    "#             write_hdf5_array(Ycurrent, fp_Y)\n",
    "    model_evals = []\n",
    "    model_evals_all = []\n",
    "    task_per_worker = dask.delayed(compute_per_worker)\n",
    "    for i, iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "        if i%6==0 and i>0:\n",
    "            print(len(model_evals))\n",
    "            model_evals_all.append(model_evals)\n",
    "            model_evals = []\n",
    "        for seed in stability_seeds[i,:]:\n",
    "            model_eval = task_per_worker(iterations_current, seed)\n",
    "            model_evals.append(model_eval)\n",
    "        \n",
    "    model_evals_all.append(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #Test\n",
    "# compute_per_worker(800, 449190993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for model_evals in model_evals_all:\n",
    "    print(len(model_evals))\n",
    "    dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsa-dev]",
   "language": "python",
   "name": "conda-env-gsa-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
