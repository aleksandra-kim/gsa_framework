{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gsa_framework.test_functions import Morris4\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.validation import Validation\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "\n",
    "    # 1. Models\n",
    "    num_params = 10000\n",
    "    num_influential = num_params // 100\n",
    "    iterations_validation = 2000\n",
    "    write_dir = path_base / \"{}_morris4\".format(num_params)\n",
    "    model = Morris4(num_params=num_params, num_influential=num_influential)\n",
    "    gsa_seed = 3407\n",
    "    validation_seed = 7043\n",
    "    num_influential_validation = 2*num_influential\n",
    "\n",
    "    fig_format = [\"pickle\"]  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "    # TODO Choose which GSA to perform\n",
    "    flag_sobol = 0\n",
    "    flag_correlation = 0\n",
    "    flag_xgboost = 0\n",
    "    flag_delta = 1\n",
    "\n",
    "    if flag_sobol:\n",
    "        iterations = 100 * num_params\n",
    "        gsa = SaltelliSobol(iterations=iterations, model=model, write_dir=write_dir)\n",
    "        # S_dict = gsa.generate_gsa_indices()\n",
    "        S_dict = gsa.perform_gsa()\n",
    "        first = S_dict[\"First order\"]\n",
    "        total = S_dict[\"Total order\"]\n",
    "#         gsa.plot_sa_results(\n",
    "#             S_dict,\n",
    "#             S_dict_analytical=model.S_dict_analytical,\n",
    "#             fig_format=fig_format,\n",
    "#         )\n",
    "\n",
    "        t0 = time.time()\n",
    "        val = Validation(\n",
    "            model=model,\n",
    "            iterations=iterations_validation,\n",
    "            seed=validation_seed,\n",
    "            default_x_rescaled=None,\n",
    "            write_dir=write_dir,\n",
    "        )\n",
    "        tag = \"TotalIndex\"\n",
    "        influential_Y = val.get_influential_Y_from_gsa(total, num_influential_validation, tag=tag)\n",
    "        t1 = time.time()\n",
    "        print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "        val.plot_histogram_Y_all_Y_inf(\n",
    "            influential_Y, num_influential_validation, tag=tag, fig_format=fig_format\n",
    "        )\n",
    "        \n",
    "#         conv = Convergence(\n",
    "#             gsa.filepath_Y,\n",
    "#             gsa.num_params,\n",
    "#             gsa.generate_gsa_indices,\n",
    "#             gsa.gsa_label,\n",
    "#             write_dir,\n",
    "#             num_steps=100,\n",
    "#         )\n",
    "#         conv.run_convergence(parameter_inds=parameter_inds, fig_format=fig_format)\n",
    "\n",
    "    if flag_correlation:\n",
    "        iterations = 4 * num_params\n",
    "        gsa = CorrelationCoefficients(\n",
    "            iterations=iterations,\n",
    "            model=model,\n",
    "            write_dir=write_dir,\n",
    "            seed=gsa_seed,\n",
    "        )\n",
    "        S_dict = gsa.perform_gsa()\n",
    "        pearson = S_dict[\"pearson\"]\n",
    "        spearman = S_dict[\"spearman\"]\n",
    "#         gsa.plot_sa_results(S_dict, S_boolean=model.S_boolean, fig_format=fig_format)\n",
    "\n",
    "        t0 = time.time()\n",
    "        val = Validation(\n",
    "            model=model,\n",
    "            iterations=iterations_validation,\n",
    "            seed=validation_seed,\n",
    "            default_x_rescaled=None,\n",
    "            write_dir=write_dir,\n",
    "        )\n",
    "        tag = \"SpearmanIndex\"\n",
    "        influential_Y = val.get_influential_Y_from_gsa(\n",
    "            spearman, num_influential_validation, tag=tag\n",
    "        )\n",
    "        t1 = time.time()\n",
    "        print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "        val.plot_histogram_Y_all_Y_inf(\n",
    "            influential_Y, num_influential_validation, tag=tag, fig_format=fig_format\n",
    "        )\n",
    "\n",
    "#         conv = Convergence(\n",
    "#             gsa.filepath_Y,\n",
    "#             gsa.num_params,\n",
    "#             gsa.generate_gsa_indices,\n",
    "#             gsa.gsa_label,\n",
    "#             write_dir,\n",
    "#             num_steps=100,\n",
    "#         )\n",
    "#         conv.run_convergence(\n",
    "#             parameter_inds=parameter_inds,\n",
    "#             fig_format=fig_format,\n",
    "#         )\n",
    "\n",
    "    if flag_delta:\n",
    "        iterations = 8 * num_params\n",
    "        num_resamples = 0\n",
    "        gsa = DeltaMoment(\n",
    "            iterations=iterations,\n",
    "            model=model,\n",
    "            write_dir=write_dir,\n",
    "            num_resamples=num_resamples,\n",
    "            seed=gsa_seed,\n",
    "        )\n",
    "        S_dict = gsa.perform_gsa()\n",
    "        S_dict.pop('delta_conf')\n",
    "        delta = S_dict['delta']\n",
    "        gsa.plot_sa_results(\n",
    "            S_dict,\n",
    "            S_boolean=model.S_boolean,\n",
    "            fig_format=fig_format,\n",
    "        )\n",
    "        t0 = time.time()\n",
    "        val = Validation(\n",
    "            model=model,\n",
    "            iterations=iterations_validation,\n",
    "            seed=validation_seed,\n",
    "            default_x_rescaled=None,\n",
    "            write_dir=write_dir,\n",
    "        )\n",
    "        tag = \"DeltaIndex\"\n",
    "        influential_Y = val.get_influential_Y_from_gsa(\n",
    "            delta, num_influential_validation, tag=tag\n",
    "        )\n",
    "        t1 = time.time()\n",
    "        print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "        val.plot_histogram_Y_all_Y_inf(\n",
    "            influential_Y, num_influential_validation, tag=tag, fig_format=fig_format\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    if flag_xgboost:\n",
    "        if num_params == 1000:\n",
    "            num_boost_round = 300\n",
    "            tuning_parameters = {\n",
    "                \"max_depth\": 2,  # higher than 10 is definitely not good\n",
    "                \"eta\": 0.25,\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"n_jobs\": -1,\n",
    "                \"refresh_leaf\": True,\n",
    "                \"subsample\": 0.35,\n",
    "                \"min_child_weight\": 0.5,\n",
    "            }\n",
    "        elif num_params == 5000:\n",
    "            num_boost_round = 300\n",
    "            tuning_parameters = {\n",
    "                \"max_depth\": 2,  # higher than 10 is definitely not good\n",
    "                \"eta\": 0.25,\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"n_jobs\": -1,\n",
    "                \"refresh_leaf\": True,\n",
    "                \"subsample\": 0.65,\n",
    "                \"min_child_weight\": 0.5,\n",
    "            }\n",
    "        elif num_params == 10000:\n",
    "            num_boost_round = 300\n",
    "            tuning_parameters = {\n",
    "                \"max_depth\": 2,  # higher than 10 is definitely not good\n",
    "                \"eta\": 0.25,\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"n_jobs\": -1,\n",
    "                \"refresh_leaf\": True,\n",
    "                \"subsample\": 0.65,\n",
    "                \"min_child_weight\": 0.5,\n",
    "            }\n",
    "        iterations = 10000\n",
    "        gsa = GradientBoosting(\n",
    "            iterations=iterations,\n",
    "            model=model,\n",
    "            write_dir=write_dir,\n",
    "            seed=gsa_seed,\n",
    "            tuning_parameters=tuning_parameters,\n",
    "            num_boost_round=num_boost_round,\n",
    "            xgb_model=None,\n",
    "        )\n",
    "        S_dict, r2, ev = gsa.perform_gsa(flag_save_S_dict=True, return_stats=True)\n",
    "        print(r2, ev)\n",
    "        # fscores = S_dict[\"fscores\"]\n",
    "        # gsa.plot_sa_results(\n",
    "        #     S_dict,\n",
    "        #     S_boolean=model.S_boolean,\n",
    "        #     fig_format=fig_format,\n",
    "        # )\n",
    "        #\n",
    "        # t0 = time.time()\n",
    "        # val = Validation(\n",
    "        #     model=model,\n",
    "        #     iterations=iterations_validation,\n",
    "        #     seed=validation_seed,\n",
    "        #     default_x_rescaled=None,\n",
    "        #     write_dir=write_dir,\n",
    "        # )\n",
    "        # tag = \"FscoresIndex\"\n",
    "        # influential_Y = val.get_influential_Y_from_gsa(\n",
    "        #     fscores, num_influential_validation, tag=tag\n",
    "        # )\n",
    "        # t1 = time.time()\n",
    "        # print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "        # val.plot_histogram_Y_all_Y_inf(\n",
    "        #     influential_Y, num_influential_validation, tag=tag, fig_format=fig_format\n",
    "        # )\n",
    "\n",
    "        # conv = Convergence(\n",
    "        #     gsa.filepath_Y,\n",
    "        #     gsa.num_params,\n",
    "        #     gsa.generate_gsa_indices,\n",
    "        #     gsa.gsa_label,\n",
    "        #     write_dir,\n",
    "        #     num_steps=100,\n",
    "        # )\n",
    "        # conv.run_convergence(\n",
    "        #     parameter_inds=parameter_inds,\n",
    "        #     fig_format=fig_format,\n",
    "        # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.sensitivity_analysis.correlations import corrcoef_parallel_stability_spearman\n",
    "from gsa_framework.test_functions import Morris4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "# read X and Y\n",
    "num_params = 1000\n",
    "num_influential = num_params // 100\n",
    "write_dir = path_base / \"{}_morris4\".format(num_params)\n",
    "model = Morris4(num_params=num_params, num_influential=num_influential)\n",
    "gsa_seed = 3407\n",
    "fig_format = [\"pickle\"]  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "iter_corr = 4*num_params\n",
    "gsa = CorrelationCoefficients(\n",
    "    iterations=iter_corr,\n",
    "    model=model,\n",
    "    write_dir=write_dir,\n",
    "    seed=gsa_seed,\n",
    ")\n",
    "\n",
    "X_rescaled = read_hdf5_array(gsa.filepath_X_rescaled)\n",
    "Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "# Convergence class\n",
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "write_dir_stability = gsa.write_dir / 'stability_intermediate_{}'.format(gsa.gsa_label)\n",
    "write_dir_stability.mkdir(parents=True, exist_ok=True)\n",
    "# Generate random seeds\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "    gsa.gsa_label, gsa.sampling_label, gsa.iterations, conv.iterations_step, num_bootstrap, gsa.seed,\n",
    ")\n",
    "filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "if filepath_S.exists():\n",
    "    print(\"--> {} already exists\".format(filename_S))\n",
    "    S_dict_stability = read_pickle(filepath_S)\n",
    "else:\n",
    "    S_dict_stability = {}\n",
    "    for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "        S_array = np.zeros([0,num_params])\n",
    "        print(\"{}\".format(iterations_current))\n",
    "        filename_S_current = \"S.{}Step{}.{}.{}.pickle\".format(iterations_current,conv.iterations_step,num_bootstrap,gsa.seed)\n",
    "        filepath_S_current = write_dir_stability / filename_S_current\n",
    "        if filepath_S_current.exists():\n",
    "            print(\"--> {} already exists\".format(filename_S_current))\n",
    "            S_dict = read_pickle(filepath_S_current)\n",
    "        else:\n",
    "            for j in range(num_bootstrap):\n",
    "                stability_seed = stability_seeds[i,j]\n",
    "                np.random.seed(stability_seed)\n",
    "                choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "                Y_current = Y[choice]\n",
    "                X_current = X_rescaled[choice,:]\n",
    "                S_current = corrcoef_parallel_stability_spearman(Y_current, X_current)['spearman']\n",
    "                S_array = np.vstack([S_array, S_current])\n",
    "            S_dict = {iterations_current: {\"spearman\": S_array}}\n",
    "            write_pickle(S_dict, filepath_S_current)\n",
    "        S_dict_stability.update(S_dict)\n",
    "    write_pickle(S_dict_stability, filepath_S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from pathlib import Path\n",
    "import os\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_pc = \"merlin_paper_gsa\"\n",
    "if 'merlin' in which_pc:\n",
    "    path_dask_logs = Path('/data/user/kim_a/dask_logs')\n",
    "    path_dask_logs.mkdir(parents=True, exist_ok=True)\n",
    "    cluster = SLURMCluster(cores     = 8,\n",
    "                           memory    =\"40GB\", \n",
    "                           walltime  = '00:59:00',\n",
    "                           interface ='ib0',\n",
    "                           local_directory = path_dask_logs.as_posix(),\n",
    "                           log_directory   = path_dask_logs.as_posix(),\n",
    "                           queue=\"hourly\",\n",
    "                           ) \n",
    "elif 'local' in which_pc:\n",
    "    cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 60\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability delta moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.test_functions import Morris4\n",
    "from gsa_framework.sensitivity_analysis.delta_moment import delta_moment_stability\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.convergence import Convergence\n",
    "from gsa_framework.utils import *\n",
    "from gsa_framework.sampling.get_samples import latin_hypercube_samples\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_worker_delt(num_params, iterations_current, stability_seed):\n",
    "    iter_delt = 8*num_params\n",
    "    gsa_delt = setup_delt(num_params, iter_delt, setup_morris4_model)\n",
    "    filepath_Y = gsa_delt.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    Y = read_pickle(filepath_Y).flatten()\n",
    "    X = latin_hypercube_samples(gsa_delt.iterations, gsa_delt.num_params, seed=gsa_delt.seed)\n",
    "    np.random.seed(stability_seed)\n",
    "    choice = np.random.choice(np.arange(gsa_delt.iterations), iterations_current, replace=False)\n",
    "    Xr = gsa_delt.model.rescale(X[choice, :])\n",
    "    del X\n",
    "    filepath_S = gsa_delt.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    if not filepath_S.exists():\n",
    "        S_dict = delta_moment_stability(\n",
    "            Y, Xr, num_resamples=gsa_delt.num_resamples, seed=stability_seed\n",
    "        )\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "    else:\n",
    "        print(\"{} already exists\".format(filepath_S.name))\n",
    "        S_dict = read_pickle(filepath_S)\n",
    "    \n",
    "    return S_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 1000\n",
    "iter_delt = 8*num_params\n",
    "gsa_delt = setup_delt(num_params, iter_delt, setup_morris4_model)\n",
    "\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "option = 'delta'\n",
    "if option=='delta':\n",
    "    gsa = gsa_delt\n",
    "    compute_per_worker = compute_per_worker_delt\n",
    "elif option=='xgboost':\n",
    "    gsa = gsa_xgbo\n",
    "    compute_per_worker = compute_per_worker_xgbo\n",
    "\n",
    "task_per_worker = dask.delayed(compute_per_worker)\n",
    "# task_per_worker = compute_per_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir_convergence,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")\n",
    "\n",
    "Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "\n",
    "num_times = n_workers // num_bootstrap\n",
    "model_evals = []\n",
    "i = 0\n",
    "for i_iter in range(len(conv.iterations_for_convergence)//num_times+1):\n",
    "    iterations_current_multiple = conv.iterations_for_convergence[i_iter*num_times:(i_iter+1)*num_times]\n",
    "    model_evals_bootstrap_j_k = []\n",
    "    for iterations_current in iterations_current_multiple:\n",
    "        model_evals_bootstrap_j = []\n",
    "        for j in range(num_bootstrap):\n",
    "            stability_seed = stability_seeds[i,j]\n",
    "            np.random.seed(stability_seed)\n",
    "            choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "            # Write Y\n",
    "            filepath_Y_ij = gsa.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "            if not filepath_Y_ij.exists():\n",
    "                Y_ij = Y[choice]\n",
    "                write_pickle(Y_ij, filepath_Y_ij)\n",
    "            else:\n",
    "    #             print(\"{} already exists\".format(filepath_Y_ij.name))  \n",
    "                pass\n",
    "            # Model evals\n",
    "            filepath_S_current = gsa.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "            if not filepath_S_current.exists():\n",
    "                model_eval = task_per_worker(num_params, iterations_current, stability_seed)\n",
    "                model_evals_bootstrap_j.append(model_eval)\n",
    "        model_evals_bootstrap_j_k += model_evals_bootstrap_j\n",
    "        i += 1\n",
    "    if len(model_evals_bootstrap_j_k) > 0:\n",
    "        model_evals.append(model_evals_bootstrap_j_k)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "# model_evals = []\n",
    "# for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "#     model_evals_bootstrap_j = []\n",
    "#     for j in range(num_bootstrap):\n",
    "#         stability_seed = stability_seeds[i,j]\n",
    "#         np.random.seed(stability_seed)\n",
    "#         choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "#         # Write Y\n",
    "#         filepath_Y_ij = gsa.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "#         if not filepath_Y_ij.exists():\n",
    "#             Y_ij = Y[choice]\n",
    "#             write_pickle(Y_ij, filepath_Y_ij)\n",
    "#         else:\n",
    "# #             print(\"{} already exists\".format(filepath_Y_ij.name))  \n",
    "#             pass\n",
    "#         # Model evals\n",
    "#         model_eval = task_per_worker(num_params, iterations_current, stability_seed)\n",
    "#         model_evals_bootstrap_j.append(model_eval)\n",
    "#     model_evals.append(model_evals_bootstrap_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i,model_evals_bootstrap_j_k in enumerate(model_evals):\n",
    "    print(i)\n",
    "    dask.compute(model_evals_bootstrap_j_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i,model_evals_bootstrap_j in enumerate(model_evals):\n",
    "#     print(i)\n",
    "#     dask.compute(model_evals_bootstrap_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "def create_stability_dict(num_params, iterations_for_convergence, stability_seeds):\n",
    "    iter_delt = 8*num_params\n",
    "    gsa_delt = setup_delt(num_params, iter_delt, setup_morris4_model)\n",
    "    iterations_step = iterations_for_convergence[1] - iterations_for_convergence[0]\n",
    "    num_bootstrap = stability_seeds.shape[1]\n",
    "    filename_S_stability = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "    gsa_delt.gsa_label, gsa_delt.sampling_label, gsa_delt.iterations, iterations_step, num_bootstrap, gsa_delt.seed,\n",
    "    )\n",
    "    filepath_S_stability = gsa_delt.write_dir / 'arrays' / filename_S_stability\n",
    "    if filepath_S_stability.exists():\n",
    "        print(\"{} already exists\".format(filepath_S_stability.name))  \n",
    "        S_dict = read_pickle(filepath_S_stability)\n",
    "    else:\n",
    "        S_dict = {}\n",
    "        for i,iterations_current in enumerate(iterations_for_convergence):\n",
    "            S_array = np.zeros((0,num_params))\n",
    "            for j in range(num_bootstrap):\n",
    "                stability_seed = stability_seeds[i,j]\n",
    "                filepath_S = \\\n",
    "                gsa_delt.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "                if not filepath_S.exists():\n",
    "                    print(\"{} does not exist\".format(filepath_S.name))\n",
    "                    return\n",
    "                else:\n",
    "                    S_current = read_pickle(filepath_S)\n",
    "                    S_array = np.vstack([S_array, S_current['delta']])\n",
    "            S_dict[iterations_current] = {\"delta\": S_array}\n",
    "        write_pickle(S_dict, filepath_S_stability)\n",
    "    return S_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "S_dict = create_stability_dict(num_params, conv.iterations_for_convergence, stability_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.test_functions import Morris4\n",
    "from gsa_framework.sensitivity_analysis.delta_moment import delta_moment_parallel_stability\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "# read X and Y\n",
    "num_params = 5000\n",
    "num_influential = num_params // 100\n",
    "write_dir = path_base / \"{}_morris4\".format(num_params)\n",
    "model = Morris4(num_params=num_params, num_influential=num_influential)\n",
    "gsa_seed = 3407\n",
    "fig_format = [\"pickle\"]  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "iter_delt = 8*num_params\n",
    "num_resamples = 1\n",
    "gsa = DeltaMoment(\n",
    "    iterations=iter_delt,\n",
    "    model=model,\n",
    "    write_dir=write_dir,\n",
    "    num_resamples=num_resamples,\n",
    "    seed=gsa_seed,\n",
    ")\n",
    "\n",
    "X_rescaled = read_hdf5_array(gsa.filepath_X_rescaled)\n",
    "Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "# Convergence class\n",
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "write_dir_stability = gsa.write_dir / 'stability_intermediate_{}'.format(gsa.gsa_label)\n",
    "write_dir_stability.mkdir(parents=True, exist_ok=True)\n",
    "# Generate random seeds\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "        gsa.gsa_label, gsa.sampling_label, gsa.iterations, conv.iterations_step, num_bootstrap, gsa.seed,\n",
    "    )\n",
    "    filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "    if filepath_S.exists():\n",
    "        print(\"--> {} already exists\".format(filename_S))\n",
    "        S_dict_stability = read_pickle(filepath_S)\n",
    "    else:\n",
    "        S_dict_stability = {}\n",
    "        for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "            S_array = np.zeros([0,num_params])\n",
    "            print(\"{}\".format(iterations_current))\n",
    "            filename_S_current = \"S.{}Step{}.{}.{}.pickle\".format(iterations_current,conv.iterations_step,num_bootstrap,gsa.seed)\n",
    "            filepath_S_current = write_dir_stability / filename_S_current\n",
    "            if filepath_S_current.exists():\n",
    "                print(\"--> {} already exists\".format(filename_S_current))\n",
    "                S_dict = read_pickle(filepath_S_current)\n",
    "            else:\n",
    "                for j in range(num_bootstrap):\n",
    "                    stability_seed = stability_seeds[i,j]\n",
    "                    np.random.seed(stability_seed)\n",
    "                    choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "                    Y_current = Y[choice]\n",
    "                    X_current = X_rescaled[choice,:]\n",
    "                    S_current = delta_moment_parallel_stability(Y_current, X_current, num_resamples=num_resamples)\n",
    "                    S_array = np.vstack([S_array, S_current['delta']])\n",
    "                S_dict = {iterations_current: {\"delta\": S_array}}\n",
    "                write_pickle(S_dict, filepath_S_current)\n",
    "            S_dict_stability.update(S_dict)\n",
    "        write_pickle(S_dict_stability, filepath_S)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze bootstrap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.test_functions import Morris4\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.validation import Validation\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import time\n",
    "from setups_paper_gwp import *\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_params = 5000\n",
    "    iter_delt = 8*num_params\n",
    "    gsa = setup_delt(num_params, iter_delt, setup_morris4_model)\n",
    "    num_steps = 50\n",
    "    num_bootstrap = 60\n",
    "\n",
    "    # Convergence class\n",
    "    conv = Convergence(\n",
    "        gsa.filepath_Y,\n",
    "        gsa.num_params,\n",
    "        gsa.generate_gsa_indices,\n",
    "        gsa.gsa_label,\n",
    "        gsa.write_dir,\n",
    "        num_steps=num_steps,\n",
    "    )\n",
    "\n",
    "    filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "        gsa.gsa_label, gsa.sampling_label, gsa.iterations, conv.iterations_step, num_bootstrap, gsa.seed,\n",
    "    )\n",
    "    filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "    S_dict = read_pickle(filepath_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_bootstrap = S_dict[39200]['delta'][:,2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = S_bootstrap\n",
    "num_bins = 14\n",
    "bin_min, bin_max = min(Y), max(Y)\n",
    "bins_ = np.linspace(bin_min, bin_max, num_bins, endpoint=True)\n",
    "freq1, bins1 = np.histogram(Y, bins=bins_)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=bins1,\n",
    "        y=freq1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsa-dev]",
   "language": "python",
   "name": "conda-env-gsa-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
