{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gsa_framework.models.test_functions import Morris4\n",
    "from gsa_framework.sensitivity_analysis.correlations import Correlations\n",
    "from gsa_framework.sensitivity_analysis.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.sensitivity_analysis.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.sensitivity_analysis.delta import Delta\n",
    "from gsa_framework.convergence_robustness_validation import Validation\n",
    "from gsa_framework.convergence_robustness_validation import Convergence\n",
    "from pathlib import Path\n",
    "import time\n",
    "from gsa_framework.utils import read_hdf5_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "\n",
    "    # 1. Models\n",
    "    num_params = 5000\n",
    "    num_influential = num_params // 100\n",
    "    iterations_validation = 2000\n",
    "    write_dir = path_base / \"{}_morris4\".format(num_params)\n",
    "    model = Morris4(num_params=num_params, num_influential=num_influential)\n",
    "#     gsa_seed = 3407\n",
    "    gsa_seed = 6000814\n",
    "    validation_seed = 7043\n",
    "    num_influential_validation = 2*num_influential\n",
    "\n",
    "    fig_format = [\"pickle\"]  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "    # TODO Choose which GSA to perform\n",
    "    flag_sobol = 0\n",
    "    flag_correlation = 1\n",
    "    flag_xgboost = 0\n",
    "    flag_delta = 0\n",
    "\n",
    "    if flag_sobol:\n",
    "        iterations = 100 * num_params\n",
    "        gsa = SaltelliSobol(iterations=iterations, model=model, write_dir=write_dir)\n",
    "        # S_dict = gsa.generate_gsa_indices()\n",
    "        S_dict = gsa.perform_gsa()\n",
    "        first = S_dict[\"First order\"]\n",
    "        total = S_dict[\"Total order\"]\n",
    "#         gsa.plot_sa_results(\n",
    "#             S_dict,\n",
    "#             S_dict_analytical=model.S_dict_analytical,\n",
    "#             fig_format=fig_format,\n",
    "#         )\n",
    "\n",
    "        t0 = time.time()\n",
    "        val = Validation(\n",
    "            model=model,\n",
    "            iterations=iterations_validation,\n",
    "            seed=validation_seed,\n",
    "            default_x_rescaled=None,\n",
    "            write_dir=write_dir,\n",
    "        )\n",
    "        tag = \"TotalIndex\"\n",
    "        influential_Y = val.get_influential_Y_from_gsa(total, num_influential_validation, tag=tag)\n",
    "        t1 = time.time()\n",
    "        print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "        val.plot_histogram_Y_all_Y_inf(\n",
    "            influential_Y, num_influential_validation, tag=tag, fig_format=fig_format\n",
    "        )\n",
    "        \n",
    "#         conv = Convergence(\n",
    "#             gsa.filepath_Y,\n",
    "#             gsa.num_params,\n",
    "#             gsa.generate_gsa_indices,\n",
    "#             gsa.gsa_label,\n",
    "#             write_dir,\n",
    "#             num_steps=100,\n",
    "#         )\n",
    "#         conv.run_convergence(parameter_inds=parameter_inds, fig_format=fig_format)\n",
    "\n",
    "    if flag_correlation:\n",
    "        iterations = 4 * num_params\n",
    "        gsa = Correlations(\n",
    "            iterations=iterations,\n",
    "            model=model,\n",
    "            write_dir=write_dir,\n",
    "            seed=gsa_seed,\n",
    "        )\n",
    "        S_dict = gsa.perform_gsa()\n",
    "        pearson = S_dict[\"pearson\"]\n",
    "        spearman = S_dict[\"spearman\"]\n",
    "#         gsa.plot_sa_results(S_dict, S_boolean=model.S_boolean, fig_format=fig_format)\n",
    "\n",
    "#         t0 = time.time()\n",
    "#         val = Validation(\n",
    "#             model=model,\n",
    "#             iterations=iterations_validation,\n",
    "#             seed=validation_seed,\n",
    "#             default_x_rescaled=None,\n",
    "#             write_dir=write_dir,\n",
    "#         )\n",
    "#         tag = \"SpearmanIndex\"\n",
    "#         influential_Y = val.get_influential_Y_from_gsa(\n",
    "#             spearman, num_influential_validation, tag=tag\n",
    "#         )\n",
    "#         t1 = time.time()\n",
    "#         print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "#         val.plot_histogram_Y_all_Y_inf(\n",
    "#             influential_Y, num_influential_validation, tag=tag, fig_format=fig_format\n",
    "#         )\n",
    "\n",
    "#         conv = Convergence(\n",
    "#             gsa.filepath_Y,\n",
    "#             gsa.num_params,\n",
    "#             gsa.generate_gsa_indices,\n",
    "#             gsa.gsa_label,\n",
    "#             write_dir,\n",
    "#             num_steps=100,\n",
    "#         )\n",
    "#         conv.run_convergence(\n",
    "#             parameter_inds=parameter_inds,\n",
    "#             fig_format=fig_format,\n",
    "#         )\n",
    "\n",
    "    if flag_delta:\n",
    "        iterations = 8 * num_params\n",
    "        num_resamples = 0\n",
    "        gsa = DeltaMoment(\n",
    "            iterations=iterations,\n",
    "            model=model,\n",
    "            write_dir=write_dir,\n",
    "            num_resamples=num_resamples,\n",
    "            seed=gsa_seed,\n",
    "        )\n",
    "        S_dict = gsa.perform_gsa()\n",
    "        S_dict.pop('delta_conf')\n",
    "        delta = S_dict['delta']\n",
    "        gsa.plot_sa_results(\n",
    "            S_dict,\n",
    "            S_boolean=model.S_boolean,\n",
    "            fig_format=fig_format,\n",
    "        )\n",
    "        t0 = time.time()\n",
    "        val = Validation(\n",
    "            model=model,\n",
    "            iterations=iterations_validation,\n",
    "            seed=validation_seed,\n",
    "            default_x_rescaled=None,\n",
    "            write_dir=write_dir,\n",
    "        )\n",
    "        tag = \"DeltaIndex\"\n",
    "        influential_Y = val.get_influential_Y_from_gsa(\n",
    "            delta, num_influential_validation, tag=tag\n",
    "        )\n",
    "        t1 = time.time()\n",
    "        print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "        val.plot_histogram_Y_all_Y_inf(\n",
    "            influential_Y, num_influential_validation, tag=tag, fig_format=fig_format\n",
    "        )\n",
    "        \n",
    "    if flag_xgboost:\n",
    "        iterations = 2 * num_params\n",
    "        test_size = 0.2\n",
    "        if num_params == 1000:\n",
    "            tuning_parameters = dict(\n",
    "                learning_rate=0.1,\n",
    "                gamma=0,\n",
    "                min_child_weight=30,\n",
    "                max_depth=2,\n",
    "                reg_lambda=10,\n",
    "                reg_alpha=0,\n",
    "                n_estimators=500,\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.3,\n",
    "            )\n",
    "        gsa = GradientBoosting(\n",
    "            iterations=iterations,\n",
    "            model=model,\n",
    "            write_dir=write_dir,\n",
    "            seed=gsa_seed,\n",
    "            tuning_parameters=tuning_parameters,\n",
    "            test_size=test_size,\n",
    "            xgb_model=None,\n",
    "        )\n",
    "\n",
    "        S_dict = gsa.perform_gsa(flag_save_S_dict=True)\n",
    "        print(S_dict[\"stat.r2\"], S_dict[\"stat.explained_variance\"])\n",
    "        gsa.plot_sa_results(\n",
    "            {\"fscores\": S_dict[\"fscores\"]},\n",
    "            fig_format=fig_format,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.sensitivity_analysis.correlations import corrcoef_parallel_stability_spearman\n",
    "from gsa_framework.models.test_functions import Morris4\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "# read X and Y\n",
    "num_params = 5000\n",
    "num_influential = num_params // 100\n",
    "write_dir = path_base / \"{}_morris4\".format(num_params)\n",
    "model = Morris4(num_params=num_params, num_influential=num_influential)\n",
    "# gsa_seed = 3407\n",
    "gsa_seed = 6000814\n",
    "fig_format = [\"pickle\"]  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "iter_corr = 4*num_params\n",
    "gsa = Correlations(\n",
    "    iterations=iter_corr,\n",
    "    model=model,\n",
    "    write_dir=write_dir,\n",
    "    seed=gsa_seed,\n",
    ")\n",
    "\n",
    "X_rescaled = read_hdf5_array(gsa.filepath_X_rescaled)\n",
    "Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "# Convergence class\n",
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "write_dir_stability = gsa.write_dir / 'stability_intermediate_{}'.format(gsa.gsa_label)\n",
    "write_dir_stability.mkdir(parents=True, exist_ok=True)\n",
    "# Generate random seeds\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "    gsa.gsa_label, gsa.sampling_label, gsa.iterations, conv.iterations_step, num_bootstrap, gsa.seed,\n",
    ")\n",
    "filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "if filepath_S.exists():\n",
    "    print(\"--> {} already exists\".format(filename_S))\n",
    "    S_dict_stability = read_pickle(filepath_S)\n",
    "else:\n",
    "    S_dict_stability = {}\n",
    "    for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "        S_array = np.zeros([0,num_params])\n",
    "        print(\"{}\".format(iterations_current))\n",
    "        filename_S_current = \"S.{}Step{}.{}.{}.pickle\".format(iterations_current,conv.iterations_step,num_bootstrap,gsa.seed)\n",
    "        filepath_S_current = write_dir_stability / filename_S_current\n",
    "        if filepath_S_current.exists():\n",
    "            print(\"--> {} already exists\".format(filename_S_current))\n",
    "            S_dict = read_pickle(filepath_S_current)\n",
    "        else:\n",
    "            for j in range(num_bootstrap):\n",
    "                stability_seed = stability_seeds[i,j]\n",
    "                np.random.seed(stability_seed)\n",
    "                choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "                Y_current = Y[choice]\n",
    "                X_current = X_rescaled[choice,:]\n",
    "                S_current = corrcoef_parallel_stability_spearman(Y_current, X_current)['spearman']\n",
    "                S_array = np.vstack([S_array, S_current])\n",
    "            S_dict = {iterations_current: {\"spearman\": S_array}}\n",
    "            write_pickle(S_dict, filepath_S_current)\n",
    "        S_dict_stability.update(S_dict)\n",
    "    write_pickle(S_dict_stability, filepath_S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from pathlib import Path\n",
    "import os\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_pc = \"merlin_paper_gsa\"\n",
    "if 'merlin' in which_pc:\n",
    "    path_dask_logs = Path('/data/user/kim_a/dask_logs')\n",
    "    path_dask_logs.mkdir(parents=True, exist_ok=True)\n",
    "    cluster = SLURMCluster(cores     = 8,\n",
    "                           memory    =\"160GB\", \n",
    "                           walltime  = '23:00:00',\n",
    "                           interface ='ib0',\n",
    "                           local_directory = path_dask_logs.as_posix(),\n",
    "                           log_directory   = path_dask_logs.as_posix(),\n",
    "                           queue=\"daily\",\n",
    "                           ) \n",
    "elif 'local' in which_pc:\n",
    "    cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 60\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability delta and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.models.test_functions import Morris4\n",
    "from gsa_framework.sensitivity_methods.delta import delta_indices_stability\n",
    "from gsa_framework.sensitivity_methods.gradient_boosting import xgboost_indices_stability\n",
    "from gsa_framework.sensitivity_analysis.delta import Delta\n",
    "from gsa_framework.sensitivity_analysis.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.convergence_robustness_validation import Convergence\n",
    "from gsa_framework.utils import *\n",
    "from gsa_framework.sampling.get_samples import latin_hypercube_samples\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "setup_xgbo = setup_xgbo_morris4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_worker_delt(num_params, iterations_current, stability_seed):\n",
    "    iter_delt = 8*num_params\n",
    "    gsa_delt = setup_delt(num_params, iter_delt, setup_morris4_model)\n",
    "    filepath_Y = gsa_delt.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    Y = read_pickle(filepath_Y).flatten()\n",
    "    X = latin_hypercube_samples(gsa_delt.iterations, gsa_delt.num_params, seed=gsa_delt.seed)\n",
    "    np.random.seed(stability_seed)\n",
    "    choice = np.random.choice(np.arange(gsa_delt.iterations), iterations_current, replace=False)\n",
    "    Xr = gsa_delt.model.rescale(X[choice, :])\n",
    "    del X\n",
    "    filepath_S = gsa_delt.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    if not filepath_S.exists():\n",
    "        S_dict = delta_moment_stability(\n",
    "            Y, Xr, num_resamples=gsa_delt.num_resamples, seed=stability_seed\n",
    "        )\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "    else:\n",
    "        print(\"{} already exists\".format(filepath_S.name))\n",
    "        S_dict = read_pickle(filepath_S)\n",
    "    \n",
    "    return S_dict\n",
    "\n",
    "def compute_per_worker_xgbo(num_params, iterations_current, stability_seed):\n",
    "    iter_xgbo =4*num_params\n",
    "    gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_morris4_model, path_base)\n",
    "    filepath_Y = gsa_xgbo.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    Y = read_pickle(filepath_Y).flatten()\n",
    "    np.random.seed(gsa_xgbo.seed)\n",
    "    X = np.random.rand(iter_xgbo, num_params)\n",
    "    np.random.seed(stability_seed)\n",
    "    choice = np.random.choice(np.arange(gsa_xgbo.iterations), iterations_current, replace=True)\n",
    "    Xr = gsa_xgbo.model.rescale(X[choice, :])\n",
    "    del X\n",
    "    filepath_S = gsa_xgbo.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    if not filepath_S.exists():\n",
    "        S_dict = xgboost_scores_stability(\n",
    "            Y, \n",
    "            Xr, \n",
    "            tuning_parameters=gsa_xgbo.tuning_parameters,\n",
    "            test_size=gsa_xgbo.test_size,\n",
    "            xgb_model = gsa_xgbo.xgb_model,\n",
    "        )\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "    else:\n",
    "        print(\"{} already exists\".format(filepath_S.name))\n",
    "        S_dict = read_pickle(filepath_S)\n",
    "    \n",
    "    return S_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 1000\n",
    "iter_delt = 8*num_params\n",
    "iter_xgbo = 4*num_params\n",
    "gsa_delt = setup_delt(num_params, iter_delt, setup_morris4_model, path_base)\n",
    "gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_morris4_model, path_base)\n",
    "\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "option = 'xgboost'\n",
    "if option=='delta':\n",
    "    gsa = gsa_delt\n",
    "    compute_per_worker = compute_per_worker_delt\n",
    "elif option=='xgboost':\n",
    "    gsa = gsa_xgbo\n",
    "    compute_per_worker = compute_per_worker_xgbo\n",
    "\n",
    "task_per_worker = dask.delayed(compute_per_worker)\n",
    "# task_per_worker = compute_per_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa_xgbo.perform_gsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pay attention to replace=True or False!! should be True in the end, but now for all methods but xgboost results are computed with False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir_convergence,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")\n",
    "\n",
    "Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "\n",
    "num_times = n_workers // num_bootstrap\n",
    "model_evals = []\n",
    "i = 0\n",
    "for i_iter in range(len(conv.iterations_for_convergence)//num_times+1):\n",
    "    iterations_current_multiple = conv.iterations_for_convergence[i_iter*num_times:(i_iter+1)*num_times]\n",
    "    model_evals_bootstrap_j_k = []\n",
    "    for iterations_current in iterations_current_multiple:\n",
    "        model_evals_bootstrap_j = []\n",
    "        for j in range(num_bootstrap):\n",
    "            stability_seed = stability_seeds[i,j]\n",
    "            np.random.seed(stability_seed)\n",
    "            choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=True) \n",
    "            # Write Y\n",
    "            filepath_Y_ij = gsa.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "            if not filepath_Y_ij.exists():\n",
    "                Y_ij = Y[choice]\n",
    "                write_pickle(Y_ij, filepath_Y_ij)\n",
    "            else:\n",
    "    #             print(\"{} already exists\".format(filepath_Y_ij.name))  \n",
    "                pass\n",
    "            # Model evals\n",
    "            filepath_S_current = gsa.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "            if not filepath_S_current.exists():\n",
    "                model_eval = task_per_worker(num_params, iterations_current, stability_seed)\n",
    "                model_evals_bootstrap_j.append(model_eval)\n",
    "        model_evals_bootstrap_j_k += model_evals_bootstrap_j\n",
    "        i += 1\n",
    "    if len(model_evals_bootstrap_j_k) > 0:\n",
    "        model_evals.append(model_evals_bootstrap_j_k)\n",
    "        \n",
    "        \n",
    "        \n",
    "# Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "# model_evals = []\n",
    "# for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "#     model_evals_bootstrap_j = []\n",
    "#     for j in range(num_bootstrap):\n",
    "#         stability_seed = stability_seeds[i,j]\n",
    "#         np.random.seed(stability_seed)\n",
    "#         choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "#         # Write Y\n",
    "#         filepath_Y_ij = gsa.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "#         if not filepath_Y_ij.exists():\n",
    "#             Y_ij = Y[choice]\n",
    "#             write_pickle(Y_ij, filepath_Y_ij)\n",
    "#         else:\n",
    "# #             print(\"{} already exists\".format(filepath_Y_ij.name))  \n",
    "#             pass\n",
    "#         # Model evals\n",
    "#         model_eval = task_per_worker(num_params, iterations_current, stability_seed)\n",
    "#         model_evals_bootstrap_j.append(model_eval)\n",
    "#     model_evals.append(model_evals_bootstrap_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i,model_evals_bootstrap_j_k in enumerate(model_evals):\n",
    "    print(i)\n",
    "    dask.compute(model_evals_bootstrap_j_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i,model_evals_bootstrap_j in enumerate(model_evals):\n",
    "#     print(i)\n",
    "#     dask.compute(model_evals_bootstrap_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "def create_stability_dict_delt(num_params, iterations_for_convergence, stability_seeds):\n",
    "    iter_delt = 8*num_params\n",
    "    gsa_delt = setup_delt(num_params, iter_delt, setup_morris4_model, path_base)\n",
    "    iterations_step = iterations_for_convergence[1] - iterations_for_convergence[0]\n",
    "    num_bootstrap = stability_seeds.shape[1]\n",
    "    filename_S_stability = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "    gsa_delt.gsa_label, gsa_delt.sampling_label, gsa_delt.iterations, iterations_step, num_bootstrap, gsa_delt.seed,\n",
    "    )\n",
    "    filepath_S_stability = gsa_delt.write_dir / 'arrays' / filename_S_stability\n",
    "    if filepath_S_stability.exists():\n",
    "        print(\"{} already exists\".format(filepath_S_stability.name))  \n",
    "        S_dict = read_pickle(filepath_S_stability)\n",
    "    else:\n",
    "        S_dict = {}\n",
    "        for i,iterations_current in enumerate(iterations_for_convergence):\n",
    "            S_array = np.zeros((0,num_params))\n",
    "            for j in range(num_bootstrap):\n",
    "                stability_seed = stability_seeds[i,j]\n",
    "                filepath_S = \\\n",
    "                gsa_delt.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "                if not filepath_S.exists():\n",
    "                    print(\"{} does not exist\".format(filepath_S.name))\n",
    "                    return\n",
    "                else:\n",
    "                    S_current = read_pickle(filepath_S)\n",
    "                    S_array = np.vstack([S_array, S_current['delta']])\n",
    "            S_dict[iterations_current] = {\"delta\": S_array}\n",
    "        write_pickle(S_dict, filepath_S_stability)\n",
    "    return S_dict\n",
    "\n",
    "# Collect all results\n",
    "def create_stability_dict_xgbo(num_params, iterations_for_convergence, stability_seeds):\n",
    "    iter_xgbo = 4*num_params\n",
    "    gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_morris4_model, path_base)\n",
    "    iterations_step = iterations_for_convergence[1] - iterations_for_convergence[0]\n",
    "    num_bootstrap = stability_seeds.shape[1]\n",
    "    filename_S_stability = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "    gsa_xgbo.gsa_label, gsa_xgbo.sampling_label, gsa_xgbo.iterations, iterations_step, num_bootstrap, gsa_xgbo.seed,\n",
    "    )\n",
    "    filepath_S_stability = gsa_xgbo.write_dir / 'arrays' / filename_S_stability\n",
    "    if filepath_S_stability.exists():\n",
    "        print(\"{} already exists\".format(filepath_S_stability.name))  \n",
    "        S_dict = read_pickle(filepath_S_stability)\n",
    "    else:\n",
    "        S_dict = {}\n",
    "        for i,iterations_current in enumerate(iterations_for_convergence):\n",
    "            S_dict_arrays = {}\n",
    "            for j in range(num_bootstrap):\n",
    "                stability_seed = stability_seeds[i,j]\n",
    "                filepath_S = \\\n",
    "                gsa_xgbo.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "                if not filepath_S.exists():\n",
    "                    print(\"{} does not exist\".format(filepath_S.name))\n",
    "                    return\n",
    "                else:\n",
    "                    S_current = read_pickle(filepath_S)\n",
    "                    if len(S_dict_arrays) == 0:\n",
    "                        stats = [s for s in S_current.keys() if 'stat.' in s]\n",
    "                        importance_types = [imp for imp in S_current.keys() if 'stat.' not in imp]\n",
    "                        keys = stats + importance_types\n",
    "                        S_dict_arrays = {s: deepcopy(np.zeros((0,1))) for s in stats}\n",
    "                        S_dict_arrays.update(\n",
    "                            {\n",
    "                                imp: deepcopy(np.zeros((0,num_params))) for imp in importance_types\n",
    "                            }\n",
    "                        )\n",
    "                    for k in keys:\n",
    "                        S_dict_arrays[k] = np.vstack([S_dict_arrays[k], S_current[k]])\n",
    "            S_dict[iterations_current] = {k: S_dict_arrays[k] for k in keys}\n",
    "        write_pickle(S_dict, filepath_S_stability)\n",
    "    return S_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "S_dict = create_stability_dict_xgbo(num_params, conv.iterations_for_convergence, stability_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.test_functions import Morris4\n",
    "from gsa_framework.sensitivity_analysis.delta_moment import delta_moment_parallel_stability\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "# read X and Y\n",
    "num_params = 5000\n",
    "num_influential = num_params // 100\n",
    "write_dir = path_base / \"{}_morris4\".format(num_params)\n",
    "model = Morris4(num_params=num_params, num_influential=num_influential)\n",
    "gsa_seed = 3407\n",
    "fig_format = [\"pickle\"]  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "iter_delt = 8*num_params\n",
    "num_resamples = 1\n",
    "gsa = DeltaMoment(\n",
    "    iterations=iter_delt,\n",
    "    model=model,\n",
    "    write_dir=write_dir,\n",
    "    num_resamples=num_resamples,\n",
    "    seed=gsa_seed,\n",
    ")\n",
    "\n",
    "X_rescaled = read_hdf5_array(gsa.filepath_X_rescaled)\n",
    "Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "# Convergence class\n",
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "write_dir_stability = gsa.write_dir / 'stability_intermediate_{}'.format(gsa.gsa_label)\n",
    "write_dir_stability.mkdir(parents=True, exist_ok=True)\n",
    "# Generate random seeds\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "        gsa.gsa_label, gsa.sampling_label, gsa.iterations, conv.iterations_step, num_bootstrap, gsa.seed,\n",
    "    )\n",
    "    filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "    if filepath_S.exists():\n",
    "        print(\"--> {} already exists\".format(filename_S))\n",
    "        S_dict_stability = read_pickle(filepath_S)\n",
    "    else:\n",
    "        S_dict_stability = {}\n",
    "        for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "            S_array = np.zeros([0,num_params])\n",
    "            print(\"{}\".format(iterations_current))\n",
    "            filename_S_current = \"S.{}Step{}.{}.{}.pickle\".format(iterations_current,conv.iterations_step,num_bootstrap,gsa.seed)\n",
    "            filepath_S_current = write_dir_stability / filename_S_current\n",
    "            if filepath_S_current.exists():\n",
    "                print(\"--> {} already exists\".format(filename_S_current))\n",
    "                S_dict = read_pickle(filepath_S_current)\n",
    "            else:\n",
    "                for j in range(num_bootstrap):\n",
    "                    stability_seed = stability_seeds[i,j]\n",
    "                    np.random.seed(stability_seed)\n",
    "                    choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "                    Y_current = Y[choice]\n",
    "                    X_current = X_rescaled[choice,:]\n",
    "                    S_current = delta_moment_parallel_stability(Y_current, X_current, num_resamples=num_resamples)\n",
    "                    S_array = np.vstack([S_array, S_current['delta']])\n",
    "                S_dict = {iterations_current: {\"delta\": S_array}}\n",
    "                write_pickle(S_dict, filepath_S_current)\n",
    "            S_dict_stability.update(S_dict)\n",
    "        write_pickle(S_dict_stability, filepath_S)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze bootstrap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.test_functions import Morris4\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.validation import Validation\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import time\n",
    "from setups_paper_gwp import *\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_params = 5000\n",
    "    iter_delt = 8*num_params\n",
    "    gsa = setup_delt(num_params, iter_delt, setup_morris4_model)\n",
    "    num_steps = 50\n",
    "    num_bootstrap = 60\n",
    "\n",
    "    # Convergence class\n",
    "    conv = Convergence(\n",
    "        gsa.filepath_Y,\n",
    "        gsa.num_params,\n",
    "        gsa.generate_gsa_indices,\n",
    "        gsa.gsa_label,\n",
    "        gsa.write_dir,\n",
    "        num_steps=num_steps,\n",
    "    )\n",
    "\n",
    "    filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "        gsa.gsa_label, gsa.sampling_label, gsa.iterations, conv.iterations_step, num_bootstrap, gsa.seed,\n",
    "    )\n",
    "    filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "    S_dict = read_pickle(filepath_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_bootstrap = S_dict[39200]['delta'][:,2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = S_bootstrap\n",
    "num_bins = 14\n",
    "bin_min, bin_max = min(Y), max(Y)\n",
    "bins_ = np.linspace(bin_min, bin_max, num_bins, endpoint=True)\n",
    "freq1, bins1 = np.histogram(Y, bins=bins_)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=bins1,\n",
    "        y=freq1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gsa_framework.test_functions import Morris4\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "#     path_base = Path(\"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/\")\n",
    "\n",
    "    # 1. Models\n",
    "    num_params = 10000\n",
    "    num_influential = max(num_params // 100, 10)\n",
    "    write_dir = path_base / \"{}_morris4\".format(num_params)\n",
    "    model = Morris4(num_params=num_params, num_influential=num_influential)\n",
    "    gsa_seed = 3407\n",
    "    validation_seed = 7043\n",
    "\n",
    "    fig_format = []  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "    iterations = 4 * num_params\n",
    "    test_size = 0.2\n",
    "\n",
    "    option = \"no tuning\"\n",
    "    if \"tuning\" in option:\n",
    "        # 1. Preparations\n",
    "        np.random.seed(gsa_seed)\n",
    "        X = np.random.rand(iterations, num_params)\n",
    "        Y = model(X)\n",
    "        # 2. Prepare training and testing sets for  gradient boosting trees\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X,\n",
    "            Y,\n",
    "            test_size=test_size,\n",
    "            random_state=gsa_seed,\n",
    "        )\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, Y_train)\n",
    "        X_dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "        if option == \"tuning\":\n",
    "            ### ROUND 1 ###\n",
    "            # xgb.train uses parameter `num_boost_round`, while XGBRegressor needs `n_estimators`. These two are the same.\n",
    "            param_grid = {\n",
    "                \"learning_rate\": [0.15],\n",
    "                \"gamma\": [0],\n",
    "                \"min_child_weight\": [60, 100, 140],\n",
    "                \"max_depth\": [2],\n",
    "                \"reg_lambda\": [0, 10],\n",
    "                \"reg_alpha\": [0, 10],\n",
    "                \"n_estimators\": [500, 800, 1100],\n",
    "                \"subsample\": [0.3, 0.6],\n",
    "                \"colsample_bytree\": [0.3, 0.6],\n",
    "            }\n",
    "\n",
    "            optimal_params = GridSearchCV(\n",
    "                estimator=xgb.XGBRegressor(\n",
    "                    objective=\"reg:squarederror\",\n",
    "                    seed=gsa_seed,\n",
    "                ),\n",
    "                param_grid=param_grid,\n",
    "                scoring=\"explained_variance\",  # explained_variance takes into account mean squared error, r2 does not. former is unbiasede, so better than r2\n",
    "                cv=3,\n",
    "            )\n",
    "            optimal_params.fit(\n",
    "                X_train,\n",
    "                Y_train,\n",
    "                early_stopping_rounds=10,\n",
    "                eval_set=[(X_test, Y_test)],\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            print(optimal_params.best_params_)\n",
    "\n",
    "            import pickle\n",
    "\n",
    "            filepath = write_dir / \"arrays\" / \"optimal_params_round_1.pickle\"\n",
    "            if filepath.exists():\n",
    "                filepath = write_dir / \"arrays\" / \"optimal_params_round_2.pickle\"\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                pickle.dump(optimal_params, f)\n",
    "\n",
    "        elif option == \"no tuning\":\n",
    "            np.random.seed(None)\n",
    "            reg = xgb.XGBRegressor(\n",
    "                verbosity=1,  # 0 (silent), 1 (warning), 2 (info), 3 (debug)\n",
    "                objective=\"reg:squarederror\",\n",
    "                seed=gsa_seed,\n",
    "                learning_rate=0.2,\n",
    "                gamma=0,\n",
    "                min_child_weight=600,\n",
    "                max_depth=2,\n",
    "                reg_lambda=0,\n",
    "                reg_alpha=0,\n",
    "                n_estimators=1500,\n",
    "                subsample=0.2,\n",
    "                colsample_bytree=0.2,\n",
    "            )\n",
    "            reg.fit(X_train, Y_train)\n",
    "            ev_train = explained_variance_score(reg.predict(X_train), Y_train)\n",
    "            ev_test = explained_variance_score(reg.predict(X_test), Y_test)\n",
    "            print(ev_train, ev_test)\n",
    "\n",
    "    else:\n",
    "        tuning_parameters = dict(\n",
    "            learning_rate=0.15,\n",
    "            gamma=0,\n",
    "            min_child_weight=100,\n",
    "            max_depth=2,\n",
    "            reg_lambda=5,\n",
    "            reg_alpha=0,\n",
    "            n_estimators=800,\n",
    "            subsample=0.3,\n",
    "            colsample_bytree=0.3,\n",
    "        )\n",
    "        gsa = GradientBoosting(\n",
    "            iterations=iterations,\n",
    "            model=model,\n",
    "            write_dir=write_dir,\n",
    "            seed=gsa_seed,\n",
    "            tuning_parameters=tuning_parameters,\n",
    "            test_size=test_size,\n",
    "            xgb_model=None,\n",
    "        )\n",
    "\n",
    "        S_dict = gsa.perform_gsa(flag_save_S_dict=True)\n",
    "        print(S_dict[\"stat.r2\"], S_dict[\"stat.explained_variance\"])\n",
    "        gsa.plot_sa_results(\n",
    "            {\"fscores\": S_dict[\"fscores\"]},\n",
    "            fig_format=fig_format,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa')\n",
    "setup_xgbo = setup_xgbo_morris4\n",
    "\n",
    "num_params = 5000\n",
    "iter_xgbo = 4*num_params\n",
    "gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_morris4_model, path_base)\n",
    "gsa_xgbo.perform_gsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsa-dev]",
   "language": "python",
   "name": "conda-env-gsa-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
