{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import os\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_pc = \"merlin_paper_gsa\"\n",
    "if 'merlin' in which_pc:\n",
    "    path_dask_logs = '/data/user/kim_a/dask_logs'\n",
    "    if not os.path.exists(path_dask_logs):\n",
    "        os.makedirs(path_dask_logs)\n",
    "    cluster = SLURMCluster(cores     = 4,\n",
    "                           processes = 4,\n",
    "                           memory    =\"80GB\", \n",
    "                           walltime  = '20:00:00',\n",
    "                           interface ='ib0',\n",
    "                           local_directory = path_dask_logs,\n",
    "                           log_directory   = path_dask_logs,\n",
    "                           queue=\"daily\",\n",
    "                           ) \n",
    "elif 'local' in which_pc:\n",
    "    cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 2\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.utils import read_hdf5_array, read_pickle, write_hdf5_array, write_pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_per_worker(option, num_params, iterations, i_worker, n_workers):\n",
    "    if option == \"random\":\n",
    "        gsa = setup_corr(num_params, iterations)\n",
    "    elif option == \"sobol\":\n",
    "        gsa = setup_sobol(num_params, iterations)\n",
    "    elif option == 'latin':\n",
    "        gsa = setup_latin(num_params, iterations)\n",
    "    gsa.dirpath_Y.mkdir(parents=True, exist_ok=True)\n",
    "    filepath_X_chunk = gsa.dirpath_Y / \"X.unitcube.{}.{}.pickle\".format(i_worker, n_workers)\n",
    "    X_chunk_unitcube = read_pickle(filepath_X_chunk)\n",
    "    X_chunk_rescaled = gsa.model.rescale(X_chunk_unitcube)\n",
    "    scores = gsa.model(X_chunk_rescaled)\n",
    "    Y_filename = \"{}.{}.pickle\".format(i_worker, n_workers)\n",
    "    filepath = gsa.dirpath_Y / Y_filename\n",
    "    write_pickle(scores, filepath)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_lca_model(num_params):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    demand_act = [act for act in co if \"Food and non-alcoholic beverages sector\" in act['name']][0]\n",
    "    demand = {demand_act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "    # Define some variables\n",
    "    write_dir = path_base / \"lca_model_food_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    return model, write_dir, gsa_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_corr(num_params, iterations):\n",
    "    model, write_dir, gsa_seed = setup_lca_model(num_params)\n",
    "    # Setup GSA\n",
    "    gsa = CorrelationCoefficients(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        seed=gsa_seed,\n",
    "    )\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_sobol(num_params, iterations):\n",
    "    model, write_dir, gsa_seed = setup_lca_model(num_params)\n",
    "    gsa = SaltelliSobol(iterations=iterations, model=model, write_dir=write_dir)\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_latin(num_params, iterations):\n",
    "    model, write_dir, gsa_seed = setup_lca_model(num_params)\n",
    "    num_resamples = 1\n",
    "    gsa = DeltaMoment(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        num_resamples=num_resamples,\n",
    "        seed=gsa_seed,\n",
    "    )\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_xgboost(num_params, iterations):\n",
    "    model, write_dir, gsa_seed = setup_lca_model(num_params)\n",
    "    num_boost_round = 400\n",
    "    tuning_parameters = {\n",
    "         'max_depth': 6,  \n",
    "         'eta': 0.1,\n",
    "         'objective': 'reg:squarederror',\n",
    "         'n_jobs': -1,\n",
    "         'refresh_leaf': True,\n",
    "         'subsample': 0.6,\n",
    "         'min_child_weight': 0.5,\n",
    "    }\n",
    "    gsa = GradientBoosting(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        seed=gsa_seed,\n",
    "        tuning_parameters=tuning_parameters,\n",
    "        num_boost_round=num_boost_round,\n",
    "        xgb_model=None,\n",
    "    )\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 10000\n",
    "iter_corr = 4*num_params\n",
    "iter_sobol = 40*num_params\n",
    "iter_latin = 4*num_params\n",
    "iter_xgboost = iter_corr\n",
    "\n",
    "n_workers_corr = 20\n",
    "n_workers_sobol = 39\n",
    "n_workers_latin = 20\n",
    "\n",
    "options = {\n",
    "    'random': {\n",
    "        \"iterations\": iter_corr,\n",
    "        \"n_workers\": n_workers_corr,\n",
    "    }, \n",
    "    'sobol': {\n",
    "        \"iterations\": iter_sobol,\n",
    "        \"n_workers\": n_workers_sobol,\n",
    "    }, \n",
    "    'latin': {\n",
    "        \"iterations\": iter_latin,\n",
    "        \"n_workers\": n_workers_latin,\n",
    "    }\n",
    "}\n",
    "gsa_corr = setup_corr(num_params, iter_corr)\n",
    "gsa_sobol = setup_sobol(num_params, iter_sobol)\n",
    "gsa_latin = setup_latin(num_params, iter_latin)\n",
    "gsa_xgboost = setup_xgboost(num_params, iter_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Correlation coefficients and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = gsa_corr.generate_unitcube_samples_based_on_method(gsa_corr.iterations)\n",
    "gsa_corr.create_model_output_dir()\n",
    "print(X.shape, gsa_corr.dirpath_Y)\n",
    "iter_corr_chunk = gsa_corr.iterations//n_workers_corr\n",
    "for i in range(n_workers_corr):\n",
    "    start = iter_corr_chunk*i\n",
    "    end = iter_corr_chunk*(i+1)\n",
    "    print(i,start,end)\n",
    "    X_chunk = X[start:end,:]\n",
    "    filepath_X_chunk = gsa_corr.dirpath_Y / \"X.unitcube.{}.{}.pickle\".format(i, n_workers_corr)\n",
    "    write_pickle(X_chunk, filepath_X_chunk)\n",
    "    \n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sobol iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gsa_sobol.generate_unitcube_samples_based_on_method(gsa_sobol.iterations)\n",
    "gsa_sobol.create_model_output_dir()\n",
    "print(X.shape, gsa_sobol.dirpath_Y)\n",
    "iter_sobol_chunk = gsa_sobol.iterations//(n_workers_sobol)\n",
    "for i in range(n_workers_sobol):\n",
    "    start = iter_sobol_chunk*i\n",
    "    end = min(iter_sobol_chunk*(i+1), gsa_sobol.iterations)\n",
    "    print(i,start,end)\n",
    "    X_chunk = X[start:end,:]\n",
    "    filepath_X_chunk = gsa_sobol.dirpath_Y / \"X.unitcube.{}.{}.pickle\".format(i, n_workers_sobol)\n",
    "    write_pickle(X_chunk, filepath_X_chunk)\n",
    "    \n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Latin sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gsa_latin.generate_unitcube_samples_based_on_method(gsa_latin.iterations)\n",
    "gsa_latin.create_model_output_dir()\n",
    "print(X.shape, gsa_latin.dirpath_Y)\n",
    "iter_latin_chunk = gsa_latin.iterations//n_workers_latin\n",
    "for i in range(n_workers_latin):\n",
    "    start = iter_latin_chunk*i\n",
    "    end = iter_latin_chunk*(i+1)\n",
    "    print(i,start,end)\n",
    "    X_chunk = X[start:end,:]\n",
    "    filepath_X_chunk = gsa_latin.dirpath_Y / \"X.unitcube.{}.{}.pickle\".format(i, n_workers_latin)\n",
    "    write_pickle(X_chunk, filepath_X_chunk)\n",
    "    \n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute model outputs for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "model_evals = []\n",
    "for option,dict_ in options.items():\n",
    "    iterations = dict_[\"iterations\"]\n",
    "    n_workers = dict_[\"n_workers\"]\n",
    "    for i in range(n_workers):\n",
    "        print(option, num_params, iterations, i, n_workers)\n",
    "        model_eval = task_per_worker(option, num_params, iterations, i, n_workers)\n",
    "        model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output_from_chunks(gsa, n_workers):\n",
    "    Y = np.zeros(\n",
    "        shape=(0,)\n",
    "    )\n",
    "    for i in range(n_workers):\n",
    "        filepath_Y_chunk = (\n",
    "            gsa.dirpath_Y\n",
    "            / \"{}.{}.pickle\".format(i, n_workers)\n",
    "        )\n",
    "        Y_chunk = read_pickle(filepath_Y_chunk)\n",
    "        Y = np.hstack(\n",
    "            [Y, Y_chunk]\n",
    "        )  # TODO change to vstack for multidimensional output\n",
    "    write_hdf5_array(Y,gsa.filepath_Y)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ycorr = generate_model_output_from_chunks(gsa_corr, n_workers_corr)\n",
    "Ysobol = generate_model_output_from_chunks(gsa_sobol, n_workers_sobol)\n",
    "Ylatin = generate_model_output_from_chunks(gsa_latin, n_workers_latin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run GSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_latin = dask.delayed(gsa_latin.perform_gsa)\n",
    "model_eval_latin = worker_latin()\n",
    "worker_xgboost = dask.delayed(gsa_xgboost.perform_gsa)\n",
    "model_eval_xgboost = worker_xgboost()\n",
    "model_evals = [model_eval_latin, model_eval_xgboost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = gsa_xgboost.S\n",
    "gsa_xgboost.model.uncertain_tech_params[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct LCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.extended_FAST import eFAST\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.validation import Validation\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.plotting import histogram_Y1_Y2\n",
    "from gsa_framework.utils import read_hdf5_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "#     )\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    demand_act = [act for act in co if \"Food and non-alcoholic beverages sector\" in act['name']][0]\n",
    "    print(demand_act)\n",
    "    demand = {demand_act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "\n",
    "    # Define some variables\n",
    "    num_params = 162299\n",
    "    iterations_validation = 2000\n",
    "    write_dir = path_base / \"lca_model_food_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir) # TODO add num_params later\n",
    "    gsa_seed = 3403\n",
    "    validation_seed = 7043\n",
    "    fig_format = [\"html\", \"pickle\"]\n",
    "\n",
    "    # Make sure  that the chosen num_params in LCA are appropriate\n",
    "    val = Validation(\n",
    "        model=model,\n",
    "        iterations=iterations_validation,\n",
    "        seed=4444,\n",
    "        default_x_rescaled=model.default_uncertain_amounts,\n",
    "        write_dir=write_dir,\n",
    "    )\n",
    "    num_params_paper = 10000\n",
    "    tag = \"numParams{}\".format(num_params_paper)\n",
    "    scores_dict = model.get_lsa_scores_pickle(model.write_dir / \"LSA_scores\")\n",
    "    uncertain_tech_params_where_subset, _ = model.get_nonzero_params_from_num_params(scores_dict, num_params_paper)\n",
    "    parameter_choice = []\n",
    "    for u in uncertain_tech_params_where_subset:\n",
    "        where_temp = np.where(model.uncertain_tech_params_where == u)[0]\n",
    "        assert len(where_temp) == 1\n",
    "        parameter_choice.append(where_temp[0])\n",
    "    parameter_choice.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_subset = val.get_influential_Y_from_parameter_choice(parameter_choice=parameter_choice, tag=tag)\n",
    "val.plot_histogram_Y_all_Y_inf(Y_subset, num_influential=num_params_paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsa-dev]",
   "language": "python",
   "name": "conda-env-gsa-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
