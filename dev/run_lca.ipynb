{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_pc = \"merlin_paper_gsa\"\n",
    "if 'merlin' in which_pc:\n",
    "    path_dask_logs = '/data/user/kim_a/dask_logs'\n",
    "    if not os.path.exists(path_dask_logs):\n",
    "        os.makedirs(path_dask_logs)\n",
    "    cluster = SLURMCluster(cores     = 8,\n",
    "                           processes = 6,\n",
    "                           memory    =\"12GB\", \n",
    "                           walltime  = '20:00:00',\n",
    "                           interface ='ib0',\n",
    "                           local_directory = path_dask_logs,\n",
    "                           log_directory   = path_dask_logs,\n",
    "                           queue=\"daily\",\n",
    "                           ) \n",
    "elif 'local' in which_pc:\n",
    "    cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 20\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using environment variable BRIGHTWAY2_DIR for data directory:\n",
      "/data/user/kim_a/Brightway3\n"
     ]
    }
   ],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.extended_FAST import eFAST\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.validation import Validation\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.plotting import histogram_Y1_Y2\n",
    "from gsa_framework.utils import read_hdf5_array, read_pickle, write_hdf5_array, write_pickle\n",
    "import h5py\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output_from_workers(n_workers, dirpath_Y, filepath_Y):\n",
    "#     if filepath_Y.exists():\n",
    "#         Y = read_hdf5_array(filepath_Y).flatten()\n",
    "#         print(\"{} already exists\".format(filepath_Y.name))\n",
    "#     else:\n",
    "    Y = np.array([])\n",
    "    for i in range(n_workers+1):\n",
    "        Y_chunk_filename = \"{}.{}.pickle\".format(i, n_workers)\n",
    "        filepath_Y_chunk = dirpath_Y / Y_chunk_filename\n",
    "        Y_chunk = read_pickle(filepath_Y_chunk)\n",
    "        Y = np.hstack(\n",
    "            [Y, Y_chunk]\n",
    "        )  # TODO change to vstack for multidimensional output\n",
    "    write_hdf5_array(Y, filepath_Y)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_scores_per_worker(num_params, iterations, n_workers, i_worker):\n",
    "#     gsa = setup_sobol(num_params, iterations)\n",
    "#     gsa.dirpath_Y.mkdir(parents=True, exist_ok=True)\n",
    "#     filepath_X_chunk = gsa.write_dir / \"arrays\" / \"x_chunks\" / \"{}.{}.pickle\".format(i_worker, n_workers)\n",
    "#     X_chunk_unitcube = read_pickle(filepath_X_chunk)\n",
    "#     X_chunk_rescaled = gsa.model.rescale(X_chunk_unitcube)\n",
    "#     scores = gsa.model(X_chunk_rescaled)\n",
    "#     Y_filename = \"{}.{}.pickle\".format(i_worker, n_workers)\n",
    "#     filepath = gsa.dirpath_Y / Y_filename\n",
    "#     write_pickle(scores, filepath)\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Correlations, 10'000 and 30'000 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_correlations(num_params, iterations):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "    # Define some variables\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    # Setup GSA\n",
    "    gsa = CorrelationCoefficients(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        seed=gsa_seed,\n",
    "    )\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_params = 30000\n",
    "iterations = 2 * num_params\n",
    "gsa = setup_correlations(num_params, iterations)\n",
    "X_unitcube = gsa.generate_unitcube_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Test\n",
    "# X_chunk_unitcube = X_unitcube[20:30,:]\n",
    "# n_workers = 20\n",
    "# i = 3\n",
    "# scores_test = compute_scores_per_worker(\n",
    "#     gsa = gsa,\n",
    "#     X_chunk_unitcube=X_chunk_unitcube, \n",
    "#     n_workers=n_workers, \n",
    "#     i_worker=i,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_per_worker = iterations // n_workers\n",
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "model_evals = []\n",
    "for i in range(n_workers):\n",
    "    start = i * iterations_per_worker\n",
    "    end = (i + 1) * iterations_per_worker\n",
    "    X_chunk_unitcube = X_unitcube[start:end, :]\n",
    "    model_eval = task_per_worker(\n",
    "        gsa = gsa,\n",
    "        X_chunk_unitcube=X_chunk_unitcube, \n",
    "        n_workers=n_workers, \n",
    "        i_worker=i,\n",
    "    )\n",
    "    model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath_Y = gsa.dirpath_Y\n",
    "filepath_Y = gsa.filepath_Y\n",
    "n_workers = 990\n",
    "Y = generate_model_output_from_workers(n_workers, dirpath_Y, filepath_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. eFAST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_efast(num_params, iterations):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "    # Define some variables\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    M = 4\n",
    "    gsa = eFAST(\n",
    "        M=M, iterations=iterations, model=model, write_dir=write_dir, seed=gsa_seed\n",
    "    )\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_params = 10000\n",
    "iterations = 65 * num_params\n",
    "gsa = setup_efast(num_params, iterations)\n",
    "# X_unitcube = gsa.generate_unitcube_samples() # TODO do it only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Test\n",
    "# start = 20\n",
    "# end = 30\n",
    "# with h5py.File(gsa.filepath_X_unitcube, \"r\") as f:\n",
    "#     X_chunk_unitcube = np.array(f[\"dataset\"][start:end,:])    \n",
    "# n_workers = 20\n",
    "# i = 3\n",
    "# scores_test = compute_scores_per_worker(\n",
    "#     gsa=gsa,\n",
    "#     X_chunk_unitcube=X_chunk_unitcube, \n",
    "#     n_workers=n_workers, \n",
    "#     i_worker=i,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_per_worker = iterations // n_workers\n",
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "model_evals = []\n",
    "for i in range(n_workers):\n",
    "    start = i * iterations_per_worker\n",
    "    end = (i + 1) * iterations_per_worker\n",
    "    with h5py.File(gsa.filepath_X_unitcube, \"r\") as f:\n",
    "        X_chunk_unitcube = np.array(f[\"dataset\"][start:end,:])  \n",
    "    model_eval = task_per_worker(\n",
    "        gsa=gsa,\n",
    "        X_chunk_unitcube=X_chunk_unitcube, \n",
    "        n_workers=n_workers, \n",
    "        i_worker=i,\n",
    "    )\n",
    "    scores = model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sobol method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_sobol(num_params, iterations):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "    # Define some variables\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    gsa = SaltelliSobol(iterations=iterations, model=model, write_dir=write_dir)\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_params = 10000\n",
    "iterations = 100 * num_params\n",
    "gsa = setup_sobol(num_params, iterations)\n",
    "# X_unitcube = gsa.generate_unitcube_samples() # TODO do it only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Create x_chunk files\n",
    "# iterations_per_worker = 1000\n",
    "# n_times = gsa.iterations // n_workers // iterations_per_worker + 1\n",
    "# n_tasks = gsa.iterations // iterations_per_worker\n",
    "\n",
    "# k=0\n",
    "# with h5py.File(gsa.filepath_X_unitcube, \"r\") as f:\n",
    "#     for j in range(n_times):\n",
    "#         for i in range(n_workers):\n",
    "#             start = k * iterations_per_worker\n",
    "#             end = (k + 1) * iterations_per_worker\n",
    "#             if k <= n_tasks:\n",
    "#                 print(k, start, end)\n",
    "#                 X_chunk_unitcube = np.array(f[\"dataset\"][start:end,:]) \n",
    "#                 filepath_X_chunk = gsa.write_dir / \"arrays\" / \"x_chunks\" / \"{}.{}.pickle\".format(k, n_tasks)\n",
    "#                 write_pickle(X_chunk_unitcube, filepath_X_chunk)\n",
    "#                 k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "# iterations_per_worker = 1000\n",
    "# n_times = gsa.iterations // n_workers // iterations_per_worker + 1\n",
    "# n_tasks = gsa.iterations // iterations_per_worker\n",
    "\n",
    "# k=0\n",
    "# for j in range(n_times):\n",
    "#     print(j)\n",
    "#     model_evals = []\n",
    "#     for i in range(n_workers):\n",
    "#         if k <= n_tasks:\n",
    "#             model_eval = task_per_worker(\n",
    "#                 gsa=gsa,\n",
    "#                 n_workers=n_tasks, \n",
    "#                 i_worker=k,\n",
    "#             )\n",
    "#             model_evals.append(model_eval)\n",
    "#             k += 1\n",
    "#     dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "iterations_per_worker = 1000\n",
    "n_times = gsa.iterations // n_workers // iterations_per_worker + 1\n",
    "n_tasks = gsa.iterations // iterations_per_worker\n",
    "\n",
    "num_params = 10000\n",
    "iterations = 100 * num_params\n",
    "\n",
    "model_evals = []\n",
    "j = 0\n",
    "for k in range(n_tasks+2):    \n",
    "    if len(model_evals) == n_workers or k==n_tasks+1:\n",
    "        print(j)\n",
    "        j += 1\n",
    "        dask.compute(model_evals)\n",
    "        model_evals = []\n",
    "    if k==n_tasks+1:\n",
    "        break\n",
    "    Y_filename = \"{}.{}.pickle\".format(k, n_tasks)\n",
    "    filepath = gsa.dirpath_Y / Y_filename\n",
    "    if not filepath.exists():\n",
    "        model_eval = task_per_worker(\n",
    "            num_params=num_params, \n",
    "            iterations=iterations,\n",
    "            n_workers=n_tasks, \n",
    "            i_worker=k,\n",
    "        )\n",
    "        model_evals.append(model_eval)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing LCA models test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.extended_FAST import eFAST\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.validation import Validation\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.plotting import histogram_Y1_Y2\n",
    "from gsa_framework.utils import read_hdf5_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "#     )\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "\n",
    "    # Define some variables\n",
    "    num_params = 10000\n",
    "    iterations_validation = 2000\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir) # TODO add num_params later\n",
    "    gsa_seed = 3403\n",
    "    validation_seed = 7043\n",
    "    fig_format = [\"html\", \"pickle\"]\n",
    "    parameter_inds_convergence_plot = [0,1,2] #TODO\n",
    "\n",
    "    # Make sure  that the chosen num_params in LCA are appropriate\n",
    "    val = Validation(\n",
    "        model=model,\n",
    "        iterations=2000,\n",
    "        seed=4444,\n",
    "        default_x_rescaled=model.default_uncertain_amounts,\n",
    "        write_dir=write_dir,\n",
    "    )\n",
    "    tag = \"numParams{}\".format(num_params)\n",
    "    scores_dict = model.get_lsa_scores_pickle(model.write_dir / \"LSA_scores\")\n",
    "    uncertain_tech_params_where_subset, _ = model.get_nonzero_params_from_num_params(scores_dict, num_params)\n",
    "    parameter_choice = []\n",
    "    for u in uncertain_tech_params_where_subset:\n",
    "        where_temp = np.where(model.uncertain_tech_params_where == u)[0]\n",
    "        assert len(where_temp) == 1\n",
    "        parameter_choice.append(where_temp[0])\n",
    "    parameter_choice.sort()\n",
    "    Y_subset = val.get_influential_Y_from_parameter_choice(parameter_choice=parameter_choice, tag=tag)\n",
    "    val.plot_histogram_Y_all_Y_inf(Y_subset, num_influential=num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_delta(num_params, iterations):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "    # Define some variables\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    num_resamples = 100\n",
    "    gsa = DeltaMoment(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        num_resamples=num_resamples,\n",
    "        seed=gsa_seed,\n",
    "    )\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa.dirpath_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_per_worker(num_params, iterations, n_workers, i_worker):\n",
    "    gsa = setup_delta(num_params, iterations)\n",
    "    gsa.dirpath_Y.mkdir(parents=True, exist_ok=True)\n",
    "    filepath_X_chunk = gsa.write_dir / \"arrays\" / \"x_chunks_lhc\" / \"{}.{}.pickle\".format(i_worker, n_workers)\n",
    "    X_chunk_unitcube = read_pickle(filepath_X_chunk)\n",
    "    X_chunk_rescaled = gsa.model.rescale(X_chunk_unitcube)\n",
    "    scores = gsa.model(X_chunk_rescaled)\n",
    "    Y_filename = \"{}.{}.pickle\".format(i_worker, n_workers)\n",
    "    filepath = gsa.dirpath_Y / Y_filename\n",
    "    write_pickle(scores, filepath)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_params = 10000\n",
    "iterations = 2 * num_params\n",
    "gsa = setup_delta(num_params, iterations)\n",
    "X_unitcube = gsa.generate_unitcube_samples() # TODO do it only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 20\n",
    "iterations_per_worker = 1000\n",
    "n_tasks = gsa.iterations // iterations_per_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create x_chunk files\n",
    "k=0\n",
    "with h5py.File(gsa.filepath_X_unitcube, \"r\") as f:\n",
    "    for i in range(n_workers):\n",
    "        start = i * iterations_per_worker\n",
    "        end = (i + 1) * iterations_per_worker\n",
    "        print(i, start, end)\n",
    "        X_chunk_unitcube = np.array(f[\"dataset\"][start:end,:]) \n",
    "        filepath_X_chunk = gsa.write_dir / \"arrays\" / \"x_chunks_lhc\" / \"{}.{}.pickle\".format(i, n_tasks-1)\n",
    "        write_pickle(X_chunk_unitcube, filepath_X_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "iterations_per_worker = 1000\n",
    "n_tasks = gsa.iterations // iterations_per_worker\n",
    "\n",
    "model_evals = []\n",
    "for i in range(n_workers):\n",
    "    Y_filename = \"{}.{}.pickle\".format(i, n_tasks-1)\n",
    "    filepath = gsa.dirpath_Y / Y_filename\n",
    "    if not filepath.exists():\n",
    "        model_eval = task_per_worker(\n",
    "            num_params=num_params, \n",
    "            iterations=iterations,\n",
    "            n_workers=n_tasks-1, \n",
    "            i_worker=i,\n",
    "        )\n",
    "        model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_params = 10000\n",
    "iterations = 100 * num_params\n",
    "path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "# LCA model\n",
    "bw.projects.set_current(\"GSA for paper\")\n",
    "co = bw.Database(\"CH consumption 1.0\")\n",
    "act = [act for act in co if \"Food\" in act['name']][0]\n",
    "demand = {act: 1}\n",
    "method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "# Define some variables\n",
    "write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "gsa_seed = 3403\n",
    "\n",
    "iterations_validation = 500\n",
    "validation_seed = 7043\n",
    "fig_format = [\"html\", \"pickle\"]\n",
    "num_influential = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa = SaltelliSobol(iterations=iterations, model=model, write_dir=write_dir)\n",
    "S_dict = gsa.generate_gsa_indices()\n",
    "first = S_dict[\"First order\"]\n",
    "total = S_dict[\"Total order\"]\n",
    "\n",
    "parameter_inds_convergence_plot = np.hstack([\n",
    "    np.argsort(total)[::-1][:10],\n",
    "    np.argsort(total)[::-1][-10:]\n",
    "])\n",
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    write_dir,\n",
    "    num_steps=100,\n",
    ")\n",
    "conv.run_convergence(\n",
    "    parameter_inds=parameter_inds_convergence_plot, fig_format=fig_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 403 ms, total: 14 s\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_params = 10000\n",
    "path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "# LCA model\n",
    "bw.projects.set_current(\"GSA for paper\")\n",
    "co = bw.Database(\"CH consumption 1.0\")\n",
    "act = [act for act in co if \"Food\" in act['name']][0]\n",
    "demand = {act: 1}\n",
    "method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "# Define some variables\n",
    "write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "gsa_seed = 3403\n",
    "\n",
    "iterations_validation = 500\n",
    "validation_seed = 7043\n",
    "fig_format = [\"html\", \"pickle\"]\n",
    "num_influential = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 2 * num_params\n",
    "num_resamples = 100\n",
    "gsa = DeltaMoment(\n",
    "    iterations=iterations,\n",
    "    model=model,\n",
    "    write_dir=write_dir,\n",
    "    num_resamples=num_resamples,\n",
    "    seed=gsa_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unitcube samples ->    0.000 s\n",
      "Rescaled samples ->    0.000 s\n",
      "Model outputs    ->    0.000 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "S_dict = gsa.perform_gsa()\n",
    "delta = S_dict[\"delta\"]\n",
    "gsa.plot_sa_results(\n",
    "    S_dict,\n",
    "    fig_format=fig_format,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "val = Validation(\n",
    "    model=model,\n",
    "    iterations=iterations_validation,\n",
    "    seed=validation_seed,\n",
    "    default_x_rescaled=None,\n",
    "    write_dir=write_dir,\n",
    ")\n",
    "tag = \"deltaIndexNr{}\".format(num_resamples)\n",
    "influential_Y = val.get_influential_Y_from_gsa(delta, num_influential, tag=tag)\n",
    "t1 = time.time()\n",
    "print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "val.plot_histogram_Y_all_Y_inf(\n",
    "    influential_Y, num_influential, tag=tag, fig_format=fig_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    write_dir,\n",
    "    num_steps=25,\n",
    ")\n",
    "parameter_inds = [0,1,2]\n",
    "conv.run_convergence(parameter_inds=parameter_inds, fig_format=fig_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
