{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_pc = \"merlin_paper_gsa\"\n",
    "if 'merlin' in which_pc:\n",
    "    path_dask_logs = '/data/user/kim_a/dask_logs'\n",
    "    if not os.path.exists(path_dask_logs):\n",
    "        os.makedirs(path_dask_logs)\n",
    "    cluster = SLURMCluster(cores     = 8,\n",
    "                           processes = 6,\n",
    "                           memory    =\"12GB\", \n",
    "                           walltime  = '20:00:00',\n",
    "                           interface ='ib0',\n",
    "                           local_directory = path_dask_logs,\n",
    "                           log_directory   = path_dask_logs,\n",
    "                           queue=\"daily\",\n",
    "                           ) \n",
    "elif 'local' in which_pc:\n",
    "    cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 60\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://192.168.196.21:38026</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.196.21:8787/status' target='_blank'>http://192.168.196.21:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://192.168.196.21:38026' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.extended_FAST import eFAST\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.validation import Validation\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.plotting import histogram_Y1_Y2\n",
    "from gsa_framework.utils import read_hdf5_array, read_pickle, write_hdf5_array, write_pickle\n",
    "import h5py\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output_from_workers(n_workers, dirpath_Y, filepath_Y):\n",
    "    if filepath_Y.exists():\n",
    "        Y = read_hdf5_array(filepath_Y).flatten()\n",
    "        print(\"{} already exists\".format(filepath_Y.name))\n",
    "    else:\n",
    "        Y = np.array([])\n",
    "        for i in range(n_workers):\n",
    "            Y_chunk_filename = \"{}.{}.pickle\".format(i, n_workers)\n",
    "            filepath_Y_chunk = dirpath_Y / Y_chunk_filename\n",
    "            Y_chunk = read_pickle(filepath_Y_chunk)\n",
    "            Y = np.hstack(\n",
    "                [Y, Y_chunk]\n",
    "            )  # TODO change to vstack for multidimensional output\n",
    "        write_hdf5_array(Y, filepath_Y)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_per_worker(num_params, iterations, n_workers, i_worker):\n",
    "    gsa = setup_sobol(num_params, iterations)\n",
    "    gsa.dirpath_Y.mkdir(parents=True, exist_ok=True)\n",
    "    filepath_X_chunk = gsa.write_dir / \"arrays\" / \"x_chunks\" / \"{}.{}.pickle\".format(i_worker, n_workers)\n",
    "    X_chunk_unitcube = read_pickle(filepath_X_chunk)\n",
    "    X_chunk_rescaled = gsa.model.rescale(X_chunk_unitcube)\n",
    "    scores = gsa.model(X_chunk_rescaled)\n",
    "    Y_filename = \"{}.{}.pickle\".format(i_worker, n_workers)\n",
    "    filepath = gsa.dirpath_Y / Y_filename\n",
    "    write_pickle(scores, filepath)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Correlations, 10'000 and 30'000 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_correlations(num_params, iterations):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "    # Define some variables\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    # Setup GSA\n",
    "    gsa = CorrelationCoefficients(\n",
    "        iterations=iterations,\n",
    "        model=model,\n",
    "        write_dir=write_dir,\n",
    "        seed=gsa_seed,\n",
    "    )\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_params = 30000\n",
    "iterations = 2 * num_params\n",
    "gsa = setup_correlations(num_params, iterations)\n",
    "X_unitcube = gsa.generate_unitcube_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Test\n",
    "# X_chunk_unitcube = X_unitcube[20:30,:]\n",
    "# n_workers = 20\n",
    "# i = 3\n",
    "# scores_test = compute_scores_per_worker(\n",
    "#     gsa = gsa,\n",
    "#     X_chunk_unitcube=X_chunk_unitcube, \n",
    "#     n_workers=n_workers, \n",
    "#     i_worker=i,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_per_worker = iterations // n_workers\n",
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "model_evals = []\n",
    "for i in range(n_workers):\n",
    "    start = i * iterations_per_worker\n",
    "    end = (i + 1) * iterations_per_worker\n",
    "    X_chunk_unitcube = X_unitcube[start:end, :]\n",
    "    model_eval = task_per_worker(\n",
    "        gsa = gsa,\n",
    "        X_chunk_unitcube=X_chunk_unitcube, \n",
    "        n_workers=n_workers, \n",
    "        i_worker=i,\n",
    "    )\n",
    "    model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath_Y = gsa.dirpath_Y\n",
    "filepath_Y = gsa.filepath_Y\n",
    "Y = generate_model_output_from_workers(n_workers, dirpath_Y, filepath_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. eFAST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_efast(num_params, iterations):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "    # Define some variables\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    M = 4\n",
    "    gsa = eFAST(\n",
    "        M=M, iterations=iterations, model=model, write_dir=write_dir, seed=gsa_seed\n",
    "    )\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_params = 10000\n",
    "iterations = 65 * num_params\n",
    "gsa = setup_efast(num_params, iterations)\n",
    "# X_unitcube = gsa.generate_unitcube_samples() # TODO do it only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Test\n",
    "# start = 20\n",
    "# end = 30\n",
    "# with h5py.File(gsa.filepath_X_unitcube, \"r\") as f:\n",
    "#     X_chunk_unitcube = np.array(f[\"dataset\"][start:end,:])    \n",
    "# n_workers = 20\n",
    "# i = 3\n",
    "# scores_test = compute_scores_per_worker(\n",
    "#     gsa=gsa,\n",
    "#     X_chunk_unitcube=X_chunk_unitcube, \n",
    "#     n_workers=n_workers, \n",
    "#     i_worker=i,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_per_worker = iterations // n_workers\n",
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "model_evals = []\n",
    "for i in range(n_workers):\n",
    "    start = i * iterations_per_worker\n",
    "    end = (i + 1) * iterations_per_worker\n",
    "    with h5py.File(gsa.filepath_X_unitcube, \"r\") as f:\n",
    "        X_chunk_unitcube = np.array(f[\"dataset\"][start:end,:])  \n",
    "    model_eval = task_per_worker(\n",
    "        gsa=gsa,\n",
    "        X_chunk_unitcube=X_chunk_unitcube, \n",
    "        n_workers=n_workers, \n",
    "        i_worker=i,\n",
    "    )\n",
    "    scores = model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sobol method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_sobol(num_params, iterations):\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "    # Define some variables\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir, num_params=num_params)\n",
    "    gsa_seed = 3403\n",
    "    gsa = SaltelliSobol(iterations=iterations, model=model, write_dir=write_dir)\n",
    "    return gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.89 s, sys: 323 ms, total: 9.21 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_params = 10000\n",
    "iterations = 100 * num_params\n",
    "gsa = setup_sobol(num_params, iterations)\n",
    "# X_unitcube = gsa.generate_unitcube_samples() # TODO do it only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Create x_chunk files\n",
    "# iterations_per_worker = 1000\n",
    "# n_times = gsa.iterations // n_workers // iterations_per_worker + 1\n",
    "# n_tasks = gsa.iterations // iterations_per_worker\n",
    "\n",
    "# k=0\n",
    "# with h5py.File(gsa.filepath_X_unitcube, \"r\") as f:\n",
    "#     for j in range(n_times):\n",
    "#         for i in range(n_workers):\n",
    "#             start = k * iterations_per_worker\n",
    "#             end = (k + 1) * iterations_per_worker\n",
    "#             if k <= n_tasks:\n",
    "#                 print(k, start, end)\n",
    "#                 X_chunk_unitcube = np.array(f[\"dataset\"][start:end,:]) \n",
    "#                 filepath_X_chunk = gsa.write_dir / \"arrays\" / \"x_chunks\" / \"{}.{}.pickle\".format(k, n_tasks)\n",
    "#                 write_pickle(X_chunk_unitcube, filepath_X_chunk)\n",
    "#                 k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "# iterations_per_worker = 1000\n",
    "# n_times = gsa.iterations // n_workers // iterations_per_worker + 1\n",
    "# n_tasks = gsa.iterations // iterations_per_worker\n",
    "\n",
    "# k=0\n",
    "# for j in range(n_times):\n",
    "#     print(j)\n",
    "#     model_evals = []\n",
    "#     for i in range(n_workers):\n",
    "#         if k <= n_tasks:\n",
    "#             model_eval = task_per_worker(\n",
    "#                 gsa=gsa,\n",
    "#                 n_workers=n_tasks, \n",
    "#                 i_worker=k,\n",
    "#             )\n",
    "#             model_evals.append(model_eval)\n",
    "#             k += 1\n",
    "#     dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "iterations_per_worker = 1000\n",
    "n_times = gsa.iterations // n_workers // iterations_per_worker + 1\n",
    "n_tasks = gsa.iterations // iterations_per_worker\n",
    "\n",
    "num_params = 10000\n",
    "iterations = 100 * num_params\n",
    "\n",
    "model_evals = []\n",
    "j = 0\n",
    "for k in range(n_tasks+2):    \n",
    "    if len(model_evals) == n_workers or k==n_tasks+1:\n",
    "        print(j)\n",
    "        j += 1\n",
    "        dask.compute(model_evals)\n",
    "        model_evals = []\n",
    "    if k==n_tasks+1:\n",
    "        break\n",
    "    Y_filename = \"{}.{}.pickle\".format(k, n_tasks)\n",
    "    filepath = gsa.dirpath_Y / Y_filename\n",
    "    if not filepath.exists():\n",
    "        model_eval = task_per_worker(\n",
    "            num_params=num_params, \n",
    "            iterations=iterations,\n",
    "            n_workers=n_tasks, \n",
    "            i_worker=k,\n",
    "        )\n",
    "        model_evals.append(model_eval)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing LCA models test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.extended_FAST import eFAST\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.validation import Validation\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.plotting import histogram_Y1_Y2\n",
    "from gsa_framework.utils import read_hdf5_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "#     )\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    act = [act for act in co if \"Food\" in act['name']][0]\n",
    "    demand = {act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "\n",
    "    # Define some variables\n",
    "    num_params = 10000\n",
    "    iterations_validation = 2000\n",
    "    write_dir = path_base / \"lca_model_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir) # TODO add num_params later\n",
    "    gsa_seed = 3403\n",
    "    validation_seed = 7043\n",
    "    fig_format = [\"html\", \"pickle\"]\n",
    "    parameter_inds_convergence_plot = [0,1,2] #TODO\n",
    "\n",
    "    # Make sure  that the chosen num_params in LCA are appropriate\n",
    "    val = Validation(\n",
    "        model=model,\n",
    "        iterations=2000,\n",
    "        seed=4444,\n",
    "        default_x_rescaled=model.default_uncertain_amounts,\n",
    "        write_dir=write_dir,\n",
    "    )\n",
    "    tag = \"numParams{}\".format(num_params)\n",
    "    scores_dict = model.get_lsa_scores_pickle(model.write_dir / \"LSA_scores\")\n",
    "    uncertain_tech_params_where_subset, _ = model.get_nonzero_params_from_num_params(scores_dict, num_params)\n",
    "    parameter_choice = []\n",
    "    for u in uncertain_tech_params_where_subset:\n",
    "        where_temp = np.where(model.uncertain_tech_params_where == u)[0]\n",
    "        assert len(where_temp) == 1\n",
    "        parameter_choice.append(where_temp[0])\n",
    "    parameter_choice.sort()\n",
    "    Y_subset = val.get_influential_Y_from_parameter_choice(parameter_choice=parameter_choice, tag=tag)\n",
    "    val.plot_histogram_Y_all_Y_inf(Y_subset, num_influential=num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bw]",
   "language": "python",
   "name": "conda-env-bw-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
