{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import bw2data as bd\n",
    "import bw2calc as bc\n",
    "import stats_arrays as sa\n",
    "import scipy.stats as stats\n",
    "from gsa_framework.utils import read_pickle, write_pickle\n",
    "from gsa_framework.models.life_cycle_assessment import LCAModelBase\n",
    "from gsa_framework.convergence_robustness_validation import Validation\n",
    "from gsa_framework.sensitivity_analysis.correlations import Correlations\n",
    "from dev.utils_graph_traversal import filter_uncertain_technosphere_exchanges\n",
    "\n",
    "from dev.utils_local_sa import get_bio_params_local_sa, get_cf_params_local_sa, get_tech_params_local_sa\n",
    "from dev.setups_paper_gwp import setup_lca_model_protocol\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa-framework-master/dev/write_files/\"\n",
    "#     )\n",
    "    path_base = Path('/data/user/kim_a')\n",
    "    write_dir = path_base / \"realistic_gsa\"\n",
    "    write_dir.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "    bd.projects.set_current(\"GSA for protocol\")\n",
    "    co = bd.Database(\"CH consumption 1.0\")\n",
    "    demand_act = [act for act in co if \"Food\" in act[\"name\"]]\n",
    "    assert len(demand_act) == 1\n",
    "    demand_act = demand_act[0]\n",
    "    demand = {demand_act: 1}\n",
    "    uncertain_method = (\"IPCC 2013\", \"climate change\", \"GWP 100a\", \"uncertain\")\n",
    "    lca = bc.LCA(demand, uncertain_method)\n",
    "    lca.lci()\n",
    "    lca.lcia()\n",
    "    print(lca.score)\n",
    "    \n",
    "    # Technosphere\n",
    "    max_calc = 1e+16\n",
    "    cutoff = 1e-16\n",
    "    tech_params_sct_filename = \"tech_params_cutoff{:1.0e}_maxcalc{:1.0e}.pickle\".format(cutoff, int(max_calc))\n",
    "    tech_params_sct_filepath = write_dir / tech_params_sct_filename\n",
    "    if not tech_params_sct_filepath.exists():\n",
    "        tech_params_sct = filter_uncertain_technosphere_exchanges(lca, cutoff=cutoff, max_calc=max_calc)\n",
    "        write_pickle(tech_params_sct, tech_params_sct_filepath)\n",
    "    else:\n",
    "        tech_params_sct = read_pickle(tech_params_sct_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_uncertainty_measure(params, q_range=0.95):\n",
    "    \"\"\"Generate uncertainty measures for all params.\"\"\"\n",
    "\n",
    "    from stats_arrays import MCRandomNumberGenerator\n",
    "\n",
    "    mc = MCRandomNumberGenerator(params)\n",
    "    params = params[mc.ordering]\n",
    "    \n",
    "    dt = np.dtype([\n",
    "        ('row', '<u4'), \n",
    "        ('col', '<u4'), \n",
    "        ('std2mean', '<f4'), \n",
    "        ('std2mean_maxnormalized', '<f4'), \n",
    "#         ('var2mean', '<f4'), \n",
    "#         ('range2mean', '<f4'), \n",
    "        ('uncertainty_type', 'u1'),\n",
    "        ('contribution', '<f4'),\n",
    "        ('contribution_maxnormalized', '<f4'),\n",
    "    ])\n",
    "\n",
    "    params_uncertainty_measure = np.zeros(len(params), dtype=dt)\n",
    "    \n",
    "    q_low = (1-q_range)/2\n",
    "    q_high = q_low + q_range\n",
    "\n",
    "    offset = 0\n",
    "    for uncertainty_type in mc.choices:\n",
    "        numparams = mc.positions[uncertainty_type]\n",
    "        if not numparams:\n",
    "            continue\n",
    "        current_params = params[offset:numparams + offset]\n",
    "        params_uncertainty_measure[offset:numparams + offset]['row'] = current_params['row']\n",
    "        params_uncertainty_measure[offset:numparams + offset]['col'] = current_params['col']\n",
    "        params_uncertainty_measure[offset:numparams + offset]['uncertainty_type'] = \\\n",
    "            current_params['uncertainty_type']\n",
    "        if uncertainty_type == sa.LognormalUncertainty:\n",
    "            s_normal = current_params['scale']\n",
    "            m_normal = current_params['loc']\n",
    "            s = np.sqrt( (np.exp(s_normal**2)-1) * np.exp(2*m_normal + s_normal**2) )\n",
    "            s = s_normal\n",
    "            m = np.exp(m_normal + (s_normal**2)/2)\n",
    "            range_ = stats.lognorm.ppf(q_high,s=s_normal, scale=np.exp(m_normal)) - \\\n",
    "                     stats.lognorm.ppf(q_low, s=s_normal, scale=np.exp(m_normal))\n",
    "        elif uncertainty_type == sa.NormalUncertainty:\n",
    "            s = current_params['scale']\n",
    "            m = current_params['loc']\n",
    "            range_ = stats.norm.ppf(q_high, loc=m, scale=s) - stats.norm.ppf(q_low, loc=m, scale=s)\n",
    "        elif uncertainty_type == sa.UniformUncertainty:\n",
    "            min_ = current_params['minimum']\n",
    "            max_ = current_params['maximum']\n",
    "            diff = max_ - min_\n",
    "            s = diff / np.sqrt(12)\n",
    "            m = current_params['loc']\n",
    "            range_ = stats.uniform.ppf(q_high, loc=min_, scale=diff) - stats.uniform.ppf(q_low, loc=min_, scale=diff)\n",
    "        elif uncertainty_type == sa.TriangularUncertainty:\n",
    "            a = current_params['minimum']\n",
    "            b = current_params['maximum']\n",
    "            c = current_params['loc']\n",
    "            diff = b - a\n",
    "            m = (a+b+c)/3\n",
    "            s = np.sqrt((a**2 + b**2 + c**2 - a*b - b*c - a*c)/18)\n",
    "            range_ = stats.triang.ppf(q_high, c=(c-a)/diff, loc=a, scale=diff) - \\\n",
    "                     stats.triang.ppf(q_low,  c=(c-a)/diff, loc=a, scale=diff)\n",
    "        else:\n",
    "            offset += numparams\n",
    "            continue\n",
    "\n",
    "        params_uncertainty_measure[offset:numparams + offset]['std2mean'] = np.abs(s/m)\n",
    "#         params_uncertainty_measure[offset:numparams + offset]['var2mean'] = np.abs((s**2)/m)\n",
    "#         params_uncertainty_measure[offset:numparams + offset]['range2mean'] = np.abs(range_/m)\n",
    "\n",
    "        offset += numparams\n",
    "\n",
    "    params_uncertainty_measure = params_uncertainty_measure[np.argsort(mc.ordering)]\n",
    "    \n",
    "    return params_uncertainty_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.dtype([\n",
    "    ('row', '<u4'), \n",
    "    ('col', '<u4'),\n",
    "])\n",
    "\n",
    "params_row_col = np.zeros(len(lca.tech_params), dt)\n",
    "params_row_col[['row', 'col']] = [(p['row'], p['col']) for p in lca.tech_params]\n",
    "\n",
    "tech_params_sct_row_col = np.zeros(len(tech_params_sct), dt)\n",
    "tech_params_sct_row_col[['row', 'col']] = [(p[0], p[1]) for p in tech_params_sct]\n",
    "\n",
    "where_inds = np.where(np.in1d(params_row_col, tech_params_sct_row_col))[0]\n",
    "input_params = lca.tech_params[where_inds]\n",
    "\n",
    "params_uncertain = add_uncertainty_measure(input_params)\n",
    "order = np.argsort(params_uncertain['std2mean'])[-1::-1]\n",
    "\n",
    "# num_params = -1\n",
    "selected_tech_params = params_uncertain[order]\n",
    "\n",
    "tech_params_sct_dict = {(p[0], p[1]): p[2] for p in tech_params_sct}\n",
    "for p in selected_tech_params:\n",
    "    p['contribution'] = tech_params_sct_dict[(p['row'], p['col'])]\n",
    "selected_tech_params['std2mean_maxnormalized'] = \\\n",
    "    selected_tech_params['std2mean'] / max(selected_tech_params['std2mean'])\n",
    "selected_tech_params['contribution_maxnormalized'] = \\\n",
    "    selected_tech_params['contribution'] / max(selected_tech_params['contribution'])\n",
    "\n",
    "uncertainty_measure = 0*selected_tech_params['std2mean'] + 1*selected_tech_params['contribution']\n",
    "\n",
    "num_params = 2000\n",
    "selected_tech_params_sorted = selected_tech_params[np.argsort(uncertainty_measure)[-1::-1][:num_params]]\n",
    "\n",
    "where_selected_tech = where_inds[order][np.argsort(uncertainty_measure)[-1::-1][:num_params]]\n",
    "where_selected_tech.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "const_factors = [1/10, 10]\n",
    "\n",
    "# Technosphere wrt tech_params\n",
    "tech_params_c = get_tech_params_local_sa(\n",
    "    where_selected_tech, \n",
    "    lca, \n",
    "    write_dir, \n",
    "    const_factors, \n",
    "    tag=\"sct{}\".format(num_params),\n",
    ")\n",
    "\n",
    "# Biosphere wrt bio_params\n",
    "bio_params_c = get_bio_params_local_sa(lca, write_dir, const_factors)\n",
    "\n",
    "# Characterization wrt cf_params\n",
    "cf_params_c = get_cf_params_local_sa(lca, write_dir, const_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {\n",
    "    'tech': {key: val['scores'] for key, val in tech_params_c.items()},\n",
    "    'bio': {key: val['scores'] for key, val in bio_params_c.items()},\n",
    "    'cf': {key: val['scores'] for key, val in cf_params_c.items()},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_where_high_var(scores_dict, lca_score, num_params):\n",
    "    vals = np.zeros([0, 3])\n",
    "    for scores_dict_exchange_type in scores_dict.values():\n",
    "        vals_temp = np.array(list(scores_dict_exchange_type.values()))\n",
    "        vals_temp = np.hstack(\n",
    "            [vals_temp, np.tile(lca_score, (len(vals_temp), 1))]\n",
    "        )\n",
    "        vals = np.vstack([vals, vals_temp])\n",
    "    # Variance of LSA scores for each input / parameter\n",
    "    var = np.var(vals, axis=1)\n",
    "    where_high_var = np.argsort(var)[::-1][:num_params]\n",
    "    assert np.all(var[where_high_var] > 0)\n",
    "    where_high_var = np.sort(where_high_var)\n",
    "    return where_high_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params_inf = 5000\n",
    "where_high_var = get_where_high_var(scores_dict, lca.score, num_params_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_tech = np.where(lca.tech_params['uncertainty_type']>1)[0]\n",
    "where_bio = np.where(lca.bio_params['uncertainty_type']>1)[0]\n",
    "where_cf = np.where(lca.cf_params['uncertainty_type']>1)[0]\n",
    "\n",
    "len_tech = len(scores_dict['tech'])\n",
    "len_bio = len(scores_dict['bio'])\n",
    "len_cf = len(scores_dict['cf'])\n",
    "\n",
    "where_tech_inf = where_high_var[where_high_var < len_tech]\n",
    "\n",
    "where_bio_inf = where_high_var[\n",
    "    np.logical_and(\n",
    "        where_high_var >= len_tech,\n",
    "        where_high_var < len_tech + len_bio,\n",
    "    )\n",
    "] - len_tech\n",
    "\n",
    "where_cf_inf = where_high_var[where_high_var >= len_tech + len_bio]  - len_tech - len_bio\n",
    "\n",
    "where_params_tech_inf = np.array(list(scores_dict['tech'].keys()))[where_tech_inf]\n",
    "where_params_bio_inf = np.array(list(scores_dict['bio'].keys()))[where_bio_inf]\n",
    "where_params_cf_inf = np.array(list(scores_dict['cf'].keys()))[where_cf_inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_params_selected_where_dict = {\n",
    "    'tech': where_tech,\n",
    "    'bio': where_bio,\n",
    "    'cf': where_cf,\n",
    "}\n",
    "\n",
    "uncertain_params = {\n",
    "    'tech': lca.tech_params[where_tech],\n",
    "    'bio': lca.bio_params[where_bio],\n",
    "    'cf': lca.cf_params[where_cf],\n",
    "}\n",
    "\n",
    "model = LCAModelBase(\n",
    "    demand,\n",
    "    uncertain_method,\n",
    "    uncertain_params,\n",
    "    uncertain_params_selected_where_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_params_inf = lca.tech_params[where_params_tech_inf]\n",
    "tech_uparams = lca.tech_params[where_tech]\n",
    "include_inds_tech_inf_forX = []\n",
    "for p in tech_params_inf:\n",
    "    w = np.where(\n",
    "        np.logical_and(\n",
    "            tech_uparams['row'] == p['row'],\n",
    "            tech_uparams['col'] == p['col'],\n",
    "        )\n",
    "    )[0]\n",
    "    assert len(w)==1\n",
    "    tech_ind = w[0]\n",
    "    include_inds_tech_inf_forX.append(tech_ind)  \n",
    "include_inds_tech_inf_forX = np.array(include_inds_tech_inf_forX) \n",
    "\n",
    "bio_params_inf  = lca.bio_params[where_params_bio_inf]\n",
    "bio_uparams = lca.bio_params[where_bio]\n",
    "include_inds_bio_inf_forX = []\n",
    "for p in bio_params_inf:\n",
    "    w = np.where(\n",
    "        np.logical_and(\n",
    "            bio_uparams['row'] == p['row'],\n",
    "            bio_uparams['col'] == p['col'],\n",
    "        )\n",
    "    )[0]\n",
    "    assert len(w)==1\n",
    "    bio_ind = w[0]\n",
    "    include_inds_bio_inf_forX.append(bio_ind)\n",
    "include_inds_bio_inf_forX = np.array(include_inds_bio_inf_forX) + model.uncertain_exchange_lengths['tech']\n",
    "\n",
    "cf_params_inf  = lca.cf_params[where_params_cf_inf]\n",
    "cf_uparams = lca.cf_params[where_cf]\n",
    "include_inds_cf_inf_forX = []\n",
    "for p in cf_params_inf:\n",
    "    w = np.where(\n",
    "        cf_uparams['row'] == p['row'],\n",
    "    )[0]\n",
    "    assert len(w)==1\n",
    "    cf_ind = w[0]\n",
    "    include_inds_cf_inf_forX.append(cf_ind)\n",
    "include_inds_cf_inf_forX = np.array(include_inds_cf_inf_forX) + \\\n",
    "                            model.uncertain_exchange_lengths['tech'] + model.uncertain_exchange_lengths['bio']\n",
    "\n",
    "parameter_choice_inf = np.hstack(\n",
    "    [include_inds_tech_inf_forX, include_inds_bio_inf_forX, include_inds_cf_inf_forX]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "default_uncertain_amounts = np.hstack([\n",
    "    v for v in model.default_uncertain_amounts.values()\n",
    "])\n",
    "\n",
    "iterations_validation = 2000\n",
    "validation_seed = 200300400\n",
    "lca_scores_axis_title = r\"$\\text{LCA scores, [kg CO}_2\\text{-eq}]$\"\n",
    "if __name__ == \"__main__\":\n",
    "    val = Validation(\n",
    "        model=model,\n",
    "        iterations=iterations_validation,\n",
    "        seed=validation_seed,\n",
    "        default_x_rescaled=default_uncertain_amounts,\n",
    "        write_dir=write_dir,\n",
    "        model_output_name=lca_scores_axis_title,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tag = \"localSA\"\n",
    "    Y_subset = val.get_influential_Y_from_parameter_choice(influential_inputs=parameter_choice_inf, tag=tag)\n",
    "    \n",
    "fig=val.plot_correlation_Y_all_Y_inf(Y_subset, num_influential=parameter_choice_inf.shape[0], tag=tag)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman correlations with dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "# from pathlib import Path\n",
    "# from dev.setups_paper_gwp import write_X_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which_pc = \"merlin_protocol_gsa\"\n",
    "# if 'merlin' in which_pc:\n",
    "#     path_dask_logs = Path('/data/user/kim_a/dask_logs')\n",
    "#     path_dask_logs.mkdir(parents=True, exist_ok=True)\n",
    "#     cluster = SLURMCluster(cores     = 8, \n",
    "#                            memory    ='20GB', \n",
    "#                            walltime  = '04:00:00',\n",
    "#                            interface ='ib0',\n",
    "#                            local_directory = path_dask_logs.as_posix(),\n",
    "#                            log_directory   = path_dask_logs.as_posix(),\n",
    "#                            queue=\"daily\",\n",
    "#                            ) \n",
    "# elif 'local' in which_pc:\n",
    "#     cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_workers = 20\n",
    "# cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_lca_model_realistic(\n",
    "    i,\n",
    "    n_workers,\n",
    "    path_base, \n",
    "    num_params=None, \n",
    "    write_dir=None, \n",
    "):\n",
    "    # LCA model\n",
    "    bd.projects.set_current(\"GSA for protocol\")\n",
    "    co = bd.Database(\"CH consumption 1.0\")\n",
    "    demand_act = [act for act in co if \"Food\" in act[\"name\"]]\n",
    "    assert len(demand_act) == 1\n",
    "    demand_act = demand_act[0]\n",
    "    demand = {demand_act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GWP 100a\", \"uncertain\")\n",
    "    lca = bc.LCA(demand, method)\n",
    "    lca.lci()\n",
    "    lca.lcia()\n",
    "    # Define some variables\n",
    "    if write_dir is None:\n",
    "        write_dir = path_base / \"realistic_gsa\"\n",
    "        \n",
    "    gsa_seed = 700800900\n",
    "    \n",
    "    dirpath_uparams = write_dir / \"arrays\" / \"Y.randomSampling.20000.{}.None\".format(gsa_seed)\n",
    "    filepath_uparams = dirpath_uparams / \"uparams_where_{}_{}.pickle\".format(i, n_workers)\n",
    "    uncertain_params_selected_where_dict = read_pickle(filepath_uparams)\n",
    "        \n",
    "    uncertain_params = {\n",
    "        'tech': lca.tech_params[uncertain_params_selected_where_dict['tech']],\n",
    "        'bio': lca.bio_params[uncertain_params_selected_where_dict['bio']],\n",
    "        'cf': lca.cf_params[uncertain_params_selected_where_dict['cf']],\n",
    "    }\n",
    "\n",
    "    model = LCAModelBase(\n",
    "        demand,\n",
    "        method,\n",
    "        uncertain_params,\n",
    "    )\n",
    "    \n",
    "    return model, write_dir, gsa_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "iterations = 4*num_params_inf\n",
    "\n",
    "# from gsa_framework.sensitivity_analysis.correlations import Correlations\n",
    "\n",
    "# uncertain_params_selected_where_dict = {\n",
    "#     'tech': where_params_tech_inf,\n",
    "#     'bio': where_params_bio_inf,\n",
    "#     'cf': where_params_cf_inf,\n",
    "# }\n",
    "# uncertain_params = {\n",
    "#     'tech': lca.tech_params[where_params_tech_inf],\n",
    "#     'bio': lca.bio_params[where_params_bio_inf],\n",
    "#     'cf': lca.cf_params[where_params_cf_inf],\n",
    "# }\n",
    "\n",
    "# model_screening = LCAModelBase(\n",
    "#     demand,\n",
    "#     uncertain_method,\n",
    "#     uncertain_params,\n",
    "# )\n",
    "\n",
    "n_workers = 20\n",
    "\n",
    "model_screening, write_dir, gsa_seed = setup_lca_model_realistic(0, n_workers, path_base)\n",
    "\n",
    "gsa_corr = Correlations(\n",
    "    iterations=iterations,\n",
    "    model=model_screening,\n",
    "    write_dir=write_dir,\n",
    "    seed=gsa_seed,\n",
    ")\n",
    "\n",
    "options = {\n",
    "    'corr': {\n",
    "        \"iterations\": iterations,\n",
    "        \"n_workers\":  n_workers,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_uparams_where(dirpath_Y, uncertain_params_selected_where_dict, n_workers):\n",
    "#     for i in range(n_workers):\n",
    "#         filepath_i = dirpath_Y / \"uparams_where_{}_{}.pickle\".format(\n",
    "#             i, n_workers\n",
    "#         )\n",
    "#         write_pickle(uncertain_params_selected_where_dict, filepath_i)\n",
    "\n",
    "# # gsa_corr.dirpath_Y.mkdir(parents=True, exist_ok=True)\n",
    "# # write_X_chunks(gsa, n_workers_corr)\n",
    "# write_uparams_where(gsa_corr.dirpath_Y, uncertain_params_selected_where_dict, n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_scores_per_worker(\n",
    "#     option, num_params, iterations, i_worker, n_workers, setup_model, path_base\n",
    "# ):\n",
    "#     model_screening, write_dir, gsa_seed = setup_model(i_worker, n_workers, path_base, num_params)\n",
    "#     from gsa_framework.sensitivity_analysis.correlations import Correlations\n",
    "#     # Setup GSA\n",
    "#     gsa = Correlations(\n",
    "#         iterations=iterations,\n",
    "#         model=model_screening,\n",
    "#         write_dir=write_dir,\n",
    "#         seed=gsa_seed,\n",
    "#     )\n",
    "#     gsa.dirpath_Y.mkdir(parents=True, exist_ok=True)\n",
    "#     filepath_X_chunk = gsa.dirpath_Y / \"X.unitcube.{}.{}.pickle\".format(\n",
    "#         i_worker, n_workers\n",
    "#     )\n",
    "#     X_chunk_unitcube = read_pickle(filepath_X_chunk)\n",
    "#     X_chunk_rescaled = gsa.model.rescale(X_chunk_unitcube)\n",
    "#     del X_chunk_unitcube\n",
    "#     scores = gsa.model(X_chunk_rescaled)\n",
    "#     Y_filename = \"{}.{}.pickle\".format(i_worker, n_workers)\n",
    "#     filepath = gsa.dirpath_Y / Y_filename\n",
    "#     write_pickle(scores, filepath)\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute model outputs\n",
    "# task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "# model_evals = []\n",
    "# for option,dict_ in options.items():\n",
    "#     iterations = dict_[\"iterations\"]\n",
    "#     n_workers = dict_[\"n_workers\"]\n",
    "#     for i in range(n_workers):\n",
    "#         print(option, num_params_inf, iterations, i, n_workers)\n",
    "#         model_eval = task_per_worker(\n",
    "#             option, \n",
    "#             num_params_inf, \n",
    "#             iterations, \n",
    "#             i, \n",
    "#             n_workers, \n",
    "#             setup_lca_model_realistic, \n",
    "#             path_base\n",
    "#         )\n",
    "#         model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dev.setups_paper_gwp import generate_model_output_from_chunks\n",
    "# generate_model_output_from_chunks(gsa_corr, n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman correlations without dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "#\n",
    "# filename_Y = \"Y.randomSampling.{}.{}.700800900\".format(num_params_inf, iterations, gsa_seed)\n",
    "# filepath_Y = gsa.filepath_Y.parent / filename_Y\n",
    "# print(filepath_Y)\n",
    "\n",
    "# X = gsa.generate_unitcube_samples(iterations)\n",
    "# Xr = model_screening.rescale(X)\n",
    "# Y = model_screening(Xr)\n",
    "# write_hdf5_array(Y, filepath_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = gsa_corr.perform_gsa()\n",
    "spearman = S['spearman']\n",
    "argsort_spearman = np.argsort(spearman)[-1::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman.shape, parameter_choice_inf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman[argsort_spearman]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params_after_spearman = 20\n",
    "parameter_choice_spearman = parameter_choice_inf[argsort_spearman][:num_params_after_spearman]\n",
    "parameter_choice_spearman.sort()\n",
    "parameter_choice_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tag = \"spearman\"\n",
    "    Y_subset = val.get_influential_Y_from_parameter_choice(influential_inputs=parameter_choice_spearman, tag=tag)\n",
    "    \n",
    "fig=val.plot_correlation_Y_all_Y_inf(Y_subset, num_influential=parameter_choice_inf.shape[0], tag=tag)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = model.uncertain_params['tech'][parameter_choice_spearman[:8]]\n",
    "rdict = lca.reverse_dict()\n",
    "for p in tech:\n",
    "    act_in = bd.get_activity(rdict[0][p['row']])\n",
    "    act_out = bd.get_activity(rdict[0][p['col']])\n",
    "    print(\"FROM {}\".format(act_in['name']))\n",
    "    print(\"TO {}\\n\".format(act_out['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio = model.uncertain_params['bio'][parameter_choice_spearman[8:17] - len(model.uncertain_params['tech'])]\n",
    "rdict = lca.reverse_dict()\n",
    "for p in bio:\n",
    "    act_in = bd.get_activity(rdict[2][p['row']])\n",
    "    act_out = bd.get_activity(rdict[0][p['col']])\n",
    "    print(\"FROM {}\".format(act_in['name']))\n",
    "    print(\"TO {}\\n\".format(act_out['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = model.uncertain_params['cf'][ parameter_choice_spearman[17:] - len(model.uncertain_params['tech'])\n",
    "                                   - len(model.uncertain_params['bio'])]\n",
    "rdict = lca.reverse_dict()\n",
    "for p in cf:\n",
    "    act_in = bd.get_activity(rdict[2][p['row']])\n",
    "    print(\"{}\\n\".format(act_in['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:protocol-dev-py38]",
   "language": "python",
   "name": "conda-env-protocol-dev-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
