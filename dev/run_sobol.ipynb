{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gsa_framework.test_functions import SobolGstar\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.extended_FAST import eFAST\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.validation import Validation\n",
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "#     )\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # 1. Models\n",
    "    num_params = 1000\n",
    "    num_influential = num_params // 100\n",
    "    iterations_validation = 2000\n",
    "    write_dir = path_base / \"sobol_Gstar_model_{}\".format(num_params)\n",
    "    gsa_seed = 3407\n",
    "    sobol_g_star_seed = 345897\n",
    "    validation_seed = 7043\n",
    "    if num_influential == 10:\n",
    "        a = np.array(\n",
    "            [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "            + [9] * (num_params - num_influential)\n",
    "        )\n",
    "        alpha = np.ones(num_params)  # case 2 from Saltelli paper\n",
    "    else:\n",
    "        a = 9 * np.ones(num_params)\n",
    "        a[:num_influential] = np.linspace(0, 4, num_influential, endpoint=True)\n",
    "        alpha = np.ones(num_params)\n",
    "    np.random.seed(sobol_g_star_seed)\n",
    "    delta = np.random.rand(num_params)  # delta doesn't affect input importance\n",
    "    np.random.seed(None)\n",
    "    model = SobolGstar(\n",
    "        num_params=num_params,\n",
    "        num_influential=num_influential,\n",
    "        a=a,\n",
    "        alpha=alpha,\n",
    "        delta=delta,\n",
    "    )\n",
    "    fig_format = []  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "    num_params_corr_plot = 10\n",
    "    parameter_inds = list(range(num_params_corr_plot)) + list(\n",
    "        range(num_influential, num_influential + num_params_corr_plot)\n",
    "    )\n",
    "\n",
    "    # TODO Choose which GSA to perform\n",
    "    flag_sobol = 0\n",
    "    flag_eFAST = 1\n",
    "    flag_xgboost = 0\n",
    "\n",
    "#     if flag_sobol:\n",
    "#         iterations = 400 * num_params\n",
    "#         gsa = SaltelliSobol(iterations=iterations, model=model, write_dir=write_dir)\n",
    "#         S_dict = gsa.perform_gsa()\n",
    "#         first = S_dict[\"First order\"]\n",
    "#         total = S_dict[\"Total order\"]\n",
    "#         gsa.plot_sa_results(\n",
    "#             S_dict,\n",
    "#             S_dict_analytical=model.S_dict_analytical,\n",
    "#             fig_format=fig_format,\n",
    "#         )\n",
    "\n",
    "#         t0 = time.time()\n",
    "#         val = Validation(\n",
    "#             model=model,\n",
    "#             iterations=iterations_validation,\n",
    "#             seed=validation_seed,\n",
    "#             default_x_rescaled=None,\n",
    "#             write_dir=write_dir,\n",
    "#         )\n",
    "#         tag_total = \"SaltelliTotalIndex\"\n",
    "#         influential_Y_total = val.get_influential_Y_from_gsa(total, num_influential, tag=tag_total)\n",
    "#         tag_first = \"SaltelliFirstIndex\"\n",
    "#         influential_Y_first = val.get_influential_Y_from_gsa(first, num_influential, tag=tag_first)\n",
    "#         t1 = time.time()\n",
    "#         print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "#         # val.plot_correlation_Y_all_Y_inf(\n",
    "#         #     influential_Y, num_influential, tag=tag, fig_format=fig_format\n",
    "#         # )\n",
    "# #         val.plot_histogram_Y_all_Y_inf(\n",
    "# #             influential_Y, num_influential, tag=tag, fig_format=fig_format\n",
    "# #         )\n",
    "#         corr_coef_total = np.corrcoef(np.vstack([influential_Y_total, val.Y_all]))[0,1]\n",
    "#         corr_coef_first = np.corrcoef(np.vstack([influential_Y_first, val.Y_all]))[0,1]\n",
    "#         spearman_total, _ = spearmanr(influential_Y_total, val.Y_all)\n",
    "#         spearman_first, _ = spearmanr(influential_Y_first, val.Y_all)\n",
    "#         print(\" \\\n",
    "#              corr_coef_total {} \\n \\\n",
    "#              corr_coef_first {} \\n \\\n",
    "#              spearman_total {} \\n \\\n",
    "#              spearman_first {} \\n\".format(corr_coef_total, corr_coef_first, spearman_total, spearman_first)\n",
    "#         )\n",
    "\n",
    "#         conv = Convergence(\n",
    "#             gsa.filepath_Y,\n",
    "#             gsa.num_params,\n",
    "#             gsa.generate_gsa_indices,\n",
    "#             gsa.gsa_label,\n",
    "#             write_dir,\n",
    "#             num_steps=100,\n",
    "#         )\n",
    "#         conv.run_convergence(parameter_inds=parameter_inds, fig_format=fig_format)\n",
    "\n",
    "    if flag_eFAST:\n",
    "        iterations = 3000 * num_params\n",
    "        M = 4\n",
    "        gsa = eFAST(\n",
    "            M=M, iterations=iterations, model=model, write_dir=write_dir, seed=gsa_seed\n",
    "        )\n",
    "        S_dict = gsa.perform_gsa()\n",
    "        first = S_dict[\"First order\"]\n",
    "        total = S_dict[\"Total order\"]\n",
    "        gsa.plot_sa_results(\n",
    "            S_dict,\n",
    "            S_dict_analytical=model.S_dict_analytical,\n",
    "            fig_format=fig_format,\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        val = Validation(\n",
    "            model=model,\n",
    "            iterations=iterations_validation,\n",
    "            seed=validation_seed,\n",
    "            default_x_rescaled=None,\n",
    "            write_dir=write_dir,\n",
    "        )\n",
    "        tag = \"eFastTotalIndex\"\n",
    "        influential_Y = val.get_influential_Y_from_gsa(total, num_sinfluential, tag=tag)\n",
    "        t1 = time.time()\n",
    "        print(\"Total validation time  -> {:8.3f} s \\n\".format(t1 - t0))\n",
    "        val.plot_histogram_Y_all_Y_inf(\n",
    "            influential_Y, num_influential, tag=tag, fig_format=fig_format\n",
    "        )\n",
    "        \n",
    "        conv = Convergence(\n",
    "            gsa.filepath_Y,\n",
    "            gsa.num_params,\n",
    "            gsa.generate_gsa_indices,\n",
    "            gsa.gsa_label,\n",
    "            write_dir,\n",
    "            num_steps=100,\n",
    "            M=M,\n",
    "        )\n",
    "        conv.run_convergence(\n",
    "            parameter_inds=parameter_inds, fig_format=fig_format\n",
    "        )\n",
    "\n",
    "    if flag_xgboost:\n",
    "        gsa = GradientBoosting(iterations=iterations, model=model, write_dir=write_dir)\n",
    "        S_dict = gsa.perform_gsa()\n",
    "        fscores = S_dict[\"fscores\"]\n",
    "        gsa.plot_sa_results(S_dict, S_boolean=model.S_boolean)\n",
    "        val = Validation(fscores, model, num_influential=model.num_influential)\n",
    "        val.generate_plots(plot_histogram=True, plot_correlation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2937040305605457"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import brightway2 as bw\n",
    "import numpy as np\n",
    "bw.projects.set_current(\"GSA for paper\")\n",
    "co = bw.Database(\"CH consumption 1.0\")\n",
    "act = [act for act in co if \"food\" in act['name']][0]\n",
    "demand = {act: 1}\n",
    "method = (\"IPCC 2013\", \"climate change\", \"GTP 100a\")\n",
    "lca = bw.LCA(demand, method)\n",
    "lca.lci()\n",
    "lca.lcia()\n",
    "lca.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<18326x18326 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 208807 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lca.technosphere_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from gsa_framework.utils import read_pickle\n",
    "path = Path('/data/user/kim_a/setac_gsa/LSA_scores/3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsa_scores_pickle(path):\n",
    "    \"\"\"Get LCIA scores stored in the ``path``, where each parameter was sampled only few (eg 3-10) times.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path that contains pickle files with LCIA scores, where each uncertain exchange in the technosphere\n",
    "        is varied independently of all other exchanges but only few times. Needed to determine which exchanges\n",
    "        do NOT change LCIA score of the given functional unit and method, and hence, can be disregarded in the GSA.\n",
    "        Pickle files explicitly contain ``(database, code)`` tuples for input and output activities, in order to\n",
    "        most certainly define exchanges.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scores : dict\n",
    "        Keys are indices of the exchanges as they appear in the lca.tech_params, values are LCIA scores.\n",
    "\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    files = [filepath.name for filepath in path.iterdir() if \"LSA_scores_\" in filepath.name and filepath.is_file()]\n",
    "    starts = [int(filepath.split(\"_\")[2]) for filepath in files]\n",
    "    ind_sort = np.argsort(starts)\n",
    "    files_sorted = [files[i] for i in ind_sort]\n",
    "\n",
    "    scores, inputs, outputs = [], [], []\n",
    "    for file in files_sorted:\n",
    "        filepath = path / file\n",
    "        temp = read_pickle(filepath)\n",
    "        inputs  += [vals[\"input\"] for vals in temp.values()]\n",
    "        outputs += [vals[\"output\"] for vals in temp.values()]\n",
    "        scores  += [vals[\"scores\"] for vals in temp.values()]\n",
    "    num_exchanges = len(inputs)\n",
    "\n",
    "    input_row_dict = {}\n",
    "    for input_ in list(set(inputs)):\n",
    "        input_row_dict[input_] = lca.activity_dict[input_]\n",
    "    output_col_dict = {}\n",
    "    for output_ in list(set(outputs)):\n",
    "        output_col_dict[output_] = lca.activity_dict[output_]\n",
    "\n",
    "    scores_dict = {}\n",
    "    for i in range(num_exchanges):\n",
    "        row = input_row_dict[inputs[i]]\n",
    "        col = output_col_dict[outputs[i]]\n",
    "        where_temp = np.where(\n",
    "            np.logical_and(\n",
    "                np.logical_and(\n",
    "                    lca.tech_params['row'] == row,\n",
    "                    lca.tech_params['col'] == col,\n",
    "                ),\n",
    "                lca.tech_params['uncertainty_type'] > 1,\n",
    "            )\n",
    "        )[0]\n",
    "        assert len(where_temp) == 1\n",
    "        scores_dict[where_temp[0]] = scores[i]\n",
    "    return scores_dict\n",
    "\n",
    "def get_nonzero_params_from_num_params(scores_dict, num_params):\n",
    "    keys = np.array(list(scores_dict.keys()))\n",
    "    vals = np.array(list(scores_dict.values()))\n",
    "    vals = np.hstack([vals, np.tile(2484.925996825716, (len(vals), 1))])\n",
    "    # Variance of LSA scores for each input / parameter\n",
    "    var = np.var(vals, axis=1)\n",
    "    where_var = np.argsort(var)[:num_params]\n",
    "    print(len(var[where_var]>0))\n",
    "#     assert np.all(var[where_var]>0)\n",
    "    params_yes = keys[where_var]\n",
    "    params_no = np.setdiff1d(keys, params_yes)\n",
    "#     params_yes.sort(), params_no.sort()\n",
    "    return params_yes, params_no, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = get_lsa_scores_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "num_params = 10000\n",
    "uncertain_tech_params_where_subset, _, aa = get_nonzero_params_from_num_params(scores_dict, num_params)\n",
    "uncertain_tech_params_subset = lca.tech_params[uncertain_tech_params_where_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47958314, 0.81853527, 1.0049876 , 0.2295648 , 0.20493902,\n",
       "       0.28284273, 0.20493902, 0.24617067, 0.11224972, 0.42261094,\n",
       "       0.91104335, 0.20976177, 0.05196152, 0.4036087 , 0.45166358,\n",
       "       0.05099019, 0.44944412, 0.2       , 0.41231057, 0.05656854],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertain_tech_params_subset['scale'][560:580]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 162296, 162297, 162298])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(var>0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bw]",
   "language": "python",
   "name": "conda-env-bw-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
