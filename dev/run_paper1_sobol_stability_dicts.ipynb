{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobol indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from gsa_framework.sensitivity_methods.saltelli_sobol import sobol_indices_stability\n",
    "from gsa_framework.convergence_robustness_validation.convergence import Convergence\n",
    "from gsa_framework.utils import read_hdf5_array, read_pickle, write_pickle\n",
    "from setups_paper_gwp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "\n",
    "    # Sobol stability dictionaries\n",
    "    num_params = 10000\n",
    "    iterations = 100 * num_params\n",
    "    num_steps = 50\n",
    "    num_bootstrap = 120\n",
    "\n",
    "    gsa = setup_salt(num_params, iterations, setup_morris4_model, path_base)\n",
    "\n",
    "    # Convergence class\n",
    "    conv = Convergence(\n",
    "        gsa.filepath_Y,\n",
    "        gsa.num_params,\n",
    "        gsa.generate_gsa_indices,\n",
    "        gsa.gsa_label,\n",
    "        gsa.write_dir,\n",
    "        num_steps=num_steps,\n",
    "    )\n",
    "    np.random.seed(gsa.seed)\n",
    "    stability_seeds = np.random.randint(\n",
    "        low=0,\n",
    "        high=2147483647,\n",
    "        size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    "    )\n",
    "\n",
    "    filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "        gsa.gsa_label,\n",
    "        gsa.sampling_label,\n",
    "        gsa.iterations,\n",
    "        conv.iterations_step,\n",
    "        num_bootstrap,\n",
    "        gsa.seed,\n",
    "    )\n",
    "    filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "    if filepath_S.exists():\n",
    "        print(\"--> {} already exists\".format(filename_S))\n",
    "        S_dict_stability = read_pickle(filepath_S)\n",
    "    else:\n",
    "        Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "        S_dict_stability = sobol_indices_stability(\n",
    "            Y,\n",
    "            num_params,\n",
    "            conv.iterations_for_convergence,\n",
    "            num_bootstrap,\n",
    "            stability_seeds,\n",
    "        )\n",
    "        write_pickle(S_dict_stability, filepath_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble multiple stability_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Morris4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from gsa_framework.utils import read_hdf5_array, read_pickle, write_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stability_dict_from_seeds(seeds, model_dir_array, filenames_stability_dict):\n",
    "    stability_dict_all = {}\n",
    "    for gsa_method, path in filenames_stability_dict.items():\n",
    "        stability_dict_raw = {}\n",
    "        steps_temp = {}\n",
    "        for seed in seeds:\n",
    "            filepath = model_dir_array / path.replace('seed', str(seed))\n",
    "            data = read_pickle(filepath)\n",
    "            steps_temp[seed] = list(data.keys())\n",
    "            stability_dict_raw[seed] = data\n",
    "        steps = np.array(list(steps_temp.values()))\n",
    "        assert (steps == steps[0]).all() # make sure that steps are the same\n",
    "        steps = steps[0]\n",
    "        # Init stability_dict\n",
    "        stability_dict = {}\n",
    "        for step in steps:\n",
    "            for seed in seeds:\n",
    "                stability_dict[step] = {}\n",
    "                for stat_name,data in stability_dict_raw[seed][step].items():\n",
    "                    stability_dict[step][stat_name] = np.zeros([0,data.shape[1]])\n",
    "        # Concatenate data in stability_dict\n",
    "        for step in steps:\n",
    "            for seed in seeds:\n",
    "                for stat_name,data in stability_dict_raw[seed][step].items():\n",
    "                    stability_dict[step][stat_name] = np.vstack(\n",
    "                        [\n",
    "                            stability_dict[step][stat_name],\n",
    "                            stability_dict_raw[seed][step][stat_name],\n",
    "                        ]\n",
    "                    )\n",
    "        stability_dict_all[gsa_method] = stability_dict\n",
    "    return stability_dict_all\n",
    "\n",
    "def get_stability_dict_all_seeds(seeds, model_dir_array, filenames_stability_dict):\n",
    "    # Get data\n",
    "    str_seed = ''\n",
    "    for seed in seeds:\n",
    "        if len(str_seed) == 0:\n",
    "            str_seed = \"{}\".format(seed)\n",
    "        else:\n",
    "            str_seed += \"_{}\".format(seed)\n",
    "    S_dict = {}\n",
    "    for gsa_method, filename in filenames_stability_dict.items():\n",
    "        filename_stability_concatenated = filename.replace('seed', str_seed)\n",
    "        list_ = filename_stability_concatenated.split('.')\n",
    "        list_[-3] = \"{:d}\".format(int(list_[-3])*len(seeds))\n",
    "        filename_stability_concatenated = \".\".join(list_)\n",
    "        print(filename_stability_concatenated)\n",
    "        filepath = model_dir_array / filename_stability_concatenated\n",
    "        if filepath.exists():\n",
    "            S_dict[gsa_method] = read_pickle(filepath)\n",
    "        else:\n",
    "            stability_dict_all = create_stability_dict_from_seeds(seeds, model_dir_array, filenames_stability_dict)\n",
    "            S_dict_gsa = stability_dict_all[gsa_method]\n",
    "            S_dict[gsa_method] = S_dict_gsa\n",
    "            write_pickle(S_dict_gsa, filepath)\n",
    "    return S_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Morris\n",
    "num_params = 10000\n",
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "model_dir_array = path_base / \"{}_morris4\".format(num_params) / \"arrays\" \n",
    "seeds = [3407, 6000814]\n",
    "filenames_stability_dict_all_models = {\n",
    "    1000: {\n",
    "#         'corr': \"stability.S.correlationsGsa.randomSampling.4000Step80.60.seed.pickle\",\n",
    "#         'delt': \"stability.S.deltaGsaNr0.latinSampling.8000Step160.60.seed.pickle\",\n",
    "#         \"xgbo\": \"stability.S.xgboostGsa_Lr0.1G0Mcw30Md2RegL10RegA0Ne500Ss0.6Cbt0.3_.randomSampling.4000Step80.60.seed.pickle\",  \n",
    "    },\n",
    "    5000: {\n",
    "#         'corr': \"stability.S.correlationsGsa.randomSampling.20000Step400.60.seed.pickle\",\n",
    "#         'delt': \"stability.S.deltaGsaNr0.latinSampling.40000Step800.60.seed.pickle\",\n",
    "#         'xgbo': \"stability.S.xgboostGsa_Lr0.2G0Mcw300Md2RegL0RegA0Ne800Ss0.3Cbt0.3_.randomSampling.20000Step400.60.seed.pickle\",\n",
    "    },\n",
    "    10000: {\n",
    "#         'corr': \"stability.S.correlationsGsa.randomSampling.40000Step800.60.seed.pickle\",\n",
    "#         'delt': \"stability.S.deltaGsaNr0.latinSampling.80000Step1600.60.seed.pickle\",\n",
    "#         'xgbo': \"stability.S.xgboostGsa_Lr0.2G0Mcw600Md2RegL0RegA0Ne1500Ss0.2Cbt0.2_.randomSampling.40000Step800.60.seed.pickle\",\n",
    "    }\n",
    "}\n",
    "\n",
    "S_dict = get_stability_dict_all_seeds(seeds, model_dir_array, filenames_stability_dict_all_models[num_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for LCA\n",
    "num_params = 10000\n",
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "model_dir_array = path_base / \"lca_model_food_{}\".format(num_params) / \"arrays\" \n",
    "seeds = [92374523, 6000814]\n",
    "filenames_stability_dict_all_models = {\n",
    "    10000: {\n",
    "#         'corr': \"stability.S.correlationsGsa.randomSampling.40000Step800.60.seed.pickle\",\n",
    "#         'delt': \"stability.S.deltaGsaNr0.latinSampling.80000Step1600.60.seed.pickle\",\n",
    "        'xgbo': \"stability.S.xgboostGsa_Lr0.15G0Mcw300Md4RegL0RegA0Ne600Ss0.3Cbt0.2_.randomSampling.40000Step800.60.seed.pickle\"\n",
    "    }\n",
    "}\n",
    "\n",
    "S_dict = get_stability_dict_all_seeds(seeds, model_dir_array, filenames_stability_dict_all_models[num_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove old stability files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.models.test_functions import Morris4\n",
    "from gsa_framework.sensitivity_methods.delta import delta_indices_stability\n",
    "from gsa_framework.sensitivity_methods.gradient_boosting import xgboost_indices_stability\n",
    "from gsa_framework.sensitivity_analysis.delta import Delta\n",
    "from gsa_framework.sensitivity_analysis.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.convergence_robustness_validation import Convergence\n",
    "from gsa_framework.utils import *\n",
    "from gsa_framework.sampling.get_samples import latin_hypercube_samples\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "setup_xgbo = setup_xgbo_morris4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 10000\n",
    "iter_delt = 8*num_params\n",
    "iter_xgbo = 4*num_params\n",
    "gsa_delt = setup_delt(num_params, iter_delt, setup_morris4_model, path_base)\n",
    "gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_morris4_model, path_base)\n",
    "\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "option = 'delta'\n",
    "if option=='delta':\n",
    "    gsa = gsa_delt\n",
    "elif option=='xgboost':\n",
    "    gsa = gsa_xgbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 60\n",
    "gsa_seed_prev = 3407\n",
    "filepath_Y_prev = gsa.write_dir / \"arrays\" / \"Y.{}.{}.{}.hdf5\".format(\n",
    "    gsa.sampling_label,\n",
    "    gsa.iterations,\n",
    "    gsa_seed_prev,\n",
    ")\n",
    "\n",
    "conv = Convergence(\n",
    "    filepath_Y_prev,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir_convergence,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")\n",
    "\n",
    "num_times = n_workers // num_bootstrap\n",
    "i = 0\n",
    "for i_iter in range(len(conv.iterations_for_convergence)//num_times+1):\n",
    "    iterations_current_multiple = conv.iterations_for_convergence[i_iter*num_times:(i_iter+1)*num_times]\n",
    "    for iterations_current in iterations_current_multiple:\n",
    "        for j in range(num_bootstrap):\n",
    "            stability_seed = stability_seeds[i,j]\n",
    "            # Write Y\n",
    "            filepath_Y_ij = gsa.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "            if filepath_Y_ij.exists():\n",
    "                print(\"deleting Y\") \n",
    "                filepath_Y_ij.unlink(missing_ok=True)\n",
    "            else:\n",
    "                print(\"{} not found\".format(filepath_Y_ij.name))  \n",
    "            # Model evals\n",
    "            filepath_S_current = gsa.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "            if filepath_S_current.exists():\n",
    "                print(\"deleting S\") \n",
    "                filepath_S_current.unlink(missing_ok=True)\n",
    "            else:\n",
    "                print(\"{} not found\".format(filepath_S_current.name)) \n",
    "        i += 1\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "# Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "# model_evals = []\n",
    "# for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "#     model_evals_bootstrap_j = []\n",
    "#     for j in range(num_bootstrap):\n",
    "#         stability_seed = stability_seeds[i,j]\n",
    "#         np.random.seed(stability_seed)\n",
    "#         choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "#         # Write Y\n",
    "#         filepath_Y_ij = gsa.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "#         if not filepath_Y_ij.exists():\n",
    "#             Y_ij = Y[choice]\n",
    "#             write_pickle(Y_ij, filepath_Y_ij)\n",
    "#         else:\n",
    "# #             print(\"{} already exists\".format(filepath_Y_ij.name))  \n",
    "#             pass\n",
    "#         # Model evals\n",
    "#         model_eval = task_per_worker(num_params, iterations_current, stability_seed)\n",
    "#         model_evals_bootstrap_j.append(model_eval)\n",
    "#     model_evals.append(model_evals_bootstrap_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_Y_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsa-dev]",
   "language": "python",
   "name": "conda-env-gsa-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
