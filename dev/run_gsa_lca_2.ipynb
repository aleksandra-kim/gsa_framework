{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from pathlib import Path\n",
    "import os\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_pc = \"merlin_paper_gsa\"\n",
    "if 'merlin' in which_pc:\n",
    "    path_dask_logs = Path('/data/user/kim_a/dask_logs')\n",
    "    path_dask_logs.mkdir(parents=True, exist_ok=True)\n",
    "    cluster = SLURMCluster(cores     = 8,\n",
    "                           memory    =\"60GB\", \n",
    "                           walltime  = '10:00:00',\n",
    "                           interface ='ib0',\n",
    "                           local_directory = path_dask_logs.as_posix(),\n",
    "                           log_directory   = path_dask_logs.as_posix(),\n",
    "                           queue=\"daily\",\n",
    "                           ) \n",
    "elif 'local' in which_pc:\n",
    "    cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 2\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GSA setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 10000\n",
    "iter_corr = 4*num_params\n",
    "iter_salt = 40*num_params\n",
    "iter_delt = 4*num_params\n",
    "iter_xgbo = 4*num_params\n",
    "\n",
    "n_workers_corr = 4\n",
    "n_workers_salt = 39\n",
    "n_workers_delt = 4\n",
    "n_workers_xgbo = 4\n",
    "\n",
    "options = {\n",
    "    'corr': {\n",
    "        \"iterations\": iter_corr,\n",
    "        \"n_workers\":  n_workers_corr,\n",
    "    }, \n",
    "    'salt': {\n",
    "        \"iterations\": iter_salt,\n",
    "        \"n_workers\": n_workers_salt,\n",
    "    }, \n",
    "    'delt': {\n",
    "        \"iterations\": iter_delt,\n",
    "        \"n_workers\": n_workers_delt,\n",
    "    },\n",
    "    'xgbo': {\n",
    "        \"iterations\": iter_delt,\n",
    "        \"n_workers\": n_workers_delt,\n",
    "    }\n",
    "}\n",
    "gsa_corr = setup_corr(num_params, iter_corr, setup_lca_model_paper)\n",
    "gsa_salt = setup_salt(num_params, iter_salt, setup_lca_model_paper)\n",
    "gsa_delt = setup_delt(num_params, iter_delt, setup_lca_model_paper)\n",
    "gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_lca_model_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write_X_chunks(gsa_corr, n_workers_corr)\n",
    "write_X_chunks(gsa_salt, n_workers_salt)\n",
    "write_X_chunks(gsa_delt, n_workers_delt)\n",
    "# write_X_chunks(gsa_xgbo, n_workers_xgbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute model outputs for all gsa methods with dask\n",
    "task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "model_evals = []\n",
    "for option,dict_ in options.items():\n",
    "    iterations = dict_[\"iterations\"]\n",
    "    n_workers = dict_[\"n_workers\"]\n",
    "    for i in range(n_workers):\n",
    "        print(option, num_params, iterations, i, n_workers)\n",
    "        model_eval = task_per_worker(option, num_params, iterations, i, n_workers, setup_lca_model_paper)\n",
    "        model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Collect model Y chunks into one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_output_from_chunks(gsa, n_workers):\n",
    "    Y = np.zeros(\n",
    "        shape=(0,)\n",
    "    )\n",
    "    for i in range(n_workers):\n",
    "        filepath_Y_chunk = (\n",
    "            gsa.dirpath_Y\n",
    "            / \"{}.{}.pickle\".format(i, n_workers)\n",
    "        )\n",
    "        Y_chunk = read_pickle(filepath_Y_chunk)\n",
    "        Y = np.hstack(\n",
    "            [Y, Y_chunk]\n",
    "        )  # TODO change to vstack for multidimensional output\n",
    "    write_hdf5_array(Y,gsa.filepath_Y)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ycorr = generate_model_output_from_chunks(gsa_corr, n_workers_corr)\n",
    "Ysalt = generate_model_output_from_chunks(gsa_salt, n_workers_salt)\n",
    "Ydelt = generate_model_output_from_chunks(gsa_delt, n_workers_delt)\n",
    "Yxgbo = generate_model_output_from_chunks(gsa_xgbo, n_workers_xgbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run GSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa_delt.generate_unitcube_samples(return_X=False)\n",
    "gsa_delt.generate_rescaled_samples(return_X=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa_xgbo.generate_unitcube_samples(return_X=False)\n",
    "gsa_xgbo.generate_rescaled_samples(return_X=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gsa_corr.perform_gsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gsa_salt.perform_gsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_delt = dask.delayed(gsa_delt.perform_gsa)\n",
    "model_eval_delt = worker_delt()\n",
    "worker_xgbo = dask.delayed(gsa_xgbo.perform_gsa)\n",
    "model_eval_xgbo = worker_xgbo()\n",
    "model_evals = [model_eval_delt, model_eval_xgbo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.validation import Validation\n",
    "import dask\n",
    "\n",
    "model, write_dir, gsa_seed = setup_lca_model(num_params)\n",
    "validation_seed = 23467\n",
    "num_influential = 60\n",
    "\n",
    "iterations_validation = 2000\n",
    "val = Validation(\n",
    "    model=model,\n",
    "    iterations=iterations_validation,\n",
    "    seed=validation_seed,\n",
    "    default_x_rescaled=None,\n",
    "    write_dir=write_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_validation =  dask.delayed(val.get_influential_Y_from_gsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_dict = gsa_corr.generate_gsa_indices()\n",
    "Scorr = abs(S_dict['spearman'])\n",
    "tag_corr = \"SpearmanIndex\"\n",
    "model_eval_corr = worker_validation(Scorr, num_influential, tag_corr)\n",
    "\n",
    "S_dict = gsa_salt.generate_gsa_indices()\n",
    "Ssalt = S_dict['Total order']\n",
    "tag_salt = \"TotalIndex\"\n",
    "model_eval_salt = worker_validation(Ssalt, num_influential, tag_salt)\n",
    "\n",
    "S_dict = gsa_delt.generate_gsa_indices()\n",
    "Sdelt = np.array(S_dict['delta'])\n",
    "tag_delt = \"DeltaIndexNr{}\".format(gsa_delt.num_resamples)\n",
    "model_eval_delt = worker_validation(Sdelt, num_influential, tag_delt)\n",
    "\n",
    "S_dict = gsa_xgbo.generate_gsa_indices()\n",
    "Sxgbo = S_dict['fscores']\n",
    "tag_xgbo = \"FscoresIndex\"\n",
    "model_eval_xgbo = worker_validation(Sxgbo, num_influential, tag_xgbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evals = [\n",
    "    model_eval_corr,\n",
    "    model_eval_salt,\n",
    "    model_eval_delt, \n",
    "    model_eval_xgbo,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_format = ['pickle']\n",
    "\n",
    "influential_Y_corr = val.get_influential_Y_from_gsa(Scorr, num_influential, tag=tag_corr)\n",
    "val.plot_histogram_Y_all_Y_inf(\n",
    "    influential_Y_corr, num_influential, tag=tag_corr, fig_format=fig_format\n",
    ")\n",
    "\n",
    "influential_Y_salt = val.get_influential_Y_from_gsa(Ssalt, num_influential, tag=tag_salt)\n",
    "val.plot_histogram_Y_all_Y_inf(\n",
    "    influential_Y_salt, num_influential, tag=tag_salt, fig_format=fig_format\n",
    ")\n",
    "\n",
    "influential_Y_delt = val.get_influential_Y_from_gsa(Sdelt, num_influential, tag=tag_delt)\n",
    "val.plot_histogram_Y_all_Y_inf(\n",
    "    influential_Y_delt, num_influential, tag=tag_delt, fig_format=fig_format\n",
    ")\n",
    "\n",
    "influential_Y_xgbo = val.get_influential_Y_from_gsa(Sxgbo, num_influential, tag=tag_xgbo)\n",
    "val.plot_histogram_Y_all_Y_inf(\n",
    "    influential_Y_xgbo, num_influential, tag=tag_xgbo, fig_format=fig_format\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_worker_delt(iterations_current, stability_seed, write_dir_stability):\n",
    "    num_params = 10000\n",
    "    iter_delt = 4*num_params\n",
    "    filepath_Y = write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    Y = read_pickle(filepath_Y).flatten()\n",
    "    gsa_delt = setup_delt(num_params, iter_delt)\n",
    "    np.random.seed(gsa_delt.seed)\n",
    "    X = np.random.rand(iter_delt, num_params)\n",
    "    np.random.seed(stability_seed)\n",
    "    choice = np.random.choice(np.arange(iter_delt), iterations_current, replace=False)\n",
    "    Xr = gsa_delt.model.rescale(X[choice, :])\n",
    "    del X\n",
    "    filepath_S = write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    if not filepath_S.exists():\n",
    "        S_dict = delta_moment_stability(\n",
    "            Y, Xr, num_resamples=1\n",
    "        )\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "    else:\n",
    "        S_dict = read_pickle(filepath_S)\n",
    "    \n",
    "    return S_dict\n",
    "\n",
    "def compute_per_worker_xgbo(iterations_current, stability_seed, write_dir_stability):\n",
    "    num_params = 10000\n",
    "    iter_xgbo = 4*num_params\n",
    "    filepath_Y = write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    Y = read_pickle(filepath_Y).flatten()\n",
    "    gsa_xgbo = setup_xgbo(num_params, iter_xgbo)\n",
    "    np.random.seed(gsa_xgbo.seed)\n",
    "    X = np.random.rand(iter_xgbo, num_params)\n",
    "    np.random.seed(stability_seed)\n",
    "    choice = np.random.choice(np.arange(iter_xgbo), iterations_current, replace=False)\n",
    "    Xr = gsa_xgbo.model.rescale(X[choice, :])\n",
    "    del X\n",
    "    filepath_S = write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    if not filepath_S.exists():\n",
    "        S_dict = xgboost_scores_stability(\n",
    "            Y,\n",
    "            Xr,\n",
    "            tuning_parameters=gsa_xgbo.tuning_parameters,\n",
    "            num_boost_round=gsa_xgbo.num_boost_round,\n",
    "        )\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "    else:\n",
    "        S_dict = read_pickle(filepath_S)\n",
    "    \n",
    "    return S_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "option = 'delta'\n",
    "if option=='delta':\n",
    "    gsa = gsa_delt\n",
    "    compute_per_worker = compute_per_worker_delt\n",
    "elif option=='xgboost':\n",
    "    gsa = gsa_xgbo\n",
    "    compute_per_worker = compute_per_worker_xgbo\n",
    "\n",
    "task_per_worker = dask.delayed(compute_per_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dir_scratch = Path(\"/shared-scratch/kim_a\")\n",
    "write_dir_stability = write_dir_scratch / 'stability_intermediate_{}'.format(gsa.gsa_label)\n",
    "write_dir_stability.mkdir(parents=True, exist_ok=True)\n",
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    write_dir_scratch,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")\n",
    "\n",
    "X_rescaled = read_hdf5_array(gsa.filepath_X_rescaled)\n",
    "Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "model_evals = []\n",
    "for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "    model_evals_bootstrap_j = []\n",
    "    for j in range(num_bootstrap):\n",
    "        stability_seed = stability_seeds[i,j]\n",
    "        np.random.seed(stability_seed)\n",
    "        choice = np.random.choice(np.arange(X_rescaled.shape[0]), iterations_current, replace=False)\n",
    "        # Write Y\n",
    "        filepath_Y_ij = write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "        if not filepath_Y_ij.exists():\n",
    "            Y_ij = Y[choice]\n",
    "            write_pickle(Y_ij, filepath_Y_ij)\n",
    "        else:\n",
    "#             print(\"{} already exists\".format(filepath_Y_ij.name))  \n",
    "            pass\n",
    "        # Model evals\n",
    "        model_eval = task_per_worker(iterations_current, stability_seed, write_dir_stability)\n",
    "        model_evals_bootstrap_j.append(model_eval)\n",
    "    model_evals.append(model_evals_bootstrap_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_evals_bootstrap_j in model_evals:\n",
    "    dask.compute(model_evals_bootstrap_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Construct LCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.extended_FAST import eFAST\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.validation import Validation\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.plotting import histogram_Y1_Y2\n",
    "from gsa_framework.utils import read_hdf5_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "#     )\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    demand_act = [act for act in co if \"Food and non-alcoholic beverages sector\" in act['name']][0]\n",
    "    print(demand_act)\n",
    "    demand = {demand_act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GWP 100a\")\n",
    "\n",
    "    # Define some variables\n",
    "    num_params = 162299\n",
    "    iterations_validation = 2000\n",
    "    write_dir = path_base / \"lca_model_food_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir) # TODO add num_params later\n",
    "    gsa_seed = 3403\n",
    "    validation_seed = 7043\n",
    "    fig_format = [\"html\", \"pickle\"]\n",
    "\n",
    "    # Make sure  that the chosen num_params in LCA are appropriate\n",
    "    val = Validation(\n",
    "        model=model,\n",
    "        iterations=iterations_validation,\n",
    "        seed=4444,\n",
    "        default_x_rescaled=model.default_uncertain_amounts,\n",
    "        write_dir=write_dir,\n",
    "    )\n",
    "    num_params_paper = 10000\n",
    "    tag = \"numParams{}\".format(num_params_paper)\n",
    "    scores_dict = model.get_lsa_scores_pickle(model.write_dir / \"LSA_scores\")\n",
    "    uncertain_tech_params_where_subset, _ = model.get_nonzero_params_from_num_params(scores_dict, num_params_paper)\n",
    "    parameter_choice = []\n",
    "    for u in uncertain_tech_params_where_subset:\n",
    "        where_temp = np.where(model.uncertain_tech_params_where == u)[0]\n",
    "        assert len(where_temp) == 1\n",
    "        parameter_choice.append(where_temp[0])\n",
    "    parameter_choice.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_subset = val.get_influential_Y_from_parameter_choice(parameter_choice=parameter_choice, tag=tag)\n",
    "val.plot_histogram_Y_all_Y_inf(Y_subset, num_influential=num_params_paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsa-dev]",
   "language": "python",
   "name": "conda-env-gsa-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
