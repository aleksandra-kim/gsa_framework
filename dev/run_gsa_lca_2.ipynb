{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from pathlib import Path\n",
    "import os\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_pc = \"merlin_paper_gsa\"\n",
    "if 'merlin' in which_pc:\n",
    "    path_dask_logs = Path('/data/user/kim_a/dask_logs')\n",
    "    path_dask_logs.mkdir(parents=True, exist_ok=True)\n",
    "    cluster = SLURMCluster(cores     = 8,\n",
    "                           memory    =\"140GB\", \n",
    "                           walltime  = '23:00:00',\n",
    "                           interface ='ib0',\n",
    "                           local_directory = path_dask_logs.as_posix(),\n",
    "                           log_directory   = path_dask_logs.as_posix(),\n",
    "                           queue=\"daily\",\n",
    "                           ) \n",
    "elif 'local' in which_pc:\n",
    "    cluster = LocalCluster(memory_limit='7GB') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 60\n",
    "cluster.scale(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()\n",
    "# cluster.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GSA setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using environment variable BRIGHTWAY2_DIR for data directory:\n",
      "/data/user/kim_a/Brightway3\n"
     ]
    }
   ],
   "source": [
    "from setups_paper_gwp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-075f67aea872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/user/kim_a/paper_gsa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msetup_lca_model_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_generate_scores_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'num_params' is not defined"
     ]
    }
   ],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa')\n",
    "num_params = 10000\n",
    "uncertain_exchanges_types = [\n",
    "    \"tech\",\n",
    "]\n",
    "model = LCAModel(\n",
    "    demand,\n",
    "    method,\n",
    "    write_dir,\n",
    "    num_params=num_params,\n",
    "    uncertain_exchanges_types=uncertain_exchanges_types,\n",
    ")\n",
    "    \n",
    "# setup_lca_model_paper(path_base, num_params, flag_generate_scores_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa')\n",
    "setup_xgbo = setup_xgbo_lca\n",
    "\n",
    "num_params = 10000\n",
    "iter_corr = 4*num_params\n",
    "iter_salt = 40*num_params\n",
    "iter_delt = 8*num_params\n",
    "iter_xgbo = 4*num_params\n",
    "\n",
    "n_workers_corr = 4\n",
    "n_workers_salt = 39\n",
    "n_workers_delt = 50\n",
    "n_workers_xgbo = 4\n",
    "\n",
    "options = {\n",
    "    'corr': {\n",
    "        \"iterations\": iter_corr,\n",
    "        \"n_workers\":  n_workers_corr,\n",
    "    }, \n",
    "    'salt': {\n",
    "        \"iterations\": iter_salt,\n",
    "        \"n_workers\": n_workers_salt,\n",
    "    }, \n",
    "    'delt': {\n",
    "        \"iterations\": iter_delt,\n",
    "        \"n_workers\": n_workers_delt,\n",
    "    },\n",
    "    'xgbo': {\n",
    "        \"iterations\": iter_delt,\n",
    "        \"n_workers\": n_workers_delt,\n",
    "    }\n",
    "}\n",
    "# gsa_corr = setup_corr(num_params, iter_corr, setup_lca_model_paper, path_base)\n",
    "# gsa_salt = setup_salt(num_params, iter_salt, setup_lca_model_paper, path_base)\n",
    "# gsa_delt = setup_delt(num_params, iter_delt, setup_lca_model_paper, path_base)\n",
    "gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_lca_model_paper, path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa_xgbo.perform_gsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# write_X_chunks(gsa_corr, n_workers_corr)\n",
    "# write_X_chunks(gsa_salt, n_workers_salt)\n",
    "# write_X_chunks(gsa_delt, n_workers_delt)\n",
    "# write_X_chunks(gsa_xgbo, n_workers_xgbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute model outputs for all gsa methods with dask\n",
    "# task_per_worker = dask.delayed(compute_scores_per_worker)\n",
    "# model_evals = []\n",
    "# for option,dict_ in options.items():\n",
    "#     iterations = dict_[\"iterations\"]\n",
    "#     n_workers = dict_[\"n_workers\"]\n",
    "#     for i in range(n_workers):\n",
    "#         print(option, num_params, iterations, i, n_workers)\n",
    "#         model_eval = task_per_worker(option, num_params, iterations, i, n_workers, setup_lca_model_paper, path_base)\n",
    "#         model_evals.append(model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Collect model Y chunks into one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_model_output_from_chunks(gsa, n_workers):\n",
    "#     Y = np.zeros(\n",
    "#         shape=(0,)\n",
    "#     )\n",
    "#     for i in range(n_workers):\n",
    "#         filepath_Y_chunk = (\n",
    "#             gsa.dirpath_Y\n",
    "#             / \"{}.{}.pickle\".format(i, n_workers)\n",
    "#         )\n",
    "#         Y_chunk = read_pickle(filepath_Y_chunk)\n",
    "#         Y = np.hstack(\n",
    "#             [Y, Y_chunk]\n",
    "#         )  # TODO change to vstack for multidimensional output\n",
    "#     write_hdf5_array(Y,gsa.filepath_Y)\n",
    "#     return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ycorr = generate_model_output_from_chunks(gsa_corr, n_workers_corr)\n",
    "# Ysalt = generate_model_output_from_chunks(gsa_salt, n_workers_salt)\n",
    "# Ydelt = generate_model_output_from_chunks(gsa_delt, n_workers_delt)\n",
    "# Yxgbo = generate_model_output_from_chunks(gsa_xgbo, n_workers_xgbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run GSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa_delt.generate_unitcube_samples(return_X=False)\n",
    "gsa_delt.generate_rescaled_samples(return_X=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsa_xgbo.generate_unitcube_samples(return_X=False)\n",
    "gsa_xgbo.generate_rescaled_samples(return_X=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gsa_corr.perform_gsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gsa_salt.perform_gsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gsa_delt.perform_gsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gsa_xgbo.perform_gsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worker_delt = dask.delayed(gsa_delt.perform_gsa)\n",
    "# model_eval_delt = worker_delt()\n",
    "# worker_xgbo = dask.delayed(gsa_xgbo.perform_gsa)\n",
    "# model_eval_xgbo = worker_xgbo()\n",
    "# model_evals = [model_eval_delt, model_eval_xgbo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.validation import Validation\n",
    "from setups_paper_gwp import *\n",
    "import dask\n",
    "\n",
    "path_base = Path('/data/user/kim_a/paper_gsa')\n",
    "num_params = 10000\n",
    "iter_corr = 4*num_params\n",
    "iter_salt = 40*num_params\n",
    "iter_delt = 8*num_params\n",
    "iter_xgbo = 4*num_params\n",
    "\n",
    "model, write_dir, gsa_seed = setup_lca_model_paper(\n",
    "    path_base, num_params\n",
    ")\n",
    "# gsa_corr = setup_corr(num_params, iter_corr, setup_lca_model_paper, path_base)\n",
    "# gsa_salt = setup_salt(num_params, iter_salt, setup_lca_model_paper, path_base)\n",
    "gsa_delt = setup_delt(num_params, iter_delt, setup_lca_model_paper, path_base)\n",
    "gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_lca_model_paper, path_base)\n",
    "\n",
    "validation_seed = 23467\n",
    "num_influential = 60\n",
    "iterations_validation = 2000\n",
    "val = Validation(\n",
    "    model=model,\n",
    "    iterations=iterations_validation,\n",
    "    seed=validation_seed,\n",
    "    default_x_rescaled=None,\n",
    "    write_dir=write_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worker_validation =  dask.delayed(val.get_influential_Y_from_gsa)\n",
    "worker_validation =  val.get_influential_Y_from_gsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S_dict = gsa_corr.generate_gsa_indices()\n",
    "# Scorr = abs(S_dict['spearman'])\n",
    "# tag_corr = \"SpearmanIndex\"\n",
    "# model_eval_corr = worker_validation(Scorr, num_influential, tag_corr)\n",
    "\n",
    "# S_dict = gsa_salt.generate_gsa_indices()\n",
    "# Ssalt = S_dict['Total order']\n",
    "# tag_salt = \"TotalIndex\"\n",
    "# model_eval_salt = worker_validation(Ssalt, num_influential, tag_salt)\n",
    "\n",
    "# S_dict = gsa_delt.generate_gsa_indices()\n",
    "# Sdelt = np.array(S_dict['delta'])\n",
    "# tag_delt = \"DeltaIndexNr{}\".format(gsa_delt.num_resamples)\n",
    "# model_eval_delt = worker_validation(Sdelt, num_influential, tag_delt)\n",
    "\n",
    "S_dict = gsa_xgbo.generate_gsa_indices()\n",
    "Sxgbo = S_dict['total_gain']\n",
    "tag_xgbo = \"TotalGain\"\n",
    "model_eval_xgbo = worker_validation(Sxgbo, num_influential, tag_xgbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evals = [\n",
    "    model_eval_corr,\n",
    "    model_eval_salt,\n",
    "    model_eval_delt, \n",
    "    model_eval_xgbo,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(model_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_format = ['pickle']\n",
    "\n",
    "# influential_Y_corr = val.get_influential_Y_from_gsa(Scorr, num_influential, tag=tag_corr)\n",
    "# val.plot_histogram_Y_all_Y_inf(\n",
    "#     influential_Y_corr, num_influential, tag=tag_corr, fig_format=fig_format\n",
    "# )\n",
    "\n",
    "# influential_Y_salt = val.get_influential_Y_from_gsa(Ssalt, num_influential, tag=tag_salt)\n",
    "# val.plot_histogram_Y_all_Y_inf(\n",
    "#     influential_Y_salt, num_influential, tag=tag_salt, fig_format=fig_format\n",
    "# )\n",
    "\n",
    "influential_Y_delt = val.get_influential_Y_from_gsa(Sdelt, num_influential, tag=tag_delt)\n",
    "val.plot_histogram_Y_all_Y_inf(\n",
    "    influential_Y_delt, num_influential, tag=tag_delt, fig_format=fig_format\n",
    ")\n",
    "\n",
    "influential_Y_xgbo = val.get_influential_Y_from_gsa(Sxgbo, num_influential, tag=tag_xgbo)\n",
    "val.plot_histogram_Y_all_Y_inf(\n",
    "    influential_Y_xgbo, num_influential, tag=tag_xgbo, fig_format=fig_format\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gsa_framework.test_functions import Morris4\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from setups_paper_gwp import setup_lca_model_paper\n",
    "from gsa_framework.utils import read_hdf5_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "#     path_base = Path(\"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/\")\n",
    "\n",
    "    # 1. Models\n",
    "    num_params = 10000\n",
    "    model, write_dir, gsa_seed = setup_lca_model_paper(path_base, num_params)\n",
    "    filepath_Xr = write_dir / \"arrays\" / \"X.rescaled.randomSampling.40000.92374523.hdf5\"\n",
    "    filepath_Y = write_dir / \"arrays\" / \"Y.randomSampling.40000.92374523.hdf5\"\n",
    "    X = read_hdf5_array(filepath_Xr)\n",
    "    Y = read_hdf5_array(filepath_Y).flatten()\n",
    "    fig_format = []  # can have elements \"pdf\", \"html\", \"pickle\"\n",
    "\n",
    "    test_size = 0.2\n",
    "\n",
    "    option = \"gsa\"\n",
    "    if \"tuning\" in option:\n",
    "        # 1. Preparations\n",
    "        np.random.seed(gsa_seed)\n",
    "#         X_unitcube = np.random.rand(iterations, num_params)\n",
    "#         X = model.rescale(X_unitcube)\n",
    "#         Y = model(X)\n",
    "        # 2. Prepare training and testing sets for  gradient boosting trees\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X,\n",
    "            Y,\n",
    "            test_size=test_size,\n",
    "            random_state=gsa_seed,\n",
    "        )\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, Y_train)\n",
    "        X_dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "        if option == \"tuning\":\n",
    "            ### ROUND 1 ###\n",
    "            # xgb.train uses parameter `num_boost_round`, while XGBRegressor needs `n_estimators`. These two are the same.\n",
    "            param_grid = {\n",
    "                \"learning_rate\": [0.15],\n",
    "                \"gamma\": [0],\n",
    "                \"min_child_weight\": [60, 100, 140],\n",
    "                \"max_depth\": [2],\n",
    "                \"reg_lambda\": [0, 10],\n",
    "                \"reg_alpha\": [0, 10],\n",
    "                \"n_estimators\": [500, 800, 1100],\n",
    "                \"subsample\": [0.3, 0.6],\n",
    "                \"colsample_bytree\": [0.3, 0.6],\n",
    "            }\n",
    "\n",
    "            optimal_params = GridSearchCV(\n",
    "                estimator=xgb.XGBRegressor(\n",
    "                    objective=\"reg:squarederror\",\n",
    "                    seed=gsa_seed,\n",
    "                ),\n",
    "                param_grid=param_grid,\n",
    "                scoring=\"explained_variance\",  # explained_variance takes into account mean squared error, r2 does not. former is unbiasede, so better than r2\n",
    "                cv=3,\n",
    "            )\n",
    "            optimal_params.fit(\n",
    "                X_train,\n",
    "                Y_train,\n",
    "                early_stopping_rounds=10,\n",
    "                eval_set=[(X_test, Y_test)],\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            print(optimal_params.best_params_)\n",
    "\n",
    "            import pickle\n",
    "\n",
    "            filepath = write_dir / \"arrays\" / \"optimal_params_round_1.pickle\"\n",
    "            if filepath.exists():\n",
    "                filepath = write_dir / \"arrays\" / \"optimal_params_round_2.pickle\"\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                pickle.dump(optimal_params, f)\n",
    "\n",
    "        elif option == \"no tuning\":\n",
    "            np.random.seed(None)\n",
    "            reg = xgb.XGBRegressor(\n",
    "                verbosity=1,  # 0 (silent), 1 (warning), 2 (info), 3 (debug)\n",
    "                objective=\"reg:squarederror\",\n",
    "                seed=gsa_seed,\n",
    "                learning_rate=0.15,\n",
    "                gamma=0,\n",
    "                min_child_weight=300,\n",
    "                max_depth=4,\n",
    "                reg_lambda=0,\n",
    "                reg_alpha=0,\n",
    "                n_estimators=600,\n",
    "                subsample=0.3,\n",
    "                colsample_bytree=0.3,\n",
    "            )\n",
    "            reg.fit(X_train, Y_train)\n",
    "            ev_train = explained_variance_score(reg.predict(X_train), Y_train)\n",
    "            ev_test = explained_variance_score(reg.predict(X_test), Y_test)\n",
    "            print(ev_train, ev_test)\n",
    "\n",
    "    else:\n",
    "        iterations = 4 * num_params\n",
    "        tuning_parameters = dict(\n",
    "            learning_rate=0.15,\n",
    "            gamma=0,\n",
    "            min_child_weight=300,\n",
    "            max_depth=4,\n",
    "            reg_lambda=0,\n",
    "            reg_alpha=0,\n",
    "            n_estimators=600,\n",
    "            subsample=0.3,\n",
    "            colsample_bytree=0.3,\n",
    "        )\n",
    "        gsa = GradientBoosting(\n",
    "            iterations=iterations,\n",
    "            model=model,\n",
    "            write_dir=write_dir,\n",
    "            seed=gsa_seed,\n",
    "            tuning_parameters=tuning_parameters,\n",
    "            test_size=test_size,\n",
    "            xgb_model=None,\n",
    "        )\n",
    "\n",
    "        S_dict = gsa.perform_gsa(flag_save_S_dict=True)\n",
    "        print(S_dict[\"stat.r2\"], S_dict[\"stat.explained_variance\"])\n",
    "        gsa.plot_sa_results(\n",
    "            {\"fscores\": S_dict[\"fscores\"]},\n",
    "            fig_format=fig_format,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.sensitivity_analysis.correlations import corrcoef_parallel_stability_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X and Y\n",
    "num_params = 10000\n",
    "iter_corr = 4*num_params\n",
    "gsa_corr = setup_corr(num_params, iter_corr, setup_lca_model_paper, path_base)\n",
    "X_rescaled = read_hdf5_array(gsa_corr.filepath_X_rescaled)\n",
    "Y = read_hdf5_array(gsa_corr.filepath_Y).flatten()\n",
    "\n",
    "gsa = deepcopy(gsa_corr)\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "# Convergence class\n",
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "write_dir_stability = gsa.write_dir / 'stability_intermediate_{}'.format(gsa.gsa_label)\n",
    "write_dir_stability.mkdir(parents=True, exist_ok=True)\n",
    "# Generate random seeds\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "    gsa.gsa_label, gsa.sampling_label, gsa.iterations, conv.iterations_step, num_bootstrap, gsa.seed,\n",
    ")\n",
    "filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "if filepath_S.exists():\n",
    "    print(\"--> {} already exists\".format(filename_S))\n",
    "    S_dict_stability = read_pickle(filepath_S)\n",
    "else:\n",
    "    S_dict_stability = {}\n",
    "    for i,iterations_current in enumerate(conv.iterations_for_convergence):\n",
    "        S_array = np.zeros([0,num_params])\n",
    "        print(\"{}\".format(iterations_current))\n",
    "        filename_S_current = \"S.{}Step{}.{}.{}.pickle\".format(iterations_current,conv.iterations_step,num_bootstrap,gsa.seed)\n",
    "        filepath_S_current = write_dir_stability / filename_S_current\n",
    "        if filepath_S_current.exists():\n",
    "            print(\"--> {} already exists\".format(filename_S_current))\n",
    "            S_dict = read_pickle(filepath_S_current)\n",
    "        else:\n",
    "            for j in range(num_bootstrap):\n",
    "                stability_seed = stability_seeds[i,j]\n",
    "                np.random.seed(stability_seed)\n",
    "                choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=False)\n",
    "                Y_current = Y[choice]\n",
    "                X_current = X_rescaled[choice,:]\n",
    "                S_current = corrcoef_parallel_stability_spearman(Y_current, X_current)['spearman']\n",
    "                S_array = np.vstack([S_array, S_current])\n",
    "            S_dict = {iterations_current: {\"spearman\": S_array}}\n",
    "            write_pickle(S_dict, filepath_S_current)\n",
    "        S_dict_stability.update(S_dict)\n",
    "    write_pickle(S_dict_stability, filepath_S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Delta and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setups_paper_gwp import *\n",
    "from copy import deepcopy\n",
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.sensitivity_analysis.delta_moment import delta_moment_stability\n",
    "from gsa_framework.sensitivity_analysis.gradient_boosting import xgboost_scores_stability\n",
    "from gsa_framework.methods.delta_moment import DeltaMoment\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.convergence import Convergence\n",
    "from gsa_framework.utils import *\n",
    "from gsa_framework.sampling.get_samples import latin_hypercube_samples\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_base = Path('/data/user/kim_a/paper_gsa/')\n",
    "setup_xgbo = setup_xgbo_lca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_worker_delt(num_params, iterations_current, stability_seed):\n",
    "    # This part might be different for morris and lca\n",
    "    iter_delt = 8*num_params\n",
    "    gsa_delt = setup_delt(num_params, iter_delt, setup_lca_model_paper, path_base)\n",
    "    # The rest is the same for both\n",
    "    filepath_Y = gsa_delt.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    Y = read_pickle(filepath_Y).flatten()\n",
    "    X = latin_hypercube_samples(gsa_delt.iterations, gsa_delt.num_params, seed=gsa_delt.seed)\n",
    "    np.random.seed(stability_seed)\n",
    "    choice = np.random.choice(np.arange(gsa_delt.iterations), iterations_current, replace=False)\n",
    "    Xr = gsa_delt.model.rescale(X[choice, :])\n",
    "    del X\n",
    "    filepath_S = gsa_delt.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    if not filepath_S.exists():\n",
    "        S_dict = delta_moment_stability(\n",
    "            Y, Xr, num_resamples=gsa_delt.num_resamples, seed=stability_seed\n",
    "        )\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "    else:\n",
    "        print(\"{} already exists\".format(filepath_S.name))\n",
    "        S_dict = read_pickle(filepath_S)\n",
    "    \n",
    "    return S_dict\n",
    "\n",
    "def compute_per_worker_xgbo(num_params, iterations_current, stability_seed):\n",
    "    iter_xgbo =4*num_params\n",
    "    gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_lca_model_paper, path_base)\n",
    "    filepath_Y = gsa_xgbo.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    Y = read_pickle(filepath_Y).flatten()\n",
    "    np.random.seed(gsa_xgbo.seed)\n",
    "    X = np.random.rand(iter_xgbo, num_params)\n",
    "    np.random.seed(stability_seed)\n",
    "    choice = np.random.choice(np.arange(gsa_xgbo.iterations), iterations_current, replace=True)\n",
    "    Xr = gsa_xgbo.model.rescale(X[choice, :])\n",
    "    del X\n",
    "    filepath_S = gsa_xgbo.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "    if not filepath_S.exists():\n",
    "        S_dict = xgboost_scores_stability(\n",
    "            Y, \n",
    "            Xr, \n",
    "            tuning_parameters=gsa_xgbo.tuning_parameters,\n",
    "            test_size=gsa_xgbo.test_size,\n",
    "            xgb_model = gsa_xgbo.xgb_model,\n",
    "        )\n",
    "        write_pickle(S_dict, filepath_S)\n",
    "    else:\n",
    "        print(\"{} already exists\".format(filepath_S.name))\n",
    "        S_dict = read_pickle(filepath_S)\n",
    "    \n",
    "    return S_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 10000\n",
    "iter_delt = 8*num_params\n",
    "gsa_delt = setup_delt(num_params, iter_delt, setup_lca_model_paper, path_base)\n",
    "iter_xgbo = 4*num_params\n",
    "gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_lca_model_paper, path_base)\n",
    "\n",
    "num_steps = 50\n",
    "num_bootstrap = 60\n",
    "\n",
    "option = 'xgboost'\n",
    "if option=='delta':\n",
    "    gsa = gsa_delt\n",
    "    compute_per_worker = compute_per_worker_delt\n",
    "elif option=='xgboost':\n",
    "    gsa = gsa_xgbo\n",
    "    compute_per_worker = compute_per_worker_xgbo\n",
    "\n",
    "task_per_worker = dask.delayed(compute_per_worker)\n",
    "# task_per_worker = compute_per_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# compute_per_worker_xgbo(num_params, 9600, 901875159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 60\n",
    "\n",
    "conv = Convergence(\n",
    "    gsa.filepath_Y,\n",
    "    gsa.num_params,\n",
    "    gsa.generate_gsa_indices,\n",
    "    gsa.gsa_label,\n",
    "    gsa.write_dir_convergence,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "np.random.seed(gsa.seed)\n",
    "stability_seeds = np.random.randint(\n",
    "    low=0,\n",
    "    high=2147483647,\n",
    "    size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    ")\n",
    "\n",
    "Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "\n",
    "num_times = n_workers // num_bootstrap\n",
    "model_evals = []\n",
    "i = 0\n",
    "for i_iter in range(len(conv.iterations_for_convergence)//num_times+1):\n",
    "    iterations_current_multiple = conv.iterations_for_convergence[i_iter*num_times:(i_iter+1)*num_times]\n",
    "    model_evals_bootstrap_j_k = []\n",
    "    for iterations_current in iterations_current_multiple:\n",
    "        model_evals_bootstrap_j = []\n",
    "        for j in range(num_bootstrap):\n",
    "            stability_seed = stability_seeds[i,j]\n",
    "            np.random.seed(stability_seed)\n",
    "            choice = np.random.choice(np.arange(gsa.iterations), iterations_current, replace=True) \n",
    "            # Write Y\n",
    "            filepath_Y_ij = gsa.write_dir_stability / \"Y.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "            if not filepath_Y_ij.exists():\n",
    "                Y_ij = Y[choice]\n",
    "                write_pickle(Y_ij, filepath_Y_ij)\n",
    "            else:\n",
    "    #             print(\"{} already exists\".format(filepath_Y_ij.name))  \n",
    "                pass\n",
    "            # Model evals\n",
    "            filepath_S_current = gsa.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "            if not filepath_S_current.exists():\n",
    "                model_eval = task_per_worker(num_params, iterations_current, stability_seed)\n",
    "                model_evals_bootstrap_j.append(model_eval)\n",
    "        model_evals_bootstrap_j_k += model_evals_bootstrap_j\n",
    "        i += 1\n",
    "    if len(model_evals_bootstrap_j_k) > 0:\n",
    "        model_evals.append(model_evals_bootstrap_j_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i,model_evals_bootstrap_j_k in enumerate(model_evals):\n",
    "    print(i)\n",
    "    dask.compute(model_evals_bootstrap_j_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "def create_stability_dict_delt(num_params, iterations_for_convergence, stability_seeds):\n",
    "    iter_delt = 8*num_params\n",
    "    gsa_delt = setup_delt(num_params, iter_delt, setup_lca_model_paper)\n",
    "    iterations_step = iterations_for_convergence[1] - iterations_for_convergence[0]\n",
    "    num_bootstrap = stability_seeds.shape[1]\n",
    "    filename_S_stability = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "    gsa_delt.gsa_label, gsa_delt.sampling_label, gsa_delt.iterations, iterations_step, num_bootstrap, gsa_delt.seed,\n",
    "    )\n",
    "    filepath_S_stability = gsa_delt.write_dir / 'arrays' / filename_S_stability\n",
    "    if filepath_S_stability.exists():\n",
    "        print(\"{} already exists\".format(filepath_S_stability.name))  \n",
    "        S_dict = read_pickle(filepath_S_stability)\n",
    "    else:\n",
    "        S_dict = {}\n",
    "        for i,iterations_current in enumerate(iterations_for_convergence):\n",
    "            S_array = np.zeros((0,num_params))\n",
    "            for j in range(num_bootstrap):\n",
    "                stability_seed = stability_seeds[i,j]\n",
    "                filepath_S = \\\n",
    "                gsa_delt.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "                if not filepath_S.exists():\n",
    "                    print(\"{} does not exist\".format(filepath_S.name))\n",
    "                    return\n",
    "                else:\n",
    "                    S_current = read_pickle(filepath_S)\n",
    "                    S_array = np.vstack([S_array, S_current['delta']])\n",
    "            S_dict[iterations_current] = {\"delta\": S_array}\n",
    "        write_pickle(S_dict, filepath_S_stability)\n",
    "    return S_dict\n",
    "\n",
    "# Collect all results\n",
    "def create_stability_dict_xgbo(num_params, iterations_for_convergence, stability_seeds):\n",
    "    iter_xgbo = 4*num_params\n",
    "    gsa_xgbo = setup_xgbo(num_params, iter_xgbo, setup_lca_model_paper, path_base)\n",
    "    iterations_step = iterations_for_convergence[1] - iterations_for_convergence[0]\n",
    "    num_bootstrap = stability_seeds.shape[1]\n",
    "    filename_S_stability = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "    gsa_xgbo.gsa_label, gsa_xgbo.sampling_label, gsa_xgbo.iterations, iterations_step, num_bootstrap, gsa_xgbo.seed,\n",
    "    )\n",
    "    filepath_S_stability = gsa_xgbo.write_dir / 'arrays' / filename_S_stability\n",
    "    if filepath_S_stability.exists():\n",
    "        print(\"{} already exists\".format(filepath_S_stability.name))  \n",
    "        S_dict = read_pickle(filepath_S_stability)\n",
    "    else:\n",
    "        S_dict = {}\n",
    "        for i,iterations_current in enumerate(iterations_for_convergence):\n",
    "            S_dict_arrays = {}\n",
    "            for j in range(num_bootstrap):\n",
    "                stability_seed = stability_seeds[i,j]\n",
    "                filepath_S = \\\n",
    "                gsa_xgbo.write_dir_stability / \"S.step{}.seed{}.pickle\".format(iterations_current, stability_seed)\n",
    "                if not filepath_S.exists():\n",
    "                    print(\"{} does not exist\".format(filepath_S.name))\n",
    "                    return\n",
    "                else:\n",
    "                    S_current = read_pickle(filepath_S)\n",
    "                    if len(S_dict_arrays) == 0:\n",
    "                        stats = [s for s in S_current.keys() if 'stat.' in s]\n",
    "                        importance_types = [imp for imp in S_current.keys() if 'stat.' not in imp]\n",
    "                        keys = stats + importance_types\n",
    "                        S_dict_arrays = {s: deepcopy(np.zeros((0,1))) for s in stats}\n",
    "                        S_dict_arrays.update(\n",
    "                            {\n",
    "                                imp: deepcopy(np.zeros((0,num_params))) for imp in importance_types\n",
    "                            }\n",
    "                        )\n",
    "                    for k in keys:\n",
    "                        S_dict_arrays[k] = np.vstack([S_dict_arrays[k], S_current[k]])\n",
    "            S_dict[iterations_current] = {k: S_dict_arrays[k] for k in keys}\n",
    "        write_pickle(S_dict, filepath_S_stability)\n",
    "    return S_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "S_dict = create_stability_dict_xgbo(num_params, conv.iterations_for_convergence, stability_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.convergence import Convergence\n",
    "from pathlib import Path\n",
    "from gsa_framework.utils import *\n",
    "from setups_paper_gwp import setup_salt, setup_lca_model_paper\n",
    "from gsa_framework.sensitivity_analysis.saltelli_sobol import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose which stability dictionaries to include\n",
    "path_base = Path('/data/user/kim_a/paper_gsa')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Sobol stability dictionaries\n",
    "    num_params = 10000\n",
    "    iterations = 40 * num_params\n",
    "    num_steps = 50\n",
    "    num_bootstrap = 60\n",
    "\n",
    "    gsa = setup_salt(num_params, iterations, setup_lca_model_paper, path_base)\n",
    "\n",
    "    # Convergence class\n",
    "    conv = Convergence(\n",
    "        gsa.filepath_Y,\n",
    "        gsa.num_params,\n",
    "        gsa.generate_gsa_indices,\n",
    "        gsa.gsa_label,\n",
    "        gsa.write_dir,\n",
    "        num_steps=num_steps,\n",
    "    )\n",
    "    np.random.seed(gsa.seed)\n",
    "    stability_seeds = np.random.randint(\n",
    "        low=0,\n",
    "        high=2147483647,\n",
    "        size=(len(conv.iterations_for_convergence), num_bootstrap),\n",
    "    )\n",
    "\n",
    "    filename_S = \"stability.S.{}.{}.{}Step{}.{}.{}.pickle\".format(\n",
    "        gsa.gsa_label,\n",
    "        gsa.sampling_label,\n",
    "        gsa.iterations,\n",
    "        conv.iterations_step,\n",
    "        num_bootstrap,\n",
    "        gsa.seed,\n",
    "    )\n",
    "    filepath_S = gsa.write_dir / \"arrays\" / filename_S\n",
    "    if filepath_S.exists():\n",
    "        print(\"--> {} already exists\".format(filename_S))\n",
    "        S_dict_stability = read_pickle(filepath_S)\n",
    "    else:\n",
    "        Y = read_hdf5_array(gsa.filepath_Y).flatten()\n",
    "        S_dict_stability = sobol_indices_stability(\n",
    "            Y,\n",
    "            num_params,\n",
    "            conv.iterations_for_convergence,\n",
    "            num_bootstrap,\n",
    "            stability_seeds,\n",
    "        )\n",
    "        write_pickle(S_dict_stability, filepath_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardized regression coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from setups_paper_gwp import setup_corr, setup_delt, setup_lca_model_paper\n",
    "from pathlib import Path\n",
    "from gsa_framework.utils import read_hdf5_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = Path('/data/user/kim_a/paper_gsa')\n",
    "\n",
    "num_params = 10000\n",
    "iter_corr = 4*num_params\n",
    "gsa_corr = setup_corr(num_params, iter_corr, setup_lca_model_paper, path_base)\n",
    "S_corr = gsa_corr.perform_gsa()\n",
    "\n",
    "iter_delt = 8*num_params\n",
    "gsa_delt = setup_delt(num_params, iter_delt, setup_lca_model_paper, path_base)\n",
    "S_delt = gsa_delt.perform_gsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = read_hdf5_array(gsa_corr.filepath_X_rescaled)\n",
    "Y = read_hdf5_array(gsa_corr.filepath_Y).flatten()\n",
    "reg_4x = LinearRegression().fit(X, Y)\n",
    "reg_4x.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_4x = reg_4x.coef_ * np.std(X, axis=0) / np.std(Y)\n",
    "np.sum(src_4x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = gsa_corr.plot_sa_results({\"reg_4x\": src_4x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_corr.pop(\"pearson\")\n",
    "fig = gsa_corr.plot_sa_results(S_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = read_hdf5_array(gsa_delt.filepath_X_rescaled)\n",
    "Y = read_hdf5_array(gsa_delt.filepath_Y).flatten()\n",
    "reg_8x = LinearRegression().fit(X, Y)\n",
    "reg_8x.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_8x = reg_8x.coef_ * np.std(X, axis=0) / np.std(Y)\n",
    "np.sum(src_8x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = gsa_delt.plot_sa_results({\"reg_8x\": src_8x**2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_delt.pop(\"delta_conf\")\n",
    "fig = gsa_delt.plot_sa_results(S_delt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X1 = X[:num_params, :]\n",
    "Y1 = Y[:num_params]\n",
    "reg_1x = LinearRegression().fit(X1, Y1)\n",
    "reg_1x.score(X1, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_1x = reg_1x.coef_ * np.std(X1, axis=0) / np.std(Y1)\n",
    "np.sum(src_1x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X2 = X[:2*num_params, :]\n",
    "Y2 = Y[:2*num_params]\n",
    "reg_2x = LinearRegression().fit(X2, Y2)\n",
    "reg_2x.score(X2, Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_2x = reg_2x.coef_ * np.std(X2, axis=0) / np.std(Y2)\n",
    "np.sum(src_2x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Construct LCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsa_framework.lca import LCAModel\n",
    "from gsa_framework.methods.correlations import CorrelationCoefficients\n",
    "from gsa_framework.methods.extended_FAST import eFAST\n",
    "from gsa_framework.methods.saltelli_sobol import SaltelliSobol\n",
    "from gsa_framework.methods.gradient_boosting import GradientBoosting\n",
    "from gsa_framework.validation import Validation\n",
    "from pathlib import Path\n",
    "import brightway2 as bw\n",
    "import time\n",
    "import numpy as np\n",
    "from gsa_framework.plotting import histogram_Y1_Y2\n",
    "from gsa_framework.utils import read_hdf5_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     path_base = Path(\n",
    "#         \"/Users/akim/PycharmProjects/gsa_framework/dev/write_files/paper_gsa/\"\n",
    "#     )\n",
    "    path_base = Path('/data/user/kim_a/paper_gsa/gsa_framework_files')\n",
    "\n",
    "    # LCA model\n",
    "    bw.projects.set_current(\"GSA for paper\")\n",
    "    co = bw.Database(\"CH consumption 1.0\")\n",
    "    demand_act = [act for act in co if \"Food and non-alcoholic beverages sector\" in act['name']][0]\n",
    "    print(demand_act)\n",
    "    demand = {demand_act: 1}\n",
    "    method = (\"IPCC 2013\", \"climate change\", \"GWP 100a\")\n",
    "\n",
    "    # Define some variables\n",
    "    num_params = 162299\n",
    "    iterations_validation = 2000\n",
    "    write_dir = path_base / \"lca_model_food_{}\".format(num_params)\n",
    "    model = LCAModel(demand, method, write_dir) # TODO add num_params later\n",
    "    gsa_seed = 3403\n",
    "    validation_seed = 7043\n",
    "    fig_format = [\"html\", \"pickle\"]\n",
    "\n",
    "    # Make sure  that the chosen num_params in LCA are appropriate\n",
    "    val = Validation(\n",
    "        model=model,\n",
    "        iterations=iterations_validation,\n",
    "        seed=4444,\n",
    "        default_x_rescaled=model.default_uncertain_amounts,\n",
    "        write_dir=write_dir,\n",
    "    )\n",
    "    num_params_paper = 10000\n",
    "    tag = \"numParams{}\".format(num_params_paper)\n",
    "    scores_dict = model.get_lsa_scores_pickle(model.write_dir / \"LSA_scores\")\n",
    "    uncertain_tech_params_where_subset, _ = model.get_nonzero_params_from_num_params(scores_dict, num_params_paper)\n",
    "    parameter_choice = []\n",
    "    for u in uncertain_tech_params_where_subset:\n",
    "        where_temp = np.where(model.uncertain_tech_params_where == u)[0]\n",
    "        assert len(where_temp) == 1\n",
    "        parameter_choice.append(where_temp[0])\n",
    "    parameter_choice.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_subset = val.get_influential_Y_from_parameter_choice(parameter_choice=parameter_choice, tag=tag)\n",
    "val.plot_histogram_Y_all_Y_inf(Y_subset, num_influential=num_params_paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gsa-dev]",
   "language": "python",
   "name": "conda-env-gsa-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
